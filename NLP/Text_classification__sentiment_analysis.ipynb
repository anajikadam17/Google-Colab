{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_classification__sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anajikadam17/Google-Colab/blob/main/NLP/Text_classification__sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "632HsFyhPgk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98a7bf3-2f76-4cfc-9fd2-8ea002c1551a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO62d3AIVbkE",
        "outputId": "e38f16ed-b91c-4a39-cfc9-b343ab676172"
      },
      "source": [
        "import os\n",
        "\n",
        "Base_path = r\"/content/drive/MyDrive/Colab/NLP/Text_classification_all_network\"\n",
        "\n",
        "os.chdir(Base_path)\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab/NLP/Text_classification_all_network\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2PoDhi3QbwC"
      },
      "source": [
        "#### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBol542lQBVg"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucFEVOV0QsU1"
      },
      "source": [
        "dataset=\"Data/IMDB Dataset.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqKrCEswcZQH"
      },
      "source": [
        "df=pd.read_csv(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhRwjblSmRgb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f4b4cea0-953f-4bb0-ae0c-5191cee32ed4"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgms4oCRmbwT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "e6430ef8-5666-48a7-c7fc-2675666312f6"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50000</td>\n",
              "      <td>50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>49582</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Loved today's show!!! It was a variety and not...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>5</td>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   review sentiment\n",
              "count                                               50000     50000\n",
              "unique                                              49582         2\n",
              "top     Loved today's show!!! It was a variety and not...  positive\n",
              "freq                                                    5     25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rRFzWDXmfC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58125bae-5c04-43b8-ac9d-de0c378a2016"
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    25000\n",
              "negative    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjT6opvJWAfQ"
      },
      "source": [
        "#### equal value count for sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BnvR11cmlo9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ff8e4046-31a9-44f9-b4a3-8ecb36f0b491"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "s = \"A wonderful little production. <br /><br />\"\n",
        "strip_html(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A wonderful little production. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWoFj8yhw5LX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3dcb2415-00f3-41ab-f13c-e457f99d06fd"
      },
      "source": [
        "import re\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "s = \"A wonderful little production.[1]\"\n",
        "remove_between_square_brackets(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A wonderful little production.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_64NFmI9w8te"
      },
      "source": [
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCqSniK8xFpj"
      },
      "source": [
        "df['review']=df['review'].apply(denoise_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NilyoOqxn1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "e8a37b21-485b-4aa6-b818-973acb14eb3e"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. The filming tec...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my all-time favorite movie, a story o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>I saw this movie when I was about 12 when it c...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>So im not a big fan of Boll's work but then ag...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>The cast played Shakespeare.Shakespeare lost.I...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>This a fantastic movie of three prisoners who ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Kind of drawn in by the erotic scenes, only to...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Some films just simply should not be remade. T...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>This movie made it into one of my top 10 most ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>I remember this film,it was the first film i h...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>An awful film! It must have been up against so...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               review sentiment\n",
              "0   One of the other reviewers has mentioned that ...  positive\n",
              "1   A wonderful little production. The filming tec...  positive\n",
              "2   I thought this was a wonderful way to spend ti...  positive\n",
              "3   Basically there's a family where a little boy ...  negative\n",
              "4   Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "5   Probably my all-time favorite movie, a story o...  positive\n",
              "6   I sure would like to see a resurrection of a u...  positive\n",
              "7   This show was an amazing, fresh & innovative i...  negative\n",
              "8   Encouraged by the positive comments about this...  negative\n",
              "9   If you like original gut wrenching laughter yo...  positive\n",
              "10  Phil the Alien is one of those quirky films wh...  negative\n",
              "11  I saw this movie when I was about 12 when it c...  negative\n",
              "12  So im not a big fan of Boll's work but then ag...  negative\n",
              "13  The cast played Shakespeare.Shakespeare lost.I...  negative\n",
              "14  This a fantastic movie of three prisoners who ...  positive\n",
              "15  Kind of drawn in by the erotic scenes, only to...  negative\n",
              "16  Some films just simply should not be remade. T...  positive\n",
              "17  This movie made it into one of my top 10 most ...  negative\n",
              "18  I remember this film,it was the first film i h...  positive\n",
              "19  An awful film! It must have been up against so...  negative"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7AWY8aww-Pg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8bc71d61-483d-410b-bed6-87728d175701"
      },
      "source": [
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "\n",
        "s = \"A wonderful little production. @ #saveNation\"\n",
        "remove_special_characters(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A wonderful little production  saveNation'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcRVvCJBTbgB"
      },
      "source": [
        "def Convert_to_bin(text, remove_digits=True):\n",
        "    return 1 if text=='positive' else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69m6G5iNxx8n"
      },
      "source": [
        "df['review']=df['review'].apply(remove_special_characters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG6v5aAQTu1l"
      },
      "source": [
        "df['sentiment']=df['sentiment'].apply(Convert_to_bin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMegnBOex5MM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "9fa56ca7-6144-44b8-9def-8e0e2356c513"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production The filming tech...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically theres a family where a little boy J...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Matteis Love in the Time of Money is a ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my alltime favorite movie a story of ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing fresh  innovative ide...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>I saw this movie when I was about 12 when it c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>So im not a big fan of Bolls work but then aga...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>The cast played ShakespeareShakespeare lostI a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>This a fantastic movie of three prisoners who ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Kind of drawn in by the erotic scenes only to ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Some films just simply should not be remade Th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>This movie made it into one of my top 10 most ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>I remember this filmit was the first film i ha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>An awful film It must have been up against som...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               review  sentiment\n",
              "0   One of the other reviewers has mentioned that ...          1\n",
              "1   A wonderful little production The filming tech...          1\n",
              "2   I thought this was a wonderful way to spend ti...          1\n",
              "3   Basically theres a family where a little boy J...          0\n",
              "4   Petter Matteis Love in the Time of Money is a ...          1\n",
              "5   Probably my alltime favorite movie a story of ...          1\n",
              "6   I sure would like to see a resurrection of a u...          1\n",
              "7   This show was an amazing fresh  innovative ide...          0\n",
              "8   Encouraged by the positive comments about this...          0\n",
              "9   If you like original gut wrenching laughter yo...          1\n",
              "10  Phil the Alien is one of those quirky films wh...          0\n",
              "11  I saw this movie when I was about 12 when it c...          0\n",
              "12  So im not a big fan of Bolls work but then aga...          0\n",
              "13  The cast played ShakespeareShakespeare lostI a...          0\n",
              "14  This a fantastic movie of three prisoners who ...          1\n",
              "15  Kind of drawn in by the erotic scenes only to ...          0\n",
              "16  Some films just simply should not be remade Th...          1\n",
              "17  This movie made it into one of my top 10 most ...          0\n",
              "18  I remember this filmit was the first film i ha...          1\n",
              "19  An awful film It must have been up against som...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSkWQBn0x9hP"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtIgQ4BF1UP9"
      },
      "source": [
        "X=df['review'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrztROLo1ZCU"
      },
      "source": [
        "Y=df['sentiment'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lyuWULG1fx3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "b8009a75-223a-4c6e-a773-12938b2452d6"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(df['sentiment'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f93884c0610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chwNb6zH1zMl"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izqKYrYpdrs5"
      },
      "source": [
        "## Artificial Neural Networks Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88RS0HPQ27oC"
      },
      "source": [
        "## Vectorizers :Bag of words\n",
        "\n",
        "### **Vectorizers**: \n",
        "In this method, we create a single feature vector using all the words in the vocubulary. Each word is basically  reagrded as a feature. So, the number of features is equal to the number of unique words in the vocab. Now, each sentence or review is a sample or record. Now, if the word is present in that sample it has some values and if the word is not present it is zero. Also called **bag of words model**\n",
        "\n",
        "So, each sample has the same feature set size which is equal to the size of the vocabulary. Now, the vocabuary is basically made of the words in the train set. All the samples of the train and test set is fit using this vocabulary only. So, there may be some words in the test samples which are not present in the vocabulary, they are ignored. \n",
        "\n",
        "Now, they form very sparse matrices or feature sets. Now, similar to a normal classification problem, the words become features of the record and the corresponding tag becomes the target value. So, it is actually like a common classification problem with number of features being equal to the distinct tokens in the training set.\n",
        "\n",
        "This can be done in two ways:\n",
        "\n",
        "1. **Count Vectorizer**: Here the count of a word in a particular sample or review. The count of that word becomes the value of the corresponding word feature. If a word in the vocab does not appear in the sample its value is 0. \n",
        "\n",
        "2. **TF-IDF Vectorizer**: It is a better approach. It calculates two things term frequency and inverse document frequency. Term frequency= No. of times the word appears in the sample. \n",
        "\n",
        "`IDF = log ( number of time the word appears in the sample / number of time the word appears in the whole document)` \n",
        "\n",
        "This helps to note some differences like the word \"The\" appears with same freq in almost all sentences while special words carrying significance like \"good\" don't. So, these TF and IDF terms are multiplied to obtain the vector formats for each sample. \n",
        "\n",
        "It can also be done by ***TensorFlows Tokenizer***.\n",
        " \n",
        "For creating the matrix here, we need to use a tensorflow tokenizer, tokenizes the text to tokens. It can be done in mainly three ways:\n",
        "1. **Binary:** `X = tokenizer.sequences_to_matrix(x, mode='binary')` \n",
        "- In this case, the value of a word feature is 1 if the word is present in the sample else zero.\n",
        "\n",
        "2. **Count:** `X = tokenizer.sequences_to_matrix(x, mode='count')` \n",
        "- In this case, it is the number of times a word appear in the sentence.\n",
        "\n",
        "3. **TF-IDF:** `X  = tokenizer.sequences_to_matrix(x, mode='tfidf')`\n",
        "- In this we consider the TF of the word in the sample and IDF of the word in the sample with resect to the occurence of the word in the whole document.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60lfWluhOYTn"
      },
      "source": [
        "#### Count Vectorizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIOFMb_62Iql"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS7erqyzQUuI"
      },
      "source": [
        "vec = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcIyRZ3uQaC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "923d7f1b-c66b-4578-a62c-eadd23aa1385"
      },
      "source": [
        "vec.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnBFYSHtQzNb"
      },
      "source": [
        "x_train = vec.transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_bIhFMZQ6bn"
      },
      "source": [
        "x_test = vec.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jnXyYWtRIN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eacc03fb-b7d8-45df-824a-ce7aadca109d"
      },
      "source": [
        "type(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjxJZg22abRp"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjzM40v0ROnm"
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=x_train.shape[1], activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYQWyJZQSZm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f4f1c9-0986-426e-8a7a-e5570a58b0e5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 16)                2817952   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 2,818,241\n",
            "Trainable params: 2,818,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOuW2LfUSdXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d171e3-0ddf-486e-db8b-ef741f8a9f03"
      },
      "source": [
        "history = model.fit(x_train, Y_train,epochs=10, verbose=True, batch_size=16 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_2/dense_6/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_2/dense_6/embedding_lookup_sparse/Reshape:0\", shape=(None, 16), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_2/dense_6/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2188/2188 [==============================] - 10s 4ms/step - loss: 0.3132 - accuracy: 0.8764\n",
            "Epoch 2/10\n",
            "2188/2188 [==============================] - 10s 4ms/step - loss: 0.1119 - accuracy: 0.9591\n",
            "Epoch 3/10\n",
            "2188/2188 [==============================] - 10s 5ms/step - loss: 0.0459 - accuracy: 0.9840\n",
            "Epoch 4/10\n",
            "2188/2188 [==============================] - 10s 5ms/step - loss: 0.0228 - accuracy: 0.9928\n",
            "Epoch 5/10\n",
            "2188/2188 [==============================] - 10s 5ms/step - loss: 0.0154 - accuracy: 0.9948\n",
            "Epoch 6/10\n",
            "2188/2188 [==============================] - 10s 4ms/step - loss: 0.0108 - accuracy: 0.9965\n",
            "Epoch 7/10\n",
            "2188/2188 [==============================] - 10s 4ms/step - loss: 0.0076 - accuracy: 0.9979\n",
            "Epoch 8/10\n",
            "2188/2188 [==============================] - 10s 4ms/step - loss: 0.0068 - accuracy: 0.9979\n",
            "Epoch 9/10\n",
            "2188/2188 [==============================] - 10s 5ms/step - loss: 0.0030 - accuracy: 0.9991\n",
            "Epoch 10/10\n",
            "2188/2188 [==============================] - 10s 5ms/step - loss: 0.0061 - accuracy: 0.9985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbFF8zS8cPlX"
      },
      "source": [
        "## Model Training Done"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmwg2o6jX3OH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e70d150-0c69-440a-cff8-c31820930dec"
      },
      "source": [
        "model.evaluate(x_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1094/1094 [==============================] - 3s 3ms/step - loss: 7.7954e-04 - accuracy: 0.9998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0007795396959409118, 0.999828577041626]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3bPAXJWdwZS"
      },
      "source": [
        "### Training Score is 100% with CountVectorizer and ANN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc0kiSG8YF0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a816fbb-9c9b-430e-bd65-f071a7a5039e"
      },
      "source": [
        "model.evaluate(x_test,Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 1s 3ms/step - loss: 0.8238 - accuracy: 0.8752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8237789869308472, 0.8751999735832214]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGs4VfWpbMYf"
      },
      "source": [
        "### Test score 87%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYEFF4DFYNXB"
      },
      "source": [
        "### TF-IDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4ljhTBIYT5W"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl5Dz-kbZCC8"
      },
      "source": [
        "tfvec = TfidfVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxz_qjmTZGG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb397f4c-74d8-466d-e634-5ee092dc1498"
      },
      "source": [
        "tfvec.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqlHfFJCZJVl"
      },
      "source": [
        "x_train = tfvec.transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imtf8G4fZNM-"
      },
      "source": [
        "x_test = tfvec.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVwpPy90ZPd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35189634-fd65-4d5a-b587-9385bc7222ae"
      },
      "source": [
        "type(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHpCJ9WOZQ3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660e3244-b197-4a1e-8c55-5b17c08c657a"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzfsMY0-eMB-",
        "outputId": "ffbfb96f-00a2-46c6-def6-54235d35650a"
      },
      "source": [
        "score_train = lr.score(x_train, Y_train)\n",
        "print(\"Training score : \",score_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score :  0.9342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH01AGn2eQ2c"
      },
      "source": [
        "### Training Score is 93% with TfidfVectorizer and LogisticRegression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2pJELG_aIKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34adb5df-0f8e-448d-c58a-4347e37885c0"
      },
      "source": [
        "score_test = lr.score(x_test, Y_test)\n",
        "print(\"Test score : \",score_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score :  0.8882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbNPv1EKedvv"
      },
      "source": [
        "### Test Score is 100% with TfidfVectorizer and LogisticRegression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9Zed2gpdz5K"
      },
      "source": [
        "## Word Embedding : Time series model\n",
        "\n",
        "**Word Embedding**:\n",
        "\n",
        " In this method, the words are individually represented as a vector. In case of the bag of words all of the words made up a vector. Here, there are 100 words in a vocabulary, so, a specific word will be represented by a vector of size 100 where the index corresponding to that word will be equal to 1, and others will be 0. \n",
        "\n",
        "So, Each sample having different number of words will basically have a different number of vectors, as each word is equal to a vector. Now, to feed a model we will need to have the same dimension for each sample, and as a result padding is needed to make the number of words in each sample equal to each other.  \n",
        "\n",
        "Basically in the bag of words or vectorizer approach, if we have 100 words in our total vocabulary, and a sample with 10 words and a sample with 15 words, after vectorization both the sample sizes would be an array of 100 words, but here for the 10 words it will be a (10 x 100) i,e 100 length vector for each of the 10 words and similarly for 15th one size will be (15 x 100). So, we need to find the longest sample and pad all others up to match the size.\n",
        "\n",
        "We can do this in some ways:\n",
        "\n",
        "**One-Hot encoding:** It is just taking the size of the vocabulary and making an array of that size with 0's at all indices and 1 at only the index of the word. But this things provides us with a very less information.\n",
        "\n",
        "The Second Choice is **word embeddings**.\n",
        "\n",
        "The one hot encoder is a pretty hard coded approach. It is of a very high dimension and sparse with a very low amount of data. **Embedding is a way to create a dense vector representation out of the sparse representations.** It is of a lower dimension and helps to capture much more informations. It more like captures the relation and similarities between words using how they appear close to each other. For example, king, queen, men and women will have some relations. \n",
        "\n",
        "Say, we are having 10k words are being embedded in a 300 dimensional embedding space. To do this, we declare the number of nodes in the embedding layer =300. Now, each word of the 10k words enter the embedding layer as a 10k sized individual vector, Now, each of the words will be placed in 300 dimensional plane based on their similarities with one another which is decieded by several factors, like the order in which the words occur. Now, being placed in 300 Dimensional plane the words will have a 300 length tuple to represent it which are actually the coordinates of the point on the 300 dimensional plane. So, this 300 dimensional tuple becomes the new feature set or representing vector for the word. \n",
        "\n",
        "So, the vector for the word decreased from 10k to 300. The tuples serve as feature vectors between two words and the cosine angle between the vectors represent the similarity between the two words.\n",
        "\n",
        "We can do this in two ways:\n",
        "\n",
        "1. Using our own embeddings\n",
        "\n",
        "2. Using pretrained embeddings\n",
        "\n",
        "**Making our own embedding** \n",
        "\n",
        "Now, for the embedding, we need to send each sample through an embedding layer first then move to make them dense using embedding. These embedding layers see how the words are used, i.e, it tries to see if two words always occur together or are used in contrast. After judging all these factors the layer places the word in a position one the n-dimensional embedding space.\n",
        "\n",
        "**Using pretrained embedded matrices**\n",
        "\n",
        "We can use pretrained word embeddings like word2vec by google and GloveText by standford.They are trained on huge corpuses with billions of examples and words. Now, they have billions of words we have only a 10k so, training our model with a billion words will be very inefficient, So, we need to just select out our required word's embeddings from their pretrained embeddings.\n",
        "\n",
        "Now, **How are these embeddings found?** \n",
        "\n",
        "For google's word2vec implementations, there are two ways:\n",
        "\n",
        "1. Continous bag of words\n",
        "2. Spin Gram.\n",
        "\n",
        "Both of these algorithms actually use a Neural Network with a single hidden layer to generate the embedding.\n",
        "\n",
        "Now, for CBOW, the context of the words , i.e, the words befor and after the required words are fed to the neural network, and the model is needed to predict the word. \n",
        "\n",
        "For the Spin-Gram, the words are given and the model has to predict the context words.\n",
        "\n",
        "In both cases, the feature vectors or encoded vectors of the words are fed to the input. The output has a softmax layer with number of nodes equal to the vocabulary size, which gives the percentage of prediction for each word. Though we don't use the output layer actually. \n",
        "\n",
        "We go for the weight matrix produced in the hidden layer. **The number of nodes in the hidden layer is equal to the embedding dimension.** So, say if there are 10k words in a vocabulary and 300 nodes in the ghidden layer, each node in the hidden layer will have an array of weights of dimension of 10k for each word after training. \n",
        "\n",
        "Because the neural network units work on\n",
        "\n",
        "y=f(w1x1+w2x2+..........wnxn)\n",
        "\n",
        "Here x1, x2...... xn are the words and so n= number of words in vocabulary=10k. \n",
        "\n",
        "So for 10k x's there will be 10k w's. Now foe 1 node there are 10k length weight matrix. For 300 combined we have a matrix of 300 x 10k wieghts. Now, if we concatenate, we will have 300 rows and 10k columns. Let's transpose the matrix. We will get 300 columns and 10k rows. Each row represents a word, and the 300 column values represent a 300 length wieght vector for that word.\n",
        "\n",
        "This weight vector is the obtained embedding of length 300."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxI40eQr4rXH"
      },
      "source": [
        "### Training own embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuOGt-ewdjdA"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSf0hC6B4gP0"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXaZtg524qb5"
      },
      "source": [
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbzweVge5ufl"
      },
      "source": [
        "1) `tokenize.fit_on_text() `\n",
        "\n",
        "->>  Creates the vocabulary index based on word frequency. For example, if you had the phrase \"My dog is different from your dog, my dog is prettier\", word_index[\"dog\"] = 0, word_index[\"is\"] = 1 (dog appears 3 times, is appears 2 times)\n",
        "\n",
        "2) `tokenize.text_to_sequence()` \n",
        "\n",
        "-->> Transforms each text into a sequence of integers. Basically if you had a sentence, it would assign an integer to each word from your sentence. You can access tokenizer.word_index() (returns a dictionary) to verify the assigned integer to your word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XrCNMJr41q2"
      },
      "source": [
        "x_train = tokenizer.texts_to_sequences(X_train) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_qOyr78hQuO",
        "outputId": "cf199c9a-0f84-4b47-8d95-592683204b13"
      },
      "source": [
        "type(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5PYo-J650wN"
      },
      "source": [
        "x_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc9Xay2m59XS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cecf06d4-a192-4076-fd0c-5669990d5eee"
      },
      "source": [
        "vocab = len(tokenizer.word_index) + 1  \n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "176758"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBAOrVYYDX1b"
      },
      "source": [
        "+1 for padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6jHFS-lDfh9"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkEKBrLnDjRe"
      },
      "source": [
        "maxlen = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwWpYRT6DlL0"
      },
      "source": [
        "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k4LumHKD6WQ"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,Dense, Activation, MaxPool1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "emb_dim=100\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Embedding(input_dim=vocab, output_dim=emb_dim, input_length=maxlen))\n",
        "model.add(MaxPool1D())\n",
        "model.add(Dense(16,activation=\"relu\"))\n",
        "model.add(Dense(16,activation=\"relu\"))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oR8iq7BG11y"
      },
      "source": [
        "Maxpooling is used to convert the sparse matrix denser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXOjlbEmGvtY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d6c79b-f433-42e3-ee9d-560ef159f1d5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 100)          17675800  \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 50, 100)           0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 50, 16)            1616      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 50, 16)            272       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 50, 1)             17        \n",
            "=================================================================\n",
            "Total params: 17,677,705\n",
            "Trainable params: 17,677,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZLFWwdRHH8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe8a0b7-fe44-473c-bd4e-85cb47eb45f6"
      },
      "source": [
        "history = model.fit(x_train, Y_train,epochs=35,verbose=True,batch_size=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "2188/2188 [==============================] - 354s 160ms/step - loss: 0.6686 - accuracy: 0.5774\n",
            "Epoch 2/35\n",
            "2188/2188 [==============================] - 351s 160ms/step - loss: 0.6567 - accuracy: 0.5959\n",
            "Epoch 3/35\n",
            "2188/2188 [==============================] - 350s 160ms/step - loss: 0.6505 - accuracy: 0.6030\n",
            "Epoch 4/35\n",
            "2188/2188 [==============================] - 350s 160ms/step - loss: 0.6438 - accuracy: 0.6101\n",
            "Epoch 5/35\n",
            "2188/2188 [==============================] - 350s 160ms/step - loss: 0.6368 - accuracy: 0.6163\n",
            "Epoch 6/35\n",
            "2188/2188 [==============================] - 351s 160ms/step - loss: 0.6290 - accuracy: 0.6231\n",
            "Epoch 7/35\n",
            "2188/2188 [==============================] - 351s 160ms/step - loss: 0.6216 - accuracy: 0.6285\n",
            "Epoch 8/35\n",
            "2188/2188 [==============================] - 352s 161ms/step - loss: 0.6143 - accuracy: 0.6339\n",
            "Epoch 9/35\n",
            "2188/2188 [==============================] - 352s 161ms/step - loss: 0.6076 - accuracy: 0.6381\n",
            "Epoch 10/35\n",
            "2188/2188 [==============================] - 351s 160ms/step - loss: 0.6016 - accuracy: 0.6419\n",
            "Epoch 11/35\n",
            "2188/2188 [==============================] - 352s 161ms/step - loss: 0.5964 - accuracy: 0.6453\n",
            "Epoch 12/35\n",
            "2188/2188 [==============================] - 352s 161ms/step - loss: 0.5919 - accuracy: 0.6483\n",
            "Epoch 13/35\n",
            "2188/2188 [==============================] - 352s 161ms/step - loss: 0.5877 - accuracy: 0.6505\n",
            "Epoch 14/35\n",
            "2188/2188 [==============================] - 351s 161ms/step - loss: 0.5843 - accuracy: 0.6525\n",
            "Epoch 15/35\n",
            "2188/2188 [==============================] - 351s 160ms/step - loss: 0.5807 - accuracy: 0.6547\n",
            "Epoch 16/35\n",
            "2188/2188 [==============================] - 351s 160ms/step - loss: 0.5783 - accuracy: 0.6556\n",
            "Epoch 17/35\n",
            "2188/2188 [==============================] - 352s 161ms/step - loss: 0.5758 - accuracy: 0.6568\n",
            "Epoch 18/35\n",
            "2188/2188 [==============================] - 354s 162ms/step - loss: 0.5734 - accuracy: 0.6581\n",
            "Epoch 19/35\n",
            "2188/2188 [==============================] - 354s 162ms/step - loss: 0.5714 - accuracy: 0.6591\n",
            "Epoch 20/35\n",
            "2188/2188 [==============================] - 352s 161ms/step - loss: 0.5693 - accuracy: 0.6602\n",
            "Epoch 21/35\n",
            "2188/2188 [==============================] - 351s 160ms/step - loss: 0.5677 - accuracy: 0.6613\n",
            "Epoch 22/35\n",
            "2188/2188 [==============================] - 351s 160ms/step - loss: 0.5661 - accuracy: 0.6622\n",
            "Epoch 23/35\n",
            "2188/2188 [==============================] - 352s 161ms/step - loss: 0.5644 - accuracy: 0.6632\n",
            "Epoch 24/35\n",
            "2188/2188 [==============================] - 357s 163ms/step - loss: 0.5632 - accuracy: 0.6637\n",
            "Epoch 25/35\n",
            "2188/2188 [==============================] - 358s 163ms/step - loss: 0.5620 - accuracy: 0.6638\n",
            "Epoch 26/35\n",
            "2188/2188 [==============================] - 357s 163ms/step - loss: 0.5608 - accuracy: 0.6647\n",
            "Epoch 27/35\n",
            "2188/2188 [==============================] - 353s 161ms/step - loss: 0.5595 - accuracy: 0.6651\n",
            "Epoch 28/35\n",
            "2188/2188 [==============================] - 354s 162ms/step - loss: 0.5585 - accuracy: 0.6658\n",
            "Epoch 29/35\n",
            "2188/2188 [==============================] - 354s 162ms/step - loss: 0.5577 - accuracy: 0.6660\n",
            "Epoch 30/35\n",
            "2188/2188 [==============================] - 355s 162ms/step - loss: 0.5565 - accuracy: 0.6672\n",
            "Epoch 31/35\n",
            "2188/2188 [==============================] - 357s 163ms/step - loss: 0.5558 - accuracy: 0.6673\n",
            "Epoch 32/35\n",
            "2188/2188 [==============================] - 358s 163ms/step - loss: 0.5551 - accuracy: 0.6672\n",
            "Epoch 33/35\n",
            "2188/2188 [==============================] - 355s 162ms/step - loss: 0.5542 - accuracy: 0.6682\n",
            "Epoch 34/35\n",
            "2188/2188 [==============================] - 354s 162ms/step - loss: 0.5536 - accuracy: 0.6685\n",
            "Epoch 35/35\n",
            "2188/2188 [==============================] - 355s 162ms/step - loss: 0.5527 - accuracy: 0.6684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDu4vJSjNb4R"
      },
      "source": [
        "Here at each step the model weights as well as the embedding dimensions are getting corrected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPoQIdJ5iz1e",
        "outputId": "c0595ec0-94eb-46a3-cecd-5d0bec4d72b6"
      },
      "source": [
        "train_score=model.evaluate(x_train,Y_train)\n",
        "print(\"Training score : \",train_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1094/1094 [==============================] - 3s 3ms/step - loss: 0.5438 - accuracy: 0.6750\n",
            "Training score :  [0.5438181757926941, 0.6750374436378479]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb37PPwAgEXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5ed63ef-33b2-48ea-80f4-1aad4aa7ef5a"
      },
      "source": [
        "test_score=model.evaluate(x_test,Y_test)\n",
        "print(\"Test score : \",test_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 1s 3ms/step - loss: 0.8898 - accuracy: 0.5772\n",
            "Test score :  [0.889846682548523, 0.5771628618240356]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi_BXOjc_j8q"
      },
      "source": [
        "Train accuray 67.5\n",
        "Test accuracy 57.71"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0gSbKkye3ay"
      },
      "source": [
        "## Done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGoOtGnENVE_"
      },
      "source": [
        "### Using pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NITbOKKJP-bp"
      },
      "source": [
        "file_path=\"drive/My Drive/glove.6B/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C32ztL-vQzze"
      },
      "source": [
        "import os\n",
        "files=os.listdir(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf30EidjRT7h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "67d39706-adea-4ec1-dd61-51d81df805e3"
      },
      "source": [
        "files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['glove.6B.100d.txt',\n",
              " 'glove.6B.200d.txt',\n",
              " 'glove.6B.50d.txt',\n",
              " 'glove.6B.300d.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUL_GkNJP9MM"
      },
      "source": [
        "So, here are four files each of a different embedding size 50, 100, 200, 300\n",
        "\n",
        "We will go for 50.\n",
        "\n",
        "Now, here there are 6 billion words we have much less words than that so what we will do is, we will find our words and pick the weights from the pretrained words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I8qjcAXNjYp"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiOqmwpQS9fS"
      },
      "source": [
        "x_train = tokenizer.texts_to_sequences(X_train) \n",
        "x_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usYQnSzQXmjm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb560077-23f9-4427-f0d0-2d93ae7fc2e7"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxlen=100\n",
        "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXNg9AULTd5q"
      },
      "source": [
        "emb_dim=50\n",
        "vocab=len(tokenizer.word_index)+1\n",
        "emb_mat= np.zeros((vocab,emb_dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDZVxcDXUHQu"
      },
      "source": [
        "Initializing a zero matrix for each word, they will be compared to have their final embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_qmGYjaUm71"
      },
      "source": [
        "with open(file_path+'glove.6B.50d.txt') as f:\n",
        "        for line in f:\n",
        "            word, *emb = line.split()\n",
        "            if word in tokenizer.word_index:\n",
        "              ind=tokenizer.word_index[word]\n",
        "              emb_mat[ind]=np.array(emb,dtype=\"float32\")[:emb_dim]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qryMbGi3VgD8"
      },
      "source": [
        "This is an extractor for the task, so we have the embeddings and the words in a line. So, we just compare the words pick out the indices in our dataset. Take the vectors and place it in the embedding matrix at a index corresponding to the index of the word in our dataset.\n",
        "\n",
        "We have used a *emb because the embedding matrix is variant in size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDWH1Y8qWRwn"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,Dense, Activation, MaxPool1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "emb_dim=50\n",
        "maxlen=100\n",
        "model= Sequential()\n",
        "model.add(Embedding(input_dim=vocab, output_dim=emb_dim,weights=[emb_mat], input_length=maxlen,trainable=False))\n",
        "model.add(MaxPool1D())\n",
        "model.add(Dense(16,activation=\"relu\"))\n",
        "model.add(Dense(16,activation=\"relu\"))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ELUTMsQXKLj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "db92bced-6ec2-443c-cb59-8f0535a26243"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 50)           8839300   \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 50, 50)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 50, 16)            816       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50, 16)            272       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50, 1)             17        \n",
            "=================================================================\n",
            "Total params: 8,840,405\n",
            "Trainable params: 1,105\n",
            "Non-trainable params: 8,839,300\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99GfEWDfXNZX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f6fb045f-424e-4ffa-ef2c-8bf01154b8d4"
      },
      "source": [
        "history = model.fit(x_train, Y_train,epochs=50,verbose=True,batch_size=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2188/2188 [==============================] - 7s 3ms/step - loss: 0.6815 - accuracy: 0.5536\n",
            "Epoch 2/50\n",
            "2188/2188 [==============================] - 7s 3ms/step - loss: 0.6809 - accuracy: 0.5552\n",
            "Epoch 3/50\n",
            "2188/2188 [==============================] - 7s 3ms/step - loss: 0.6806 - accuracy: 0.5557\n",
            "Epoch 4/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6801 - accuracy: 0.5567\n",
            "Epoch 5/50\n",
            "2188/2188 [==============================] - 7s 3ms/step - loss: 0.6797 - accuracy: 0.5575\n",
            "Epoch 6/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6795 - accuracy: 0.5582\n",
            "Epoch 7/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6794 - accuracy: 0.5584\n",
            "Epoch 8/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6792 - accuracy: 0.5589\n",
            "Epoch 9/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6789 - accuracy: 0.5599\n",
            "Epoch 10/50\n",
            "2188/2188 [==============================] - 7s 3ms/step - loss: 0.6787 - accuracy: 0.5602\n",
            "Epoch 11/50\n",
            "2188/2188 [==============================] - 7s 3ms/step - loss: 0.6786 - accuracy: 0.5604\n",
            "Epoch 12/50\n",
            "2188/2188 [==============================] - 7s 3ms/step - loss: 0.6786 - accuracy: 0.5604\n",
            "Epoch 13/50\n",
            "2188/2188 [==============================] - 7s 3ms/step - loss: 0.6785 - accuracy: 0.5609\n",
            "Epoch 14/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6786 - accuracy: 0.5603\n",
            "Epoch 15/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6782 - accuracy: 0.5618\n",
            "Epoch 16/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6782 - accuracy: 0.5615\n",
            "Epoch 17/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6781 - accuracy: 0.5619\n",
            "Epoch 18/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6780 - accuracy: 0.5620\n",
            "Epoch 19/50\n",
            "2188/2188 [==============================] - 7s 3ms/step - loss: 0.6779 - accuracy: 0.5620\n",
            "Epoch 20/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6780 - accuracy: 0.5621\n",
            "Epoch 21/50\n",
            "2188/2188 [==============================] - 7s 3ms/step - loss: 0.6780 - accuracy: 0.5618\n",
            "Epoch 22/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6779 - accuracy: 0.5621\n",
            "Epoch 23/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6777 - accuracy: 0.5626\n",
            "Epoch 24/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6779 - accuracy: 0.5623\n",
            "Epoch 25/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6776 - accuracy: 0.5630\n",
            "Epoch 26/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6779 - accuracy: 0.5626\n",
            "Epoch 27/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6776 - accuracy: 0.5624\n",
            "Epoch 28/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6777 - accuracy: 0.5629\n",
            "Epoch 29/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6776 - accuracy: 0.5630\n",
            "Epoch 30/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6776 - accuracy: 0.5628\n",
            "Epoch 31/50\n",
            "2188/2188 [==============================] - 8s 4ms/step - loss: 0.6774 - accuracy: 0.5633\n",
            "Epoch 32/50\n",
            "2188/2188 [==============================] - 8s 4ms/step - loss: 0.6776 - accuracy: 0.5628\n",
            "Epoch 33/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6773 - accuracy: 0.5637\n",
            "Epoch 34/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6774 - accuracy: 0.5635\n",
            "Epoch 35/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6774 - accuracy: 0.5637\n",
            "Epoch 36/50\n",
            "2188/2188 [==============================] - 8s 4ms/step - loss: 0.6774 - accuracy: 0.5634\n",
            "Epoch 37/50\n",
            "2188/2188 [==============================] - 8s 4ms/step - loss: 0.6774 - accuracy: 0.5635\n",
            "Epoch 38/50\n",
            "2188/2188 [==============================] - 8s 4ms/step - loss: 0.6772 - accuracy: 0.5640\n",
            "Epoch 39/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6770 - accuracy: 0.5647\n",
            "Epoch 40/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6773 - accuracy: 0.5638\n",
            "Epoch 41/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6772 - accuracy: 0.5643\n",
            "Epoch 42/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6772 - accuracy: 0.5638\n",
            "Epoch 43/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6771 - accuracy: 0.5640\n",
            "Epoch 44/50\n",
            "2188/2188 [==============================] - 8s 4ms/step - loss: 0.6771 - accuracy: 0.5646\n",
            "Epoch 45/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6772 - accuracy: 0.5644\n",
            "Epoch 46/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6771 - accuracy: 0.5645\n",
            "Epoch 47/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6770 - accuracy: 0.5645\n",
            "Epoch 48/50\n",
            "2188/2188 [==============================] - 8s 4ms/step - loss: 0.6772 - accuracy: 0.5637\n",
            "Epoch 49/50\n",
            "2188/2188 [==============================] - 8s 4ms/step - loss: 0.6770 - accuracy: 0.5646\n",
            "Epoch 50/50\n",
            "2188/2188 [==============================] - 8s 3ms/step - loss: 0.6771 - accuracy: 0.5646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9WBeRZ-Xa49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "833cefe3-e1e8-4c31-dec5-8eb04fa946d8"
      },
      "source": [
        "test_score=model.evaluate(x_test,Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 1s 3ms/step - loss: 0.6773 - accuracy: 0.5639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBFtlyrJX1IK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a21e3ffa-02f0-4550-d747-c4e22a8520eb"
      },
      "source": [
        "test_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6773159503936768, 0.563922643661499]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJBh9F4ZX5hk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b101e977-c803-4161-d05d-b83ee22d2298"
      },
      "source": [
        "train_score=model.evaluate(x_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1094/1094 [==============================] - 3s 3ms/step - loss: 0.6767 - accuracy: 0.5651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7dgrLrBX6bL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d23de8d7-b855-4a86-af99-5212933934a2"
      },
      "source": [
        "train_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6766629815101624, 0.5651329159736633]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYBj13rNe_GM"
      },
      "source": [
        "## Done"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}