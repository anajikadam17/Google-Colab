{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spaCy__.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPpTmiDI/qfL62MDdviRH1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anajikadam17/Google-Colab/blob/main/NLP/spaCy__.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC0YV8aS3f9L"
      },
      "source": [
        "# spaCy\n",
        "\n",
        "I’ve listed below the different statistical models in spaCy along with their specifications:\n",
        "\n",
        "- en_core_web_sm: English multi-task CNN trained on OntoNotes. Size – 11 MB\n",
        "- en_core_web_md: English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Size – 91 MB\n",
        "- en_core_web_lg: English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Size – 789 MB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc0PK1Df3u_L"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQGnnorG3XtJ"
      },
      "source": [
        "st1 = \"He went to play basketball\"\n",
        "\n",
        "# Create an nlp object\n",
        "doc = nlp(st1 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNZbG23S32Ci",
        "outputId": "354df392-a40a-4d3d-c63c-f149c3cf3e14"
      },
      "source": [
        "nlp.pipe_names   #  active pipeline components"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger', 'parser', 'ner']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6FiRDKY4AJq",
        "outputId": "20da2f64-abf5-421a-d8f4-633195fdbc59"
      },
      "source": [
        "# to disable the pipeline components and keep only the tokenizer up and running\n",
        "nlp.disable_pipes('tagger', 'parser')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7fd48b0fda50>),\n",
              " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7fd48b248830>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2Bn9U7j4NiS",
        "outputId": "19c20ef7-13dd-4d6d-b666-907acca73980"
      },
      "source": [
        "nlp.pipe_names\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ner']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It3w-K9g48mL"
      },
      "source": [
        "### Part-of-Speech (POS) Tagging using spaCy\n",
        "In English grammar, the parts of speech tell us *what is the function of a word and how it is used in a sentence*. Some of the common parts of speech in English are *Noun, Pronoun, Adjective, Verb, Adverb*, etc.\n",
        "\n",
        "POS tagging is the task of automatically assigning POS tags to all the words of a sentence. It is helpful in various downstream tasks in NLP, such as ***feature engineering, language understanding, and information extraction***.\n",
        "\n",
        "Performing POS tagging, in spaCy,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJRCIIVr4xia",
        "outputId": "aef5e61a-b79d-4706-d32e-712789e40bf9"
      },
      "source": [
        "# import spacy \n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create an nlp object\n",
        "doc = nlp(\"He went to play basketball, he is Good in Play\")\n",
        " \n",
        "# Iterate over the tokens\n",
        "for token in doc:\n",
        "    # Print the token and its part-of-speech tag\n",
        "    print(token.text, \"-->\", token.pos_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "He --> PRON\n",
            "went --> VERB\n",
            "to --> PART\n",
            "play --> VERB\n",
            "basketball --> NOUN\n",
            ", --> PUNCT\n",
            "he --> PRON\n",
            "is --> AUX\n",
            "Good --> ADJ\n",
            "in --> ADP\n",
            "Play --> NOUN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3asbD_685PaT",
        "outputId": "d8af023f-98b9-422f-f787-28aebdca02de"
      },
      "source": [
        "print(\"{} : {}\".format(\"PART\",spacy.explain(\"PART\")))\n",
        "print(\"{} : {}\".format(\"VERB\",spacy.explain(\"VERB\")))\n",
        "print(\"{} : {}\".format(\"NOUN\",spacy.explain(\"NOUN\")))\n",
        "print(\"{} : {}\".format(\"PRON\",spacy.explain(\"PRON\")))\n",
        "print(\"{} : {}\".format(\"PUNCT\",spacy.explain(\"PUNCT\")))\n",
        "print(\"{} : {}\".format(\"AUX\",spacy.explain(\"AUX\")))\n",
        "print(\"{} : {}\".format(\"ADJ\",spacy.explain(\"ADJ\")))\n",
        "print(\"{} : {}\".format(\"ADP\",spacy.explain(\"ADP\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PART : particle\n",
            "VERB : verb\n",
            "NOUN : noun\n",
            "PRON : pronoun\n",
            "PUNCT : punctuation\n",
            "AUX : auxiliary\n",
            "ADJ : adjective\n",
            "ADP : adposition\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwN-Szyg65as"
      },
      "source": [
        "### Dependency Parsing using spaCy\n",
        "Every sentence has a grammatical structure to it and with the help of dependency parsing, we can extract this structure. It can also be thought of as a directed graph, where nodes correspond to the words in the sentence and the edges between the nodes are the corresponding dependencies between the word.\n",
        "\n",
        "Performing dependency parsing is again pretty easy in spaCy. We will use the same sentence here that we used for POS tagging:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7VgSq9p5sYK",
        "outputId": "bdc8f286-9776-493b-d2e9-05377e941499"
      },
      "source": [
        "# dependency parsing\n",
        "for token in doc:\n",
        "    print(token.text, \"-->\", token.dep_)\n",
        "    print(\"{} : {}\".format(token.dep_, spacy.explain(token.dep_)))\n",
        "    print(\"---\"*10)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "He --> nsubj\n",
            "nsubj : nominal subject\n",
            "------------------------------\n",
            "went --> ccomp\n",
            "ccomp : clausal complement\n",
            "------------------------------\n",
            "to --> aux\n",
            "aux : auxiliary\n",
            "------------------------------\n",
            "play --> advcl\n",
            "advcl : adverbial clause modifier\n",
            "------------------------------\n",
            "basketball --> dobj\n",
            "dobj : direct object\n",
            "------------------------------\n",
            ", --> punct\n",
            "punct : punctuation\n",
            "------------------------------\n",
            "he --> nsubj\n",
            "nsubj : nominal subject\n",
            "------------------------------\n",
            "is --> ROOT\n",
            "ROOT : None\n",
            "------------------------------\n",
            "Good --> acomp\n",
            "acomp : adjectival complement\n",
            "------------------------------\n",
            "in --> prep\n",
            "prep : prepositional modifier\n",
            "------------------------------\n",
            "Play --> pobj\n",
            "pobj : object of preposition\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "His8dl197vfz"
      },
      "source": [
        "### Named Entity Recognition using spaCy\n",
        "Entities are the words or groups of words that represent information about common things such as **persons, locations, organizations**, etc. These entities have proper names.\n",
        "\n",
        "spaCy recognizes named entities in a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MexY92pl5sT6",
        "outputId": "f8651bd9-819c-43e6-effc-f7416e375bcb"
      },
      "source": [
        "st2 = \"Indians spent over $71 billion on clothes in 2018, by PM Modi in India\"\n",
        "\n",
        "\n",
        "doc = nlp(st2)\n",
        " \n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "    print(\"{} : {}\".format(ent.label_, spacy.explain(ent.label_)))\n",
        "    print(\"---\"*10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indians NORP\n",
            "NORP : Nationalities or religious or political groups\n",
            "------------------------------\n",
            "$71 billion MONEY\n",
            "MONEY : Monetary values, including unit\n",
            "------------------------------\n",
            "2018 DATE\n",
            "DATE : Absolute or relative dates or periods\n",
            "------------------------------\n",
            "PM Modi PERSON\n",
            "PERSON : People, including fictional\n",
            "------------------------------\n",
            "India GPE\n",
            "GPE : Countries, cities, states\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S8gWy478jwL"
      },
      "source": [
        "### Rule-Based Matching using spaCy\n",
        "Rule-based matching is a new addition to spaCy’s arsenal. With this spaCy matcher, you can find words and phrases in the text using user-defined rules.\n",
        "\n",
        "While Regular Expressions use text patterns to find words and phrases, the spaCy matcher not only uses the text patterns but lexical properties of the word, such as POS tags, dependency tags, lemma, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqeqeBNi9EKL",
        "outputId": "92ee52f5-627a-4d11-80f3-f3148184173c"
      },
      "source": [
        "nlp.vocab[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lexeme.Lexeme at 0x7fd48a238c30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tEOSR-L5sPq"
      },
      "source": [
        "# import spacy\n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Import spaCy Matcher\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Initialize the matcher with the spaCy vocabulary\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "\n",
        "st3 = \"Some people start their day with lemon water\"\n",
        "st4 = \"Some people start their day with lemon water, lemon water is soft drink.\"\n",
        "\n",
        "doc = nlp(st3 )\n",
        "doc1 = nlp(st4)\n",
        "\n",
        "# Define rule\n",
        "pattern = [{'TEXT': 'lemon'}, {'TEXT': 'water'}]\n",
        "\n",
        "# Add rule\n",
        "matcher.add('rule_1', None, pattern)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPcatJsv9y2b"
      },
      "source": [
        "in the code above:\n",
        "\n",
        "First, we import the spaCy matcher\n",
        "After that, we initialize the matcher object with the default spaCy vocabulary\n",
        "Then, we pass the input in an NLP object as usual\n",
        "In the next step, we define the rule/pattern for what we want to extract from the text.\n",
        "Let’s say we want to extract the phrase “lemon water” from the text. So, our objective is that whenever “lemon” is followed by the word “water”, then the matcher should be able to find this pattern in the text. That’s exactly what we have done while defining the pattern in the code above. Finally, we add the defined rule to the matcher object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llyWSQf991Zz",
        "outputId": "4b1b00be-b524-4137-9de4-3466c366b3ef"
      },
      "source": [
        "matches = matcher(doc)  #The output has three elements. The first element is the match ID. \n",
        "# The second and third elements are the positions of the matched tokens.\n",
        "matches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(7604275899133490726, 6, 8)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGBwML9p_HNU",
        "outputId": "df773859-4a0e-44c6-ba5a-2562192df358"
      },
      "source": [
        "# Extract matched text\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lemon water\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDVa0WID-l5D",
        "outputId": "2e3ed70d-a81c-46f9-b62d-984cf7b46dad"
      },
      "source": [
        "matches = matcher(doc1)  \n",
        "matches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(7604275899133490726, 6, 8), (7604275899133490726, 9, 11)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzmicwdK-EF8",
        "outputId": "675986bd-b4e2-4405-f28d-8691a2719eca"
      },
      "source": [
        "# Extract matched text\n",
        "for match_id, start, end in matches:\n",
        "  print(match_id, start, end)\n",
        "  # Get the matched span\n",
        "  matched_span = doc1[start:end]\n",
        "  print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7604275899133490726 6 8\n",
            "lemon water\n",
            "7604275899133490726 9 11\n",
            "lemon water\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjSzSRY-_mTE"
      },
      "source": [
        " For example, ‘TEXT’ is a token attribute that means the exact text of the token. There are, in fact, many other useful token attributes in spaCy which can be used to define a variety of rules and patterns. [https://spacy.io/usage/rule-based-matching](https://spacy.io/usage/rule-based-matching)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifID-l2H_nVc"
      },
      "source": [
        "doc1 = nlp(\"You read this book\")\n",
        "doc2 = nlp(\"I will book my ticket\")\n",
        "\n",
        "pattern = [{'TEXT': 'book', 'POS': 'NOUN'}]  # when Book word is noun only\n",
        "\n",
        "# Initialize the matcher with the shared vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add('rule_2', None, pattern)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLl_Qo5XAP6c",
        "outputId": "5ddd2a88-c744-447f-9853-d5879d70fce7"
      },
      "source": [
        "matches = matcher(doc1)\n",
        "matches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(375134486054924901, 3, 4)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T__ltwdLASe0",
        "outputId": "afbe013b-6047-4606-e978-0e0c0e6ad266"
      },
      "source": [
        "matches = matcher(doc2)  # word book present but it ignore because it is not noun\n",
        "matches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg9OxkTiUa9C"
      },
      "source": [
        "# give your filename here\n",
        "with open(\"file.txt\", \"r\") as fp:\n",
        "  text = fp.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "_kMi6LtNWTjp",
        "outputId": "58caf25e-72a0-43a6-844b-da8871e0ee7e"
      },
      "source": [
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\nSharad S Sawant \\nRESUME SAMPLE \\n\\nfrom Resume Genius \\n\\nEmail:  youremail@gmail.com \\n\\nPhone: 895 555  555 \\n\\nAddress: 4397 Aaron Smith  Drive \\nHarrisburg, PA  17101 \\n\\nLinkedin:  linkedin.com/in/yourprole \\n\\nR E S U M E  O B J E CT I V E \\n\\nHuman Resources Generalist with 6+ years of experience assisting with and fulfilling organization staffing needs \\n\\nand requirements. Aiming to use my dynamic communication and organization skills to achieve your HR \\n\\ninitiatives. Possess a BA in Human Resources Management and a Professional in Human Resources certification. \\n\\n \\n\\n \\nSKILL\\n \\nS \\n \\n\\n90WPM Typing Speed  Workday \\n\\nKronos \\n\\nConflict Management \\n\\nMS Office Suite \\n\\nTeamwork \\n\\n \\n\\nLeadership \\n\\nTime Management \\n\\nInterpersonal Communication \\n\\nAdaptability. \\n\\nPublic Speaking \\n\\nEXPERIENCE \\nHR GENERALIST \\nMeadow Laboratories, Chicago, IL  /  September 2014 - Present \\n\\n \\n\\n \\n\\nImplemented effective HR policies to ensure all practices are in compliance with labor and \\nemployment regulations \\n\\nIncreased employee retention rates above 90% by creating and maintaining a positive work \\nenvironment \\n\\n  Developed targeted outreach recruitment programs to recruit more minorities and meet affirmative \\n\\naction requirements \\n\\n  Created a website with an embedded database and FTP functionality to enable online recruitment \\n\\nwhich has resulted in a 10% reduction in recruitment costs for the organization \\n\\n  Manage travel and expense reports for department team members \\n\\nHUMAN RESOURCES INTERN \\nTrenton Childrens Hospital, Boston, MA  /  June 2012  August 2014 \\n\\n  Advised prospective employees on various tips and tricks that would assist them in gaining \\n\\nemployment with the organization \\n\\n  Conducted several seminars for hospital employees to update them on employee benefit options \\n\\n \\n\\nInvited 10+ motivational speakers and industry experts to give lectures and speeches to employees \\non new industry standards and how to build confidence and morale in the workplace \\n\\n  Updated 100+ employee records and job assignments daily \\n\\nEDUCATION \\n\\nB.A. H.R. MANAGEMENT  \\n\\nA.S. H.R. MANAGEMENT  \\n\\nCERTIFICATION \\n\\nMEMBER \\n\\nMiami University, 2012  \\n\\nMiami University, 2010  \\n\\nDear Job Seeker, \\n\\nMiami, FL \\n\\nMiami, FL \\n\\nProfessional in Human \\nResources \\n\\nSociety for Human \\nResource Management \\n\\n \\n\\n \\n\\n \\nPLEASE note that you *must* install the font files to make this resume template work. \\nIts very simple to do  just follow the instructions in the Read Me file in the Zip file \\nyou downloaded this template from. \\n \\nIf youre struggling to write your resume, dont worry. Youre in good company  \\neveryone struggles with it. For a high quality resume that will land you employment, \\nwe recommend consulting the certified experts at Resume Writer Direct. \\n \\nOr, heres some other content that might help you finish your resume. \\n \\n\\n  Free Resume Builder \\n  How to Write a Resume \\n  Resume Samples by Industry \\n\\n \\nOh, and by the way, youre also going to need a cover letter. \\n \\n\\n  Cover Letter Builder \\n  How to Write a Cover Letter \\n  Cover Letter Examples by Industry \\n\\n \\n \\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSn9RFRuX_ky"
      },
      "source": [
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkP2UbgSXMXi",
        "outputId": "c6637365-53ca-4d66-b991-1500efca343b"
      },
      "source": [
        "for ent in doc.ents:\n",
        "    if ent.label_==\"PERSON\":\n",
        "      print(\"{}, {}({})\".format(ent.text, ent.label_, spacy.explain(ent.label_)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aaron Smith, PERSON(People, including fictional)\n",
            "Zip, PERSON(People, including fictional)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3Lzh3mDZP0y",
        "outputId": "91e1a2e7-f5e4-4939-805f-d558f780b9a3"
      },
      "source": [
        "for ent in doc.ents:\n",
        "    if ent.label_==\"GPE\":\n",
        "      print(\"{}, {}\".format(ent.text, ent.label_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "India, GPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnohGxNwWder",
        "outputId": "29eb5ed2-0187-48fd-9c03-ecba4ea95089"
      },
      "source": [
        "\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "\n",
        "pattern = [{\"TEXT\": {\"REGEX\": r\"[\\w.-]+@[\\w.-]+.[\\w.-]+\"}} ]\n",
        "matcher.add( \"email\", None, pattern)\n",
        "\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]\n",
        "    if string_id==\"email\":\n",
        "      span = doc[start:end]\n",
        "      print(match_id, string_id, start, end)\n",
        "      print(\"Emsil ID : \", span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7320900731437023467 email 15 16\n",
            "Emsil ID :  youremail@gmail.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIRhHrRwRA8h",
        "outputId": "841c73f2-5f4b-4798-f6fa-8b6182ced6fc"
      },
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"TEXT\": {\"REGEX\": r\"[\\w.-]+@[\\w.-]+.[\\w.-]+\"}} ]\n",
        "matcher.add( \"email\", None, pattern)\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]\n",
        "    span = doc[start:end]\n",
        "    print(match_id, string_id, start, end)\n",
        "    print( span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7320900731437023467 email 15 16\n",
            "youremail@gmail.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90gk2bUdPfLv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgqCvGe6PfJA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6uhUsTvZ9rK"
      },
      "source": [
        "# Regular Expressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFro6NShPfGI",
        "outputId": "309955f7-9e8c-4a59-ca34-374fa45d6370"
      },
      "source": [
        "import re\n",
        "\n",
        "# insert your text here\n",
        "text = \"my email id is anaji17@gmail.com and old email ak@gmail.com\"\n",
        "\n",
        "re.findall(r\"[\\w.-]+@[\\w.-]+\", text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anaji17@gmail.com', 'ak@gmail.com']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f44kqEvxUCAZ",
        "outputId": "0888a7c2-70ea-418b-9cea-05d145ce03bb"
      },
      "source": [
        "# give your filename here\n",
        "with open(\"file.txt\", \"r\") as fp:\n",
        "  text = fp.read()\n",
        "  \n",
        "re.findall(r\"[\\w.-]+@[\\w.-]+\", text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['youremail@gmail.com']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qNmWiA3aizk"
      },
      "source": [
        "## Regular Expressions for Web Scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRdoJfA1aPR7",
        "outputId": "7feaae2e-1366-4304-e3ce-f80a242b8e69"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "\n",
        "URL = \"https://en.wikipedia.org/wiki/Unsupervised_learning\"\n",
        "\n",
        "r = requests.get(URL)\n",
        "soup = BeautifulSoup(r.content, \"html\")\n",
        "type(soup)\n",
        "# for text in soup.body.find_all(string=True):\n",
        "#     if text.parent.name not in ['script', 'meta', 'link', 'style'] and not isinstance(text, Comment) and text != '\\n':\n",
        "#         print(text.strip())\n",
        "\n",
        "# re.findall(r\"https:+[\\w.-]+.jpg+\", soup)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bs4.BeautifulSoup"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVhiu3DherXU",
        "outputId": "8a20c55c-a0d4-4583-f0dd-d3a5dcf17a77"
      },
      "source": [
        "re.findall(r\"\\/wiki\\/[\\w-]*\",  str(soup))[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/wiki/Unsupervised_learning',\n",
              " '/wiki/Unsupervised_learning',\n",
              " '/wiki/Machine_learning',\n",
              " '/wiki/Data_mining',\n",
              " '/wiki/File']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YytnNmu3ehBi",
        "outputId": "18435261-c1a5-4224-c71c-09f8dfc3fa07"
      },
      "source": [
        "re.findall(r\">([\\w\\s()]*?)</a>\", str(soup))[:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', 'Jump to navigation', 'Jump to search', 'Machine learning']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSgPqZMjf1Zt"
      },
      "source": [
        "## Working with Date-Time features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63hOY9zpby8T",
        "outputId": "a33b7e21-9803-454d-aca6-6c78a9f60071"
      },
      "source": [
        "date = \"2018-03-14 06:08:18\"\n",
        "re.findall(r\"\\d{4}\", date)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2018']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvJyQFUxby5r",
        "outputId": "e00b45fe-2b26-43fe-9315-5ad2f292eb7a"
      },
      "source": [
        "re.findall(r\"(\\d{4})-(\\d{2})-(\\d{2})\", date)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2018', '03', '14')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozT_r-MRby2s",
        "outputId": "7e91adf1-529f-4f1f-cd48-c1d40f449fd2"
      },
      "source": [
        "d = \"12th September, 2019\"\n",
        "\n",
        "re.findall(r\"(\\d{2})\\w+\\s(\\w+),\\s(\\d{4})\", d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('12', 'September', '2019')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJyrlpfRgtQu"
      },
      "source": [
        "[https://docs.python.org/3/howto/regex.html](https://docs.python.org/3/howto/regex.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj2KgeLzg4ts"
      },
      "source": [
        "[http://www.rexegg.com/regex-quickstart.html#ref](http://www.rexegg.com/regex-quickstart.html#ref)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q5qSM6-gihl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI4UlD-ygWws"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# download stopwords list from nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    # converting to lowercase\n",
        "    newString = text.lower()\n",
        "    # removing links\n",
        "    newString = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', newString) \n",
        "    # fetching alphabetic characters\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "    # removing stop words\n",
        "    tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    # removing short words\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>=4:                                                 \n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pEfrwWNgWuh"
      },
      "source": [
        "# removing links\n",
        "newString = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', newString)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfJBaEe8gWsM"
      },
      "source": [
        "# fetching alphabetic characters\n",
        "newString = re.sub(\"[^a-zA-Z]\", \" \", newString)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0wzTFwbgWpl"
      },
      "source": [
        " # removing stop words\n",
        "tokens = [w for w in newString.split() if not w in stop_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS6NjWT4gWmy"
      },
      "source": [
        "# removing short words\n",
        "long_words=[]\n",
        "for i in tokens:\n",
        "    if len(i)>=4: \n",
        "        long_words.append(i)\n",
        "return (\" \".join(long_words)).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrYRVMghgy6E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}