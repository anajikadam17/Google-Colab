{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1JYfr1wc0-584jLvEbeQTagG0bPjdTfrW",
      "authorship_tag": "ABX9TyMoJhbCwPAQCdQPpGocjGia",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anajikadam17/Google-Colab/blob/main/CNN/Emotion_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYg4iUnpcz-a"
      },
      "source": [
        "# Emotion_Detection\n",
        "\n",
        "Dataset [Link](https://www.kaggle.com/jangedoo/utkface-new)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE9O06-LcwWK"
      },
      "source": [
        "path1 = r\"/content/drive/MyDrive/Colab/Kaggle/UTKFace\"\n",
        "path2 = r\"/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL5TvhPydlhR",
        "outputId": "923999fa-eb04-4a08-b03c-39231aed0fa7"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "files=os.listdir(path2)\n",
        "print(files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['happy', 'anger', 'sadness', 'disgust', 'contempt', 'surprise', 'fear']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9JYDr1Cdr3K"
      },
      "source": [
        "Exp=['fear', 'contempt', 'happy', 'anger', 'surprise', 'disgust', 'sadness']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS17Vh2odw9x",
        "outputId": "678417df-1de8-4ffc-868f-8d536b299aa8"
      },
      "source": [
        "i=0\n",
        "last=[]\n",
        "images=[]\n",
        "labels=[]\n",
        "for fle in files:\n",
        "  idx=Exp.index(fle)\n",
        "  label=idx\n",
        "  \n",
        "  total = path2 + '/'+fle\n",
        "  files_exp= os.listdir(total)\n",
        "\n",
        "  for fle_2 in files_exp:\n",
        "    file_main=total+'/'+fle_2\n",
        "    print(file_main+\"   \"+str(label))\n",
        "    image= cv2.imread(file_main)\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image= cv2.resize(image,(48,48))\n",
        "    images.append(image)\n",
        "    labels.append(label)\n",
        "    i+=1\n",
        "  last.append(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S061_002_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S075_006_00000025.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S032_006_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S011_006_00000012.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S052_004_00000033.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S136_006_00000020.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S108_008_00000012.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S056_004_00000018.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S070_003_00000016.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S130_013_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S098_004_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S116_007_00000017.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S064_003_00000024.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S133_010_00000012.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S068_002_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S067_005_00000021.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S010_006_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S128_011_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S116_007_00000016.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S125_005_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S055_005_00000043.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S062_004_00000024.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S109_006_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S114_006_00000021.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S071_005_00000020.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S052_004_00000032.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S089_002_00000021.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S069_004_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S064_003_00000023.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S037_006_00000019.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S086_002_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S135_012_00000018.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S128_011_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S128_011_00000016.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S127_004_00000016.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S108_008_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S092_004_00000023.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S138_005_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S138_005_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S072_006_00000022.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S068_002_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S065_004_00000028.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S091_003_00000019.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S074_005_00000042.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S093_004_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S032_006_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S137_011_00000019.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S095_007_00000019.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S060_002_00000026.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S131_006_00000021.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S115_008_00000016.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S083_003_00000018.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S099_004_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S075_006_00000023.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S066_003_00000012.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S087_005_00000012.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S071_005_00000019.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S067_005_00000020.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S042_006_00000016.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S076_006_00000018.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S066_003_00000011.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S137_011_00000018.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S057_006_00000031.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S136_006_00000018.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S069_004_00000016.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S099_004_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S072_006_00000021.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S050_006_00000023.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S062_004_00000022.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S130_013_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S129_012_00000009.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S066_003_00000010.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S067_005_00000022.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S069_004_00000017.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S055_005_00000045.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S061_002_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S109_006_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S093_004_00000016.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S060_002_00000025.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S052_004_00000031.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S068_002_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S050_006_00000021.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S100_006_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S014_005_00000017.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S125_005_00000012.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S138_005_00000016.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S099_004_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S065_004_00000027.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S072_006_00000020.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S125_005_00000011.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S097_006_00000019.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S063_002_00000022.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S131_006_00000022.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S097_006_00000018.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S026_006_00000012.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S078_004_00000027.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S010_006_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S087_005_00000010.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S124_007_00000022.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S042_006_00000017.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S034_005_00000008.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S035_006_00000017.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S135_012_00000020.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S134_004_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S135_012_00000019.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S086_002_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S133_010_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S096_004_00000010.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S114_006_00000023.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S095_007_00000020.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S136_006_00000019.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S086_002_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S109_006_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S079_004_00000026.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S044_003_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S056_004_00000019.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S056_004_00000020.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S092_004_00000024.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S078_004_00000025.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S124_007_00000023.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S133_010_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S098_004_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S137_011_00000020.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S089_002_00000019.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S076_006_00000017.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S063_002_00000023.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S129_012_00000010.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S014_005_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S034_005_00000009.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S053_004_00000023.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S091_003_00000020.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S026_006_00000011.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S098_004_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S071_005_00000021.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S087_005_00000011.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S074_005_00000041.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S091_003_00000021.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S035_006_00000016.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S070_003_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S106_006_00000010.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S108_008_00000011.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S127_004_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S075_006_00000024.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S132_006_00000023.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S116_007_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S035_006_00000018.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S131_006_00000020.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S115_008_00000017.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S062_004_00000023.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S063_002_00000021.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S083_003_00000019.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S115_008_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S064_003_00000025.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S097_006_00000017.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S050_006_00000022.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S044_003_00000012.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S037_006_00000020.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S011_006_00000011.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S011_006_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S130_013_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S055_005_00000044.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S060_002_00000024.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S061_002_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S092_004_00000022.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S094_004_00000010.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S114_006_00000022.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S079_004_00000025.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S026_006_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S044_003_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S057_006_00000032.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S078_004_00000026.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S037_006_00000021.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S085_002_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S074_005_00000043.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S010_006_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S042_006_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S134_004_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S100_006_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S129_012_00000011.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S096_004_00000009.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S134_004_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S085_002_00000012.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S094_004_00000011.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S070_003_00000017.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S053_004_00000022.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S127_004_00000015.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S132_006_00000022.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S093_004_00000014.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S085_002_00000013.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S057_006_00000033.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S014_005_00000016.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S124_007_00000024.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S053_004_00000024.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S132_006_00000021.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S034_005_00000010.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S106_006_00000011.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S100_006_00000016.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S096_004_00000011.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S079_004_00000024.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S032_006_00000016.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S089_002_00000020.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S095_007_00000021.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S065_004_00000026.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S076_006_00000019.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S106_006_00000009.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S094_004_00000012.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/happy/S083_003_00000017.png   2\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S011_004_00000021.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S010_004_00000017.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S011_004_00000020.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S011_004_00000019.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S010_004_00000019.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S010_004_00000018.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S014_003_00000028.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S127_010_00000017.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S127_010_00000018.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S501_001_00000066.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S042_004_00000019.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S071_004_00000026.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S029_001_00000018.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S506_001_00000040.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S022_005_00000030.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S506_001_00000038.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S504_001_00000020.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S072_005_00000018.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S112_005_00000017.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S034_003_00000026.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S136_005_00000008.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S113_008_00000022.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S037_003_00000022.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S119_008_00000017.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S126_008_00000028.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S502_001_00000014.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S130_007_00000018.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S045_005_00000028.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S504_001_00000022.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S092_003_00000013.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S111_006_00000010.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S066_005_00000011.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S050_004_00000019.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S130_007_00000020.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S134_003_00000010.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S067_004_00000023.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S075_008_00000010.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S014_003_00000030.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S066_005_00000010.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S134_003_00000009.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S032_003_00000017.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S026_003_00000013.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S034_003_00000027.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S071_004_00000027.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S066_005_00000009.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S090_007_00000014.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S133_003_00000046.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S029_001_00000017.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S129_006_00000010.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S999_001_00000016.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S075_008_00000012.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S034_003_00000025.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S503_001_00000070.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S112_005_00000016.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S045_005_00000030.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S090_007_00000012.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S028_001_00000023.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S109_003_00000015.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S082_005_00000015.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S113_008_00000023.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S055_004_00000028.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S126_008_00000027.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S129_006_00000009.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S999_001_00000018.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S072_005_00000019.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S111_006_00000008.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S022_005_00000031.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S087_007_00000015.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S082_005_00000016.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S100_005_00000021.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S501_001_00000067.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S037_003_00000021.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S100_005_00000022.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S092_003_00000014.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S111_006_00000009.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S045_005_00000029.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S503_001_00000071.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S100_005_00000023.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S117_006_00000009.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S026_003_00000014.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S032_003_00000016.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S109_003_00000016.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S119_008_00000016.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S014_003_00000029.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S129_006_00000008.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S130_007_00000019.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S503_001_00000069.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S109_003_00000017.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S092_003_00000012.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S050_004_00000020.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S502_001_00000016.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S050_004_00000021.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S126_008_00000029.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S113_008_00000021.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S136_005_00000010.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S089_003_00000035.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S090_007_00000013.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S087_007_00000016.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S072_005_00000017.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S055_004_00000026.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S082_005_00000017.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S089_003_00000036.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S117_006_00000010.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S133_003_00000045.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S058_005_00000009.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S133_003_00000047.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S999_001_00000017.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S028_001_00000022.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S087_007_00000014.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S029_001_00000019.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S127_010_00000016.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S136_005_00000009.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S071_004_00000028.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S022_005_00000032.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S117_006_00000008.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S506_001_00000039.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S501_001_00000065.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S028_001_00000024.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S067_004_00000021.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S504_001_00000021.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S042_004_00000020.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S037_003_00000020.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S042_004_00000018.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S067_004_00000022.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S075_008_00000011.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S055_004_00000027.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S026_003_00000015.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S119_008_00000018.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S058_005_00000010.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S112_005_00000015.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S502_001_00000015.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S089_003_00000034.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S032_003_00000015.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S058_005_00000008.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/anger/S134_003_00000011.png   3\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S503_006_00000018.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S113_003_00000015.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S131_003_00000022.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S130_009_00000017.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S108_005_00000020.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S046_001_00000023.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S108_005_00000021.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S066_004_00000010.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S138_007_00000010.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S506_006_00000042.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S506_006_00000040.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S081_002_00000024.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S026_002_00000016.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S504_006_00000017.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S125_001_00000012.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S504_006_00000018.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S505_006_00000019.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S071_002_00000019.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S137_005_00000025.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S130_009_00000019.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S136_003_00000013.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S503_006_00000019.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S066_004_00000009.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S014_002_00000015.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S115_004_00000016.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S501_006_00000040.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S137_005_00000026.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S137_005_00000027.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S136_003_00000014.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S042_002_00000016.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S014_002_00000016.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S046_001_00000024.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S132_002_00000016.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S125_001_00000013.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S026_002_00000015.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S115_004_00000015.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S138_007_00000011.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S093_001_00000020.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S080_005_00000012.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S011_002_00000021.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S042_002_00000015.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S106_002_00000015.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S042_002_00000014.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S125_001_00000014.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S106_002_00000014.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S064_004_00000012.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S108_005_00000022.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S501_006_00000039.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S014_002_00000014.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S081_002_00000023.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S506_006_00000041.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S071_002_00000020.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S115_004_00000017.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S113_003_00000013.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S106_002_00000016.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S081_002_00000022.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S095_010_00000013.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S080_005_00000013.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S093_001_00000019.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S505_006_00000017.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S504_006_00000016.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S080_005_00000011.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S131_003_00000024.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S130_009_00000018.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S095_010_00000014.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S138_007_00000009.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S026_002_00000014.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S113_003_00000014.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S066_004_00000008.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S011_002_00000022.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S136_003_00000012.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S132_002_00000018.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S132_002_00000017.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S503_006_00000020.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S501_006_00000041.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S505_006_00000018.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S131_003_00000023.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S046_001_00000025.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S095_010_00000012.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S064_004_00000014.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S011_002_00000020.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S071_002_00000018.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S093_001_00000018.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/sadness/S064_004_00000013.png   6\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S131_010_00000018.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S098_003_00000013.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S069_003_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S099_007_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S032_005_00000016.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S080_008_00000007.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S129_011_00000017.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S044_006_00000017.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S052_006_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S032_005_00000015.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S067_006_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S109_005_00000014.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S079_002_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S087_004_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S044_006_00000018.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S097_004_00000030.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S078_007_00000013.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S116_006_00000006.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S075_005_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S068_005_00000019.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S068_005_00000020.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S088_004_00000019.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S134_008_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S065_005_00000007.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S046_004_00000016.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S095_006_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S128_004_00000013.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S011_005_00000020.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S074_004_00000018.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S085_004_00000017.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S105_008_00000008.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S130_012_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S052_006_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S088_004_00000018.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S108_006_00000018.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S074_004_00000017.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S060_005_00000020.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S057_003_00000013.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S067_006_00000009.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S051_003_00000017.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S069_003_00000009.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S076_005_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S099_007_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S075_005_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S058_006_00000018.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S081_008_00000009.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S129_011_00000016.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S060_005_00000021.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S061_004_00000020.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S057_003_00000015.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S087_004_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S096_003_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S111_007_00000014.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S061_004_00000022.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S056_002_00000008.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S080_008_00000008.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S116_006_00000005.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S065_005_00000008.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S131_010_00000017.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S105_008_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S087_004_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S073_006_00000013.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S109_005_00000013.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S022_006_00000017.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S102_009_00000014.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S090_006_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S005_001_00000009.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S067_006_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S070_005_00000016.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S060_005_00000019.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S079_002_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S095_006_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S046_004_00000015.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S107_005_00000009.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S054_004_00000023.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S058_006_00000016.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S125_008_00000008.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S130_012_00000009.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S035_005_00000017.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S076_005_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S078_007_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S105_008_00000009.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S054_004_00000022.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S111_007_00000013.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S078_007_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S081_008_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S098_003_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S090_006_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S088_004_00000020.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S032_005_00000014.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S052_006_00000013.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S071_006_00000013.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S055_003_00000008.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S055_003_00000007.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S096_003_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S132_005_00000014.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S099_007_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S062_005_00000027.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S057_003_00000014.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S082_007_00000008.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S062_005_00000029.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S128_004_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S134_008_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S081_008_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S107_005_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S070_005_00000015.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S022_006_00000016.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S098_003_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S096_003_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S082_007_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S077_006_00000014.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S077_006_00000013.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S051_003_00000018.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S080_008_00000009.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S124_006_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S055_003_00000009.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S106_004_00000008.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S075_005_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S124_006_00000009.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S035_005_00000019.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S045_004_00000015.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S085_004_00000016.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S108_006_00000019.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S085_004_00000015.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S035_005_00000018.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S082_007_00000009.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S005_001_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S011_005_00000018.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S069_003_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S134_008_00000013.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S071_006_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S073_006_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S056_002_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S102_009_00000015.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S132_005_00000015.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S062_005_00000028.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S022_006_00000015.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S090_006_00000009.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S107_005_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S097_004_00000028.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S128_004_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S095_006_00000013.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S125_008_00000009.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S070_005_00000014.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S071_006_00000014.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S079_002_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S074_004_00000016.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S102_009_00000013.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S129_011_00000018.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S108_006_00000020.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S130_012_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S045_004_00000013.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S005_001_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S106_004_00000006.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S054_004_00000024.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S011_005_00000019.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S056_002_00000009.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S076_005_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S058_006_00000017.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S125_008_00000010.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S106_004_00000007.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S111_007_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S077_006_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S109_005_00000012.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S132_005_00000016.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S131_010_00000016.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S044_006_00000019.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S061_004_00000021.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S073_006_00000014.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S097_004_00000029.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S124_006_00000011.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S051_003_00000016.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S046_004_00000017.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S065_005_00000006.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S045_004_00000014.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S116_006_00000007.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/disgust/S068_005_00000021.png   5\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S155_002_00000011.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S139_002_00000011.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S151_002_00000029.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S160_006_00000008.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S505_002_00000021.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S149_002_00000013.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S149_002_00000011.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S502_002_00000009.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S504_002_00000008.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S151_002_00000027.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S505_002_00000020.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S504_002_00000009.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S138_008_00000009.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S506_002_00000008.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S895_002_00000007.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S503_002_00000007.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S158_002_00000011.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S139_002_00000012.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S157_002_00000011.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S156_002_00000019.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S160_006_00000009.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S154_002_00000011.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S147_002_00000013.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S148_002_00000014.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S157_002_00000010.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S139_002_00000013.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S160_006_00000010.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S895_002_00000005.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S138_008_00000008.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S502_002_00000007.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S895_002_00000006.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S506_002_00000009.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S154_002_00000012.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S154_002_00000013.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S503_002_00000006.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S149_002_00000012.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S155_002_00000012.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S505_002_00000019.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S148_002_00000013.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S157_002_00000009.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S502_002_00000008.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S156_002_00000021.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S147_002_00000011.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S155_002_00000010.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S138_008_00000007.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S156_002_00000020.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S147_002_00000012.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S506_002_00000007.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S158_002_00000010.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S503_002_00000008.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S151_002_00000028.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S148_002_00000015.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S158_002_00000009.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/contempt/S504_002_00000007.png   1\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S034_001_00000028.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S037_001_00000020.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S056_003_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S022_001_00000029.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S026_001_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S032_001_00000021.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S026_001_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S055_001_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S014_001_00000029.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S060_003_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S032_001_00000020.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S010_002_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S063_001_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S056_003_00000008.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S046_002_00000006.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S046_002_00000004.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S014_001_00000027.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S062_002_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S055_001_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S035_001_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S035_001_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S034_001_00000029.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S042_001_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S010_002_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S042_001_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S060_003_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S022_001_00000030.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S044_001_00000022.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S044_001_00000024.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S051_002_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S058_001_00000019.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S042_001_00000019.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S060_003_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S055_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S051_002_00000019.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S052_001_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S058_001_00000020.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S053_001_00000021.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S037_001_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S014_001_00000028.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S061_001_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S032_001_00000022.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S046_002_00000005.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S057_001_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S059_001_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S059_001_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S056_003_00000009.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S050_002_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S011_001_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S011_001_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S022_001_00000028.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S034_001_00000027.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S050_002_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S057_001_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S054_003_00000005.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S010_002_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S051_002_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S052_001_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S058_001_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S057_001_00000019.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S061_001_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S059_001_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S062_002_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S054_003_00000006.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S037_001_00000019.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S062_002_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S050_002_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S035_001_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S044_001_00000023.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S011_001_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S054_003_00000007.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S052_001_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S053_001_00000023.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S063_001_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S053_001_00000022.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S026_001_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S063_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S061_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S066_002_00000022.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S097_001_00000020.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S081_001_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S122_001_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S124_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S069_002_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S065_003_00000022.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S130_001_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S137_001_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S125_007_00000007.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S117_001_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S075_002_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S087_001_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S114_001_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S084_001_00000008.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S137_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S088_001_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S114_001_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S129_002_00000009.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S085_003_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S085_003_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S138_004_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S069_002_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S089_001_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S124_001_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S107_001_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S081_001_00000019.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S116_001_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S116_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S097_001_00000019.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S115_001_00000008.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S113_001_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S099_001_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S126_004_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S111_001_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S071_001_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S133_009_00000004.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S079_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S076_001_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S074_002_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S095_001_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S087_001_00000009.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S074_002_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S070_002_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S113_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S132_008_00000009.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S076_001_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S107_001_00000009.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S090_002_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S077_001_00000026.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S064_001_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S119_001_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S089_001_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S131_001_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S073_001_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S119_001_00000009.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S078_001_00000032.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S066_002_00000020.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S101_002_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S067_002_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S135_001_00000039.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S080_001_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S124_001_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S090_002_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S088_001_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S122_001_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S138_004_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S067_002_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S136_001_00000019.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S110_001_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S122_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S117_001_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S094_001_00000008.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S065_003_00000021.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S088_001_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S064_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S080_001_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S133_009_00000006.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S135_001_00000037.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S077_001_00000028.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S099_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S066_002_00000021.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S071_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S114_001_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S073_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S067_002_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S111_001_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S136_001_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S126_004_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S078_001_00000031.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S073_001_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S095_001_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S127_001_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S115_001_00000006.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S096_001_00000005.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S095_001_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S094_001_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S126_004_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S079_001_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S102_002_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S129_002_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S070_002_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S097_001_00000021.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S086_001_00000019.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S131_001_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S117_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S113_001_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S096_001_00000006.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S082_001_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S096_001_00000007.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S078_001_00000033.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S069_002_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S107_001_00000008.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S130_001_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S099_001_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S084_001_00000009.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S086_001_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S100_002_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S068_003_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S084_001_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S115_001_00000007.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S133_009_00000005.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S074_002_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S130_001_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S127_001_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S064_001_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S137_001_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S101_002_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S065_003_00000020.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S070_002_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S119_001_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S092_001_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S081_001_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S102_002_00000018.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S100_002_00000014.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S075_002_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S090_002_00000009.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S129_002_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S132_008_00000010.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S136_001_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S077_001_00000027.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S132_008_00000008.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S080_001_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S125_007_00000008.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S086_001_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S101_002_00000019.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S082_001_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S094_001_00000009.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S116_001_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S127_001_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S138_004_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S071_001_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S082_001_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S079_001_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S100_002_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S131_001_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S089_001_00000015.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S111_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S075_002_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S135_001_00000038.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S087_001_00000011.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S110_001_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S068_003_00000013.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S125_007_00000009.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S092_001_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S076_001_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S102_002_00000017.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S085_003_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S092_001_00000016.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S110_001_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/surprise/S068_003_00000012.png   4\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S124_003_00000009.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S501_004_00000056.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S084_002_00000023.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S102_003_00000014.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S119_003_00000023.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S055_006_00000007.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S117_003_00000014.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S062_001_00000017.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S059_002_00000016.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S046_003_00000016.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S032_004_00000013.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S065_002_00000022.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S091_001_00000015.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S117_003_00000013.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S054_002_00000015.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S065_002_00000021.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S125_006_00000020.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S050_001_00000016.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S055_006_00000006.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S124_003_00000011.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S011_003_00000012.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S117_003_00000012.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S502_004_00000050.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S132_003_00000022.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S124_003_00000010.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S502_004_00000052.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S504_004_00000015.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S132_003_00000021.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S084_002_00000022.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S501_004_00000054.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S504_004_00000014.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S504_004_00000013.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S074_001_00000018.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S068_004_00000009.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S062_001_00000016.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S074_001_00000019.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S138_001_00000010.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S125_006_00000022.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S074_001_00000020.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S138_001_00000012.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S506_004_00000038.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S999_003_00000054.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S132_003_00000023.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S046_003_00000014.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S119_003_00000022.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S119_003_00000024.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S046_003_00000015.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S011_003_00000014.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S084_002_00000021.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S506_004_00000036.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S059_002_00000017.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S102_003_00000016.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S138_001_00000011.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S032_004_00000012.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S125_006_00000021.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S102_003_00000015.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S091_001_00000013.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S054_002_00000014.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S059_002_00000015.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S501_004_00000055.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S999_003_00000053.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S068_004_00000010.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S054_002_00000013.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S032_004_00000014.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S011_003_00000013.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S506_004_00000037.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S065_002_00000020.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S999_003_00000055.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S055_006_00000008.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S502_004_00000051.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S091_001_00000014.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S050_001_00000017.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S050_001_00000015.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S068_004_00000008.png   0\n",
            "/content/drive/MyDrive/Colab/DL/CKPLUS/CK+48/fear/S062_001_00000015.png   0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yt8sCLWeWAO"
      },
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejgzuGlieV9f",
        "outputId": "dec78e41-0d9f-4d09-c5c6-0fcab33d29e3"
      },
      "source": [
        "print(last)\n",
        "folder_list =  ['happy', 'anger', 'sadness', 'disgust', 'contempt', 'surprise', 'fear']\n",
        "# Exp=['fear', 'contempt', 'happy', 'anger', 'surprise', 'disgust', 'sadness']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[207, 342, 426, 603, 657, 906, 981]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoZesenmn2Ux",
        "outputId": "182fdd92-a86f-453d-afae-fb4846e02780"
      },
      "source": [
        "idx = last.index(207)\n",
        "print(folder_list[idx])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Xf7lWvf8mrhI",
        "outputId": "96dcec3a-19fd-4e37-ef0f-4bd45dd3da20"
      },
      "source": [
        "for i in last:\n",
        "  idx = last.index(i)\n",
        "  print(folder_list[idx])\n",
        "  cv2_imshow(images[i-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "happy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7F2AE6D510>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "anger\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7F2AE6D490>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "sadness\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7F2ADD9690>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "disgust\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7F2ADD94D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "contempt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7F2ADD9910>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "surprise\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7F2ADD9A50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "fear\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7F2ADD9690>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGTXUML6exyP"
      },
      "source": [
        "# Fear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "id": "yufHvSsleV7A",
        "outputId": "a248d6e3-9546-4007-e9e0-0b247bc1a70f"
      },
      "source": [
        "cv2_imshow(images[980])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7F35B32690>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "id": "zMsHiJYJfBKX",
        "outputId": "1dedb445-f69e-4e81-9706-8ed8d8df2e46"
      },
      "source": [
        "cv2_imshow(images[910])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7F35B32190>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-tQ9_26e4Cf"
      },
      "source": [
        "# Contempt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "id": "vbAv2dbmeV4I",
        "outputId": "2c53cab4-acd1-4721-bed0-ac644d8e7bb3"
      },
      "source": [
        "cv2_imshow(images[655])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7F2ADE63D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgo7U-lje7Y5"
      },
      "source": [
        "# Happy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "id": "RM5rs3WjeV1Q",
        "outputId": "1d259bb7-9c5f-4c53-9d18-611314d736c8"
      },
      "source": [
        "cv2_imshow(images[200])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7F2ADD9D10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "id": "F1e_d6lofFHg",
        "outputId": "c125dc4b-fb7a-4cf6-8028-e42a81aeae87"
      },
      "source": [
        "cv2_imshow(images[206])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7F2AE6D650>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtZhpQwOfHGf"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7x3-BmjoZWK"
      },
      "source": [
        "import numpy as np\n",
        "images_f=np.array(images)\n",
        "labels_f=np.array(labels)\n",
        "\n",
        "images_f_2=images_f/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jOHIfEhfPef",
        "outputId": "5e7a57fa-62ea-4874-c8c2-02e30f2530f2"
      },
      "source": [
        "images_f_2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(981, 48, 48, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKofjGhufglX"
      },
      "source": [
        "num_of_classes = 7  # len(labels)\n",
        "labels_encoded=tf.keras.utils.to_categorical(labels_f,num_classes=num_of_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVSrhB7zfi40"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test= train_test_split(images_f_2, labels_encoded,test_size=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YychZMnPouQf"
      },
      "source": [
        "## Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39Dgy8yOotlz"
      },
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten,BatchNormalization\n",
        "from tensorflow.keras.layers import Dense, MaxPooling2D,Conv2D\n",
        "from tensorflow.keras.layers import Input,Activation,Add\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfICUZCgvwwv"
      },
      "source": [
        "p = \"/content/drive/MyDrive/Colab/DL/CKPLUS\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTHSplu_fi05"
      },
      "source": [
        "def Convolution(input_tensor,filters):\n",
        "    \n",
        "    x = Conv2D(filters=filters,kernel_size=(3, 3),padding = 'same',strides=(1, 1),kernel_regularizer=l2(0.001))(input_tensor)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x= Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def model():\n",
        "  input_shape = (48,48,3)\n",
        "  inputs = Input((input_shape))\n",
        "  \n",
        "  conv_1= Convolution(inputs,32)\n",
        "  maxp_1 = MaxPooling2D(pool_size = (2,2)) (conv_1)\n",
        "  conv_2 = Convolution(maxp_1,64)\n",
        "  maxp_2 = MaxPooling2D(pool_size = (2, 2)) (conv_2)\n",
        "  conv_3 = Convolution(maxp_2,128)\n",
        "  maxp_3 = MaxPooling2D(pool_size = (2, 2)) (conv_3)\n",
        "  conv_4 = Convolution(maxp_3,256)\n",
        "  maxp_4 = MaxPooling2D(pool_size = (2, 2)) (conv_4)\n",
        "  flatten= Flatten() (maxp_4)\n",
        "  dense_1= Dense(128,activation='relu')(flatten)\n",
        "  drop_1=Dropout(0.2)(dense_1)\n",
        "  output= Dense(7,activation=\"sigmoid\")(drop_1)\n",
        "\n",
        "  model = Model(inputs=[inputs], outputs=[output])\n",
        "\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
        "  print(model.summary())\n",
        "  plot_model(model, to_file=p+'/model101.png')\n",
        "  return Model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF2_FaUkycNK"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Activation , Dropout ,Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.metrics import categorical_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E31U-8JYx7ft"
      },
      "source": [
        "def create_model():\n",
        "    input_shape=(48,48,3)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(6, (5, 5), input_shape=input_shape, padding='same', activation = 'relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(16, (5, 5), padding='same', activation = 'relu'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation = 'relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(7, activation = 'softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='RMSprop')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvnKSWpPyA-8"
      },
      "source": [
        "Model = create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwBkvCF23Iia",
        "outputId": "dde5ae8a-aec2-4013-9542-1bcdf1e0564a"
      },
      "source": [
        "Model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 48, 48, 6)         456       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 24, 24, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 16)        2416      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 10, 10, 64)        9280      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 217,983\n",
            "Trainable params: 217,983\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrq1AaA8fiwx",
        "outputId": "fe711c00-2e90-4a48-9dee-afcedfa9adf2"
      },
      "source": [
        "Model=model()\n",
        "# Model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 48, 48, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 48, 48, 32)        896       \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 6, 6, 256)         295168    \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               295040    \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 684,359\n",
            "Trainable params: 684,359\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g88cSuzQwI19"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz8Pg5Ivo8yE"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1T8fZYPo8ts"
      },
      "source": [
        "fle_s = p + '/Emotion_detection.h5'\n",
        "checkpointer = ModelCheckpoint(fle_s, monitor='loss',verbose=1,save_best_only=True,save_weights_only=False, mode='auto',save_freq='epoch')\n",
        "callback_list=[checkpointer]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNOqvIpbo8o3",
        "outputId": "be6b904e-2a15-455d-cf29-ac9a6a43c4eb"
      },
      "source": [
        "History=Model.fit(X_train,Y_train, batch_size=32, validation_data=(X_test,Y_test), epochs=1000, callbacks=[callback_list])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "23/23 [==============================] - 46s 48ms/step - loss: 1.8984 - accuracy: 0.1942 - val_loss: 1.7934 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.86390, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 2/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 1.7448 - accuracy: 0.3402 - val_loss: 1.5406 - val_accuracy: 0.6341\n",
            "\n",
            "Epoch 00002: loss improved from 1.86390 to 1.69262, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 3/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 1.3680 - accuracy: 0.5453 - val_loss: 0.9801 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00003: loss improved from 1.69262 to 1.29736, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 4/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.0005 - accuracy: 0.6320 - val_loss: 0.7398 - val_accuracy: 0.7276\n",
            "\n",
            "Epoch 00004: loss improved from 1.29736 to 0.99197, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 5/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7610 - accuracy: 0.7328 - val_loss: 0.5962 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00005: loss improved from 0.99197 to 0.76248, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 6/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.6600 - accuracy: 0.7534 - val_loss: 0.5156 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00006: loss improved from 0.76248 to 0.60419, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 7/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.5286 - accuracy: 0.8084 - val_loss: 0.3810 - val_accuracy: 0.8699\n",
            "\n",
            "Epoch 00007: loss improved from 0.60419 to 0.52910, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 8/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.4477 - accuracy: 0.8326 - val_loss: 0.3242 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00008: loss improved from 0.52910 to 0.44541, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 9/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.4125 - accuracy: 0.8521 - val_loss: 0.3462 - val_accuracy: 0.8740\n",
            "\n",
            "Epoch 00009: loss improved from 0.44541 to 0.37130, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 10/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.3263 - accuracy: 0.8866 - val_loss: 0.2848 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00010: loss improved from 0.37130 to 0.32413, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 11/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.3377 - accuracy: 0.8757 - val_loss: 0.1760 - val_accuracy: 0.9390\n",
            "\n",
            "Epoch 00011: loss improved from 0.32413 to 0.29460, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 12/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.2547 - accuracy: 0.9155 - val_loss: 0.1970 - val_accuracy: 0.9350\n",
            "\n",
            "Epoch 00012: loss improved from 0.29460 to 0.22368, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 13/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.2090 - accuracy: 0.9223 - val_loss: 0.1826 - val_accuracy: 0.9431\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.22368\n",
            "Epoch 14/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1320 - accuracy: 0.9675 - val_loss: 0.2165 - val_accuracy: 0.9228\n",
            "\n",
            "Epoch 00014: loss improved from 0.22368 to 0.16774, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 15/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1543 - accuracy: 0.9429 - val_loss: 0.1752 - val_accuracy: 0.9390\n",
            "\n",
            "Epoch 00015: loss improved from 0.16774 to 0.14123, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 16/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1383 - accuracy: 0.9572 - val_loss: 0.1617 - val_accuracy: 0.9431\n",
            "\n",
            "Epoch 00016: loss improved from 0.14123 to 0.14081, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 17/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.1467 - accuracy: 0.9509 - val_loss: 0.1022 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.14081\n",
            "Epoch 18/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.1057 - accuracy: 0.9602 - val_loss: 0.1030 - val_accuracy: 0.9675\n",
            "\n",
            "Epoch 00018: loss improved from 0.14081 to 0.10554, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 19/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1182 - accuracy: 0.9526 - val_loss: 0.0696 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.10554\n",
            "Epoch 20/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0676 - accuracy: 0.9818 - val_loss: 0.0736 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00020: loss improved from 0.10554 to 0.07437, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 21/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.1119 - accuracy: 0.9536 - val_loss: 0.1061 - val_accuracy: 0.9715\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.07437\n",
            "Epoch 22/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0805 - accuracy: 0.9741 - val_loss: 0.1511 - val_accuracy: 0.9593\n",
            "\n",
            "Epoch 00022: loss improved from 0.07437 to 0.06981, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 23/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0916 - accuracy: 0.9768 - val_loss: 0.0789 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.06981\n",
            "Epoch 24/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0754 - accuracy: 0.9719 - val_loss: 0.0747 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00024: loss improved from 0.06981 to 0.06778, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 25/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0415 - accuracy: 0.9894 - val_loss: 0.0745 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00025: loss improved from 0.06778 to 0.04624, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 26/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0716 - accuracy: 0.9803 - val_loss: 0.1267 - val_accuracy: 0.9715\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.04624\n",
            "Epoch 27/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0420 - accuracy: 0.9848 - val_loss: 0.0825 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.04624\n",
            "Epoch 28/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0354 - accuracy: 0.9851 - val_loss: 0.0853 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00028: loss improved from 0.04624 to 0.03445, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 29/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0430 - accuracy: 0.9860 - val_loss: 0.0660 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.03445\n",
            "Epoch 30/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0522 - accuracy: 0.9875 - val_loss: 0.0892 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.03445\n",
            "Epoch 31/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0213 - accuracy: 0.9933 - val_loss: 0.0698 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00031: loss improved from 0.03445 to 0.02241, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 32/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0499 - accuracy: 0.9831 - val_loss: 0.0748 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.02241\n",
            "Epoch 33/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0268 - accuracy: 0.9928 - val_loss: 0.0972 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.02241\n",
            "Epoch 34/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0142 - accuracy: 0.9991 - val_loss: 0.1006 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00034: loss improved from 0.02241 to 0.01803, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 35/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0289 - accuracy: 0.9883 - val_loss: 0.1041 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.01803\n",
            "Epoch 36/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0305 - accuracy: 0.9889 - val_loss: 0.1078 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.01803\n",
            "Epoch 37/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0285 - accuracy: 0.9896 - val_loss: 0.1487 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.01803\n",
            "Epoch 38/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0312 - accuracy: 0.9904 - val_loss: 0.1436 - val_accuracy: 0.9675\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.01803\n",
            "Epoch 39/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.1391 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00039: loss improved from 0.01803 to 0.01071, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 40/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0152 - accuracy: 0.9939 - val_loss: 0.1297 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.01071\n",
            "Epoch 41/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.1319 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00041: loss improved from 0.01071 to 0.00562, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 42/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0511 - accuracy: 0.9818 - val_loss: 0.0916 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.00562\n",
            "Epoch 43/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0129 - accuracy: 0.9957 - val_loss: 0.0818 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.00562\n",
            "Epoch 44/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0264 - accuracy: 0.9916 - val_loss: 0.0863 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.00562\n",
            "Epoch 45/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0128 - accuracy: 0.9962 - val_loss: 0.1431 - val_accuracy: 0.9634\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.00562\n",
            "Epoch 46/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0155 - accuracy: 0.9959 - val_loss: 0.0903 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.00562\n",
            "Epoch 47/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0184 - accuracy: 0.9926 - val_loss: 0.1002 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.00562\n",
            "Epoch 48/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0175 - accuracy: 0.9935 - val_loss: 0.1050 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.00562\n",
            "Epoch 49/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0236 - accuracy: 0.9922 - val_loss: 0.0906 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.00562\n",
            "Epoch 50/1000\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.0072 - accuracy: 0.9974 - val_loss: 0.1234 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.00562\n",
            "Epoch 51/1000\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.0075 - accuracy: 0.9989 - val_loss: 0.1062 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.00562\n",
            "Epoch 52/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0152 - accuracy: 0.9945 - val_loss: 0.0869 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.00562\n",
            "Epoch 53/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0150 - accuracy: 0.9891 - val_loss: 0.0905 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.00562\n",
            "Epoch 54/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0120 - accuracy: 0.9922 - val_loss: 0.1018 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.00562\n",
            "Epoch 55/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1191 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00055: loss improved from 0.00562 to 0.00223, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 56/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0389 - accuracy: 0.9855 - val_loss: 0.1565 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.00223\n",
            "Epoch 57/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0130 - accuracy: 0.9937 - val_loss: 0.0923 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.00223\n",
            "Epoch 58/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0126 - accuracy: 0.9981 - val_loss: 0.0733 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.00223\n",
            "Epoch 59/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1262 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.00223\n",
            "Epoch 60/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0292 - accuracy: 0.9934 - val_loss: 0.0923 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.00223\n",
            "Epoch 61/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.1065 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.00223\n",
            "Epoch 62/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.0740 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.00223\n",
            "Epoch 63/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.0870 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.00223\n",
            "Epoch 64/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0378 - accuracy: 0.9904 - val_loss: 0.1482 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.00223\n",
            "Epoch 65/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0130 - accuracy: 0.9965 - val_loss: 0.1242 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.00223\n",
            "Epoch 66/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.1004 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.00223\n",
            "Epoch 67/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0089 - accuracy: 0.9928 - val_loss: 0.1422 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.00223\n",
            "Epoch 68/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.1277 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.00223\n",
            "Epoch 69/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0079 - accuracy: 0.9988 - val_loss: 0.0999 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.00223\n",
            "Epoch 70/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0069 - accuracy: 0.9957 - val_loss: 0.1075 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.00223\n",
            "Epoch 71/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0265 - accuracy: 0.9919 - val_loss: 0.1154 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.00223\n",
            "Epoch 72/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0129 - accuracy: 0.9965 - val_loss: 0.1245 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.00223\n",
            "Epoch 73/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0080 - accuracy: 0.9954 - val_loss: 0.1385 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.00223\n",
            "Epoch 74/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0120 - accuracy: 0.9978 - val_loss: 0.1681 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.00223\n",
            "Epoch 75/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0259 - accuracy: 0.9975 - val_loss: 0.1304 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.00223\n",
            "Epoch 76/1000\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.1291 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.00223\n",
            "Epoch 77/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0219 - accuracy: 0.9910 - val_loss: 0.0804 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.00223\n",
            "Epoch 78/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1158 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00078: loss improved from 0.00223 to 0.00176, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 79/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0046 - accuracy: 0.9975 - val_loss: 0.1105 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.00176\n",
            "Epoch 80/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0113 - accuracy: 0.9973 - val_loss: 0.0925 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.00176\n",
            "Epoch 81/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0129 - accuracy: 0.9931 - val_loss: 0.0815 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.00176\n",
            "Epoch 82/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0097 - accuracy: 0.9959 - val_loss: 0.0769 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.00176\n",
            "Epoch 83/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.1027 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.00176\n",
            "Epoch 84/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.1093 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.00176\n",
            "Epoch 85/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0735 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.00176\n",
            "Epoch 86/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 0.1005 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.00176\n",
            "Epoch 87/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0082 - accuracy: 0.9966 - val_loss: 0.1568 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.00176\n",
            "Epoch 88/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0147 - accuracy: 0.9939 - val_loss: 0.1401 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.00176\n",
            "Epoch 89/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1753 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.00176\n",
            "Epoch 90/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.1813 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.00176\n",
            "Epoch 91/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0025 - accuracy: 0.9999 - val_loss: 0.1401 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.00176\n",
            "Epoch 92/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 7.5077e-04 - accuracy: 1.0000 - val_loss: 0.1657 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00092: loss improved from 0.00176 to 0.00051, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 93/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.1460 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.00051\n",
            "Epoch 94/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0129 - accuracy: 0.9906 - val_loss: 0.0620 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.00051\n",
            "Epoch 95/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.1380 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.00051\n",
            "Epoch 96/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0453 - accuracy: 0.9835 - val_loss: 0.0710 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.00051\n",
            "Epoch 97/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1193 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.00051\n",
            "Epoch 98/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0297 - accuracy: 0.9945 - val_loss: 0.1161 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.00051\n",
            "Epoch 99/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.1055 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.00051\n",
            "Epoch 100/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.1522 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.00051\n",
            "Epoch 101/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0120 - accuracy: 0.9958 - val_loss: 0.1596 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00101: loss did not improve from 0.00051\n",
            "Epoch 102/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.1163 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00102: loss did not improve from 0.00051\n",
            "Epoch 103/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.1365 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00103: loss did not improve from 0.00051\n",
            "Epoch 104/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.0964 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00104: loss did not improve from 0.00051\n",
            "Epoch 105/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1272 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00105: loss did not improve from 0.00051\n",
            "Epoch 106/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0064 - accuracy: 0.9992 - val_loss: 0.1321 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00106: loss did not improve from 0.00051\n",
            "Epoch 107/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 0.0971 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00107: loss did not improve from 0.00051\n",
            "Epoch 108/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1225 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00108: loss did not improve from 0.00051\n",
            "Epoch 109/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.1298 - val_accuracy: 0.9634\n",
            "\n",
            "Epoch 00109: loss did not improve from 0.00051\n",
            "Epoch 110/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0971 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00110: loss did not improve from 0.00051\n",
            "Epoch 111/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.1190 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00111: loss did not improve from 0.00051\n",
            "Epoch 112/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.1161 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00112: loss did not improve from 0.00051\n",
            "Epoch 113/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0129 - accuracy: 0.9945 - val_loss: 0.0749 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00113: loss did not improve from 0.00051\n",
            "Epoch 114/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 9.4674e-04 - accuracy: 1.0000 - val_loss: 0.0980 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00114: loss did not improve from 0.00051\n",
            "Epoch 115/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.5424e-04 - accuracy: 1.0000 - val_loss: 0.1074 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00115: loss improved from 0.00051 to 0.00036, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 116/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 2.1977e-04 - accuracy: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.9715\n",
            "\n",
            "Epoch 00116: loss improved from 0.00036 to 0.00033, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 117/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0043 - accuracy: 0.9989 - val_loss: 0.0989 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00117: loss did not improve from 0.00033\n",
            "Epoch 118/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.1182 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00118: loss did not improve from 0.00033\n",
            "Epoch 119/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0044 - accuracy: 0.9969 - val_loss: 0.0605 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00119: loss did not improve from 0.00033\n",
            "Epoch 120/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.1260 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00120: loss did not improve from 0.00033\n",
            "Epoch 121/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.1038 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00121: loss did not improve from 0.00033\n",
            "Epoch 122/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.1217 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00122: loss did not improve from 0.00033\n",
            "Epoch 123/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 6.7411e-04 - accuracy: 1.0000 - val_loss: 0.1220 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00123: loss did not improve from 0.00033\n",
            "Epoch 124/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2255 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00124: loss did not improve from 0.00033\n",
            "Epoch 125/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0220 - accuracy: 0.9963 - val_loss: 0.1383 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00125: loss did not improve from 0.00033\n",
            "Epoch 126/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.1218 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00126: loss did not improve from 0.00033\n",
            "Epoch 127/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0113 - accuracy: 0.9956 - val_loss: 0.1312 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00127: loss did not improve from 0.00033\n",
            "Epoch 128/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0107 - accuracy: 0.9947 - val_loss: 0.1286 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00128: loss did not improve from 0.00033\n",
            "Epoch 129/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0047 - accuracy: 0.9981 - val_loss: 0.0829 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00129: loss did not improve from 0.00033\n",
            "Epoch 130/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 7.9729e-04 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00130: loss did not improve from 0.00033\n",
            "Epoch 131/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.0493 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00131: loss did not improve from 0.00033\n",
            "Epoch 132/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0509 - accuracy: 0.9925 - val_loss: 0.0792 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00132: loss did not improve from 0.00033\n",
            "Epoch 133/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 2.5450e-04 - accuracy: 1.0000 - val_loss: 0.0997 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00133: loss improved from 0.00033 to 0.00031, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 134/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.9991 - val_loss: 0.0719 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00134: loss did not improve from 0.00031\n",
            "Epoch 135/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0030 - accuracy: 0.9964 - val_loss: 0.0988 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00135: loss did not improve from 0.00031\n",
            "Epoch 136/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0019 - accuracy: 0.9985 - val_loss: 0.2286 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00136: loss did not improve from 0.00031\n",
            "Epoch 137/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0055 - accuracy: 0.9986 - val_loss: 0.1425 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00137: loss did not improve from 0.00031\n",
            "Epoch 138/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.1537 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00138: loss did not improve from 0.00031\n",
            "Epoch 139/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0144 - accuracy: 0.9949 - val_loss: 0.1637 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00139: loss did not improve from 0.00031\n",
            "Epoch 140/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 1.9820e-04 - accuracy: 1.0000 - val_loss: 0.1354 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00140: loss improved from 0.00031 to 0.00021, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 141/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 2.7824e-04 - accuracy: 1.0000 - val_loss: 0.1417 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00141: loss improved from 0.00021 to 0.00015, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 142/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.1848 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00142: loss did not improve from 0.00015\n",
            "Epoch 143/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 6.2600e-04 - accuracy: 1.0000 - val_loss: 0.1110 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00143: loss did not improve from 0.00015\n",
            "Epoch 144/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 1.4332e-04 - accuracy: 1.0000 - val_loss: 0.1072 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00144: loss did not improve from 0.00015\n",
            "Epoch 145/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.4213e-04 - accuracy: 1.0000 - val_loss: 0.1273 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00145: loss improved from 0.00015 to 0.00010, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 146/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0073 - accuracy: 0.9971 - val_loss: 0.1489 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00146: loss did not improve from 0.00010\n",
            "Epoch 147/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0017 - accuracy: 0.9978 - val_loss: 0.1596 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00147: loss did not improve from 0.00010\n",
            "Epoch 148/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.9978 - val_loss: 0.1952 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00148: loss did not improve from 0.00010\n",
            "Epoch 149/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0102 - accuracy: 0.9961 - val_loss: 0.1477 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00149: loss did not improve from 0.00010\n",
            "Epoch 150/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 6.0439e-04 - accuracy: 0.9999 - val_loss: 0.1655 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00150: loss did not improve from 0.00010\n",
            "Epoch 151/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.1091 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00151: loss did not improve from 0.00010\n",
            "Epoch 152/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.6264e-05 - accuracy: 1.0000 - val_loss: 0.1128 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00152: loss did not improve from 0.00010\n",
            "Epoch 153/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0157 - accuracy: 0.9976 - val_loss: 0.1148 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00153: loss did not improve from 0.00010\n",
            "Epoch 154/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0019 - accuracy: 0.9985 - val_loss: 0.1786 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00154: loss did not improve from 0.00010\n",
            "Epoch 155/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 8.5665e-04 - accuracy: 1.0000 - val_loss: 0.2134 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00155: loss did not improve from 0.00010\n",
            "Epoch 156/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0444 - accuracy: 0.9929 - val_loss: 0.1525 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00156: loss did not improve from 0.00010\n",
            "Epoch 157/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 2.3749e-04 - accuracy: 1.0000 - val_loss: 0.1331 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00157: loss did not improve from 0.00010\n",
            "Epoch 158/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 6.6053e-04 - accuracy: 1.0000 - val_loss: 0.1645 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00158: loss did not improve from 0.00010\n",
            "Epoch 159/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 5.8856e-04 - accuracy: 0.9998 - val_loss: 0.0933 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00159: loss did not improve from 0.00010\n",
            "Epoch 160/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0965 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00160: loss did not improve from 0.00010\n",
            "Epoch 161/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0076 - accuracy: 0.9974 - val_loss: 0.1012 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00161: loss did not improve from 0.00010\n",
            "Epoch 162/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 9.1167e-04 - accuracy: 0.9998 - val_loss: 0.1494 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00162: loss did not improve from 0.00010\n",
            "Epoch 163/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0111 - accuracy: 0.9951 - val_loss: 0.0987 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00163: loss did not improve from 0.00010\n",
            "Epoch 164/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0940 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00164: loss did not improve from 0.00010\n",
            "Epoch 165/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 8.4613e-04 - accuracy: 1.0000 - val_loss: 0.1147 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00165: loss did not improve from 0.00010\n",
            "Epoch 166/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 3.5424e-04 - accuracy: 1.0000 - val_loss: 0.1351 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00166: loss did not improve from 0.00010\n",
            "Epoch 167/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0102 - accuracy: 0.9983 - val_loss: 0.1941 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00167: loss did not improve from 0.00010\n",
            "Epoch 168/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1564 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00168: loss did not improve from 0.00010\n",
            "Epoch 169/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0018 - accuracy: 0.9989 - val_loss: 0.1947 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00169: loss did not improve from 0.00010\n",
            "Epoch 170/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1818 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00170: loss did not improve from 0.00010\n",
            "Epoch 171/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 1.9573e-04 - accuracy: 1.0000 - val_loss: 0.1851 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00171: loss did not improve from 0.00010\n",
            "Epoch 172/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.8935e-05 - accuracy: 1.0000 - val_loss: 0.2130 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00172: loss improved from 0.00010 to 0.00003, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 173/1000\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2129 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00173: loss did not improve from 0.00003\n",
            "Epoch 174/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0175 - accuracy: 0.9958 - val_loss: 0.1919 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00174: loss did not improve from 0.00003\n",
            "Epoch 175/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0052 - accuracy: 0.9965 - val_loss: 0.1722 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00175: loss did not improve from 0.00003\n",
            "Epoch 176/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2064 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00176: loss did not improve from 0.00003\n",
            "Epoch 177/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.2820e-04 - accuracy: 1.0000 - val_loss: 0.2101 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00177: loss did not improve from 0.00003\n",
            "Epoch 178/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 8.7921e-05 - accuracy: 1.0000 - val_loss: 0.2269 - val_accuracy: 0.9715\n",
            "\n",
            "Epoch 00178: loss did not improve from 0.00003\n",
            "Epoch 179/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0099 - accuracy: 0.9982 - val_loss: 0.1058 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00179: loss did not improve from 0.00003\n",
            "Epoch 180/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0025 - accuracy: 0.9981 - val_loss: 0.1348 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00180: loss did not improve from 0.00003\n",
            "Epoch 181/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0118 - accuracy: 0.9970 - val_loss: 0.1188 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00181: loss did not improve from 0.00003\n",
            "Epoch 182/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0427 - accuracy: 0.9915 - val_loss: 0.1539 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00182: loss did not improve from 0.00003\n",
            "Epoch 183/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1623 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00183: loss did not improve from 0.00003\n",
            "Epoch 184/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.1518e-05 - accuracy: 1.0000 - val_loss: 0.1551 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00184: loss improved from 0.00003 to 0.00002, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 185/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0125 - accuracy: 0.9980 - val_loss: 0.1371 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00185: loss did not improve from 0.00002\n",
            "Epoch 186/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 3.0578e-04 - accuracy: 1.0000 - val_loss: 0.1391 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00186: loss did not improve from 0.00002\n",
            "Epoch 187/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0198 - accuracy: 0.9957 - val_loss: 0.1326 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00187: loss did not improve from 0.00002\n",
            "Epoch 188/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0035 - accuracy: 0.9975 - val_loss: 0.0861 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00188: loss did not improve from 0.00002\n",
            "Epoch 189/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 2.8507e-04 - accuracy: 1.0000 - val_loss: 0.1065 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00189: loss did not improve from 0.00002\n",
            "Epoch 190/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.2763e-04 - accuracy: 1.0000 - val_loss: 0.2218 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00190: loss did not improve from 0.00002\n",
            "Epoch 191/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.8385e-04 - accuracy: 1.0000 - val_loss: 0.2063 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00191: loss did not improve from 0.00002\n",
            "Epoch 192/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.1500e-04 - accuracy: 1.0000 - val_loss: 0.2239 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00192: loss did not improve from 0.00002\n",
            "Epoch 193/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0020 - accuracy: 0.9989 - val_loss: 0.2018 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00193: loss did not improve from 0.00002\n",
            "Epoch 194/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 0.2242 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00194: loss did not improve from 0.00002\n",
            "Epoch 195/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0067 - accuracy: 0.9968 - val_loss: 0.1901 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00195: loss did not improve from 0.00002\n",
            "Epoch 196/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.8252e-04 - accuracy: 1.0000 - val_loss: 0.2210 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00196: loss did not improve from 0.00002\n",
            "Epoch 197/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 6.5833e-04 - accuracy: 1.0000 - val_loss: 0.1483 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00197: loss did not improve from 0.00002\n",
            "Epoch 198/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1676 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00198: loss did not improve from 0.00002\n",
            "Epoch 199/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.2009 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00199: loss did not improve from 0.00002\n",
            "Epoch 200/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 6.4208e-04 - accuracy: 1.0000 - val_loss: 0.2150 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00200: loss did not improve from 0.00002\n",
            "Epoch 201/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 8.9096e-05 - accuracy: 1.0000 - val_loss: 0.2111 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00201: loss did not improve from 0.00002\n",
            "Epoch 202/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.8622e-04 - accuracy: 1.0000 - val_loss: 0.2440 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00202: loss did not improve from 0.00002\n",
            "Epoch 203/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0080 - accuracy: 0.9944 - val_loss: 0.1647 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00203: loss did not improve from 0.00002\n",
            "Epoch 204/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0050 - accuracy: 0.9993 - val_loss: 0.2136 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00204: loss did not improve from 0.00002\n",
            "Epoch 205/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0050 - accuracy: 0.9949 - val_loss: 0.2669 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00205: loss did not improve from 0.00002\n",
            "Epoch 206/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 7.2595e-04 - accuracy: 1.0000 - val_loss: 0.2783 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00206: loss did not improve from 0.00002\n",
            "Epoch 207/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0073 - accuracy: 0.9984 - val_loss: 0.2301 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00207: loss did not improve from 0.00002\n",
            "Epoch 208/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.0335e-04 - accuracy: 1.0000 - val_loss: 0.2154 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00208: loss did not improve from 0.00002\n",
            "Epoch 209/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.7356e-04 - accuracy: 1.0000 - val_loss: 0.2130 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00209: loss did not improve from 0.00002\n",
            "Epoch 210/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 8.4822e-05 - accuracy: 1.0000 - val_loss: 0.2258 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00210: loss did not improve from 0.00002\n",
            "Epoch 211/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 4.0461e-05 - accuracy: 1.0000 - val_loss: 0.2140 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00211: loss did not improve from 0.00002\n",
            "Epoch 212/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.7119e-04 - accuracy: 0.9997 - val_loss: 0.1503 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00212: loss did not improve from 0.00002\n",
            "Epoch 213/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 6.1984e-04 - accuracy: 1.0000 - val_loss: 0.1490 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00213: loss did not improve from 0.00002\n",
            "Epoch 214/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0024 - accuracy: 0.9985 - val_loss: 0.2651 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00214: loss did not improve from 0.00002\n",
            "Epoch 215/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.5540e-04 - accuracy: 1.0000 - val_loss: 0.2244 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00215: loss did not improve from 0.00002\n",
            "Epoch 216/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.0685e-05 - accuracy: 1.0000 - val_loss: 0.2082 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00216: loss improved from 0.00002 to 0.00002, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 217/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0032 - accuracy: 0.9977 - val_loss: 0.2126 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00217: loss did not improve from 0.00002\n",
            "Epoch 218/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.2164 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00218: loss did not improve from 0.00002\n",
            "Epoch 219/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 6.8670e-04 - accuracy: 1.0000 - val_loss: 0.1881 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00219: loss did not improve from 0.00002\n",
            "Epoch 220/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 8.1556e-06 - accuracy: 1.0000 - val_loss: 0.1801 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00220: loss improved from 0.00002 to 0.00001, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 221/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0054 - accuracy: 0.9975 - val_loss: 0.1579 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00221: loss did not improve from 0.00001\n",
            "Epoch 222/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0013 - accuracy: 0.9990 - val_loss: 0.2300 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00222: loss did not improve from 0.00001\n",
            "Epoch 223/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.1662e-05 - accuracy: 1.0000 - val_loss: 0.2698 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00223: loss did not improve from 0.00001\n",
            "Epoch 224/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 2.1203e-04 - accuracy: 1.0000 - val_loss: 0.2483 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00224: loss did not improve from 0.00001\n",
            "Epoch 225/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0046 - accuracy: 0.9992 - val_loss: 0.1342 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00225: loss did not improve from 0.00001\n",
            "Epoch 226/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.4262e-04 - accuracy: 1.0000 - val_loss: 0.1684 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00226: loss did not improve from 0.00001\n",
            "Epoch 227/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.1343 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00227: loss did not improve from 0.00001\n",
            "Epoch 228/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.4669e-04 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00228: loss did not improve from 0.00001\n",
            "Epoch 229/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.9990 - val_loss: 0.1298 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00229: loss did not improve from 0.00001\n",
            "Epoch 230/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.8506e-04 - accuracy: 1.0000 - val_loss: 0.1961 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00230: loss did not improve from 0.00001\n",
            "Epoch 231/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 4.0198e-04 - accuracy: 1.0000 - val_loss: 0.1707 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00231: loss did not improve from 0.00001\n",
            "Epoch 232/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 7.4288e-04 - accuracy: 1.0000 - val_loss: 0.1724 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00232: loss did not improve from 0.00001\n",
            "Epoch 233/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 4.6964e-05 - accuracy: 1.0000 - val_loss: 0.1938 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00233: loss did not improve from 0.00001\n",
            "Epoch 234/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.4081e-04 - accuracy: 1.0000 - val_loss: 0.1454 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00234: loss did not improve from 0.00001\n",
            "Epoch 235/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.8586e-05 - accuracy: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00235: loss did not improve from 0.00001\n",
            "Epoch 236/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.1098 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00236: loss did not improve from 0.00001\n",
            "Epoch 237/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.1527 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00237: loss did not improve from 0.00001\n",
            "Epoch 238/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.1775 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00238: loss did not improve from 0.00001\n",
            "Epoch 239/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.8537e-04 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00239: loss did not improve from 0.00001\n",
            "Epoch 240/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 6.6394e-05 - accuracy: 1.0000 - val_loss: 0.0699 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00240: loss did not improve from 0.00001\n",
            "Epoch 241/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0055 - accuracy: 0.9992 - val_loss: 0.0573 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00241: loss did not improve from 0.00001\n",
            "Epoch 242/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0052 - accuracy: 0.9951 - val_loss: 0.1346 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00242: loss did not improve from 0.00001\n",
            "Epoch 243/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0071 - accuracy: 0.9964 - val_loss: 0.1783 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00243: loss did not improve from 0.00001\n",
            "Epoch 244/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 4.2878e-04 - accuracy: 0.9999 - val_loss: 0.1268 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00244: loss did not improve from 0.00001\n",
            "Epoch 245/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1556 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00245: loss did not improve from 0.00001\n",
            "Epoch 246/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.5996e-04 - accuracy: 1.0000 - val_loss: 0.1702 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00246: loss did not improve from 0.00001\n",
            "Epoch 247/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.9242e-04 - accuracy: 1.0000 - val_loss: 0.1560 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00247: loss did not improve from 0.00001\n",
            "Epoch 248/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.4696e-05 - accuracy: 1.0000 - val_loss: 0.1928 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00248: loss improved from 0.00001 to 0.00001, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 249/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0016 - accuracy: 0.9989 - val_loss: 0.1099 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00249: loss did not improve from 0.00001\n",
            "Epoch 250/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0020 - accuracy: 0.9985 - val_loss: 0.1292 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00250: loss did not improve from 0.00001\n",
            "Epoch 251/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.3034e-05 - accuracy: 1.0000 - val_loss: 0.2117 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00251: loss did not improve from 0.00001\n",
            "Epoch 252/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0012 - accuracy: 0.9994 - val_loss: 0.1999 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00252: loss did not improve from 0.00001\n",
            "Epoch 253/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 3.7503e-04 - accuracy: 1.0000 - val_loss: 0.4851 - val_accuracy: 0.9715\n",
            "\n",
            "Epoch 00253: loss did not improve from 0.00001\n",
            "Epoch 254/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0037 - accuracy: 0.9997 - val_loss: 0.2296 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00254: loss did not improve from 0.00001\n",
            "Epoch 255/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0188 - accuracy: 0.9920 - val_loss: 0.3216 - val_accuracy: 0.9715\n",
            "\n",
            "Epoch 00255: loss did not improve from 0.00001\n",
            "Epoch 256/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.6276e-06 - accuracy: 1.0000 - val_loss: 0.3115 - val_accuracy: 0.9715\n",
            "\n",
            "Epoch 00256: loss improved from 0.00001 to 0.00000, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 257/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.1756e-04 - accuracy: 1.0000 - val_loss: 0.2921 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00257: loss did not improve from 0.00000\n",
            "Epoch 258/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.5552e-06 - accuracy: 1.0000 - val_loss: 0.2724 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00258: loss did not improve from 0.00000\n",
            "Epoch 259/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 7.6622e-06 - accuracy: 1.0000 - val_loss: 0.2508 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00259: loss did not improve from 0.00000\n",
            "Epoch 260/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 9.2879e-04 - accuracy: 1.0000 - val_loss: 0.2660 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00260: loss did not improve from 0.00000\n",
            "Epoch 261/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.7139e-05 - accuracy: 1.0000 - val_loss: 0.2880 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00261: loss did not improve from 0.00000\n",
            "Epoch 262/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 6.6288e-06 - accuracy: 1.0000 - val_loss: 0.2665 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00262: loss did not improve from 0.00000\n",
            "Epoch 263/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 9.7714e-04 - accuracy: 1.0000 - val_loss: 0.3243 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00263: loss did not improve from 0.00000\n",
            "Epoch 264/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0032 - accuracy: 0.9981 - val_loss: 0.1549 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00264: loss did not improve from 0.00000\n",
            "Epoch 265/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 2.8534e-04 - accuracy: 1.0000 - val_loss: 0.1447 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00265: loss did not improve from 0.00000\n",
            "Epoch 266/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.4513e-05 - accuracy: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00266: loss did not improve from 0.00000\n",
            "Epoch 267/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0027 - accuracy: 0.9978 - val_loss: 0.1886 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00267: loss did not improve from 0.00000\n",
            "Epoch 268/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.1848 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00268: loss did not improve from 0.00000\n",
            "Epoch 269/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2187 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00269: loss did not improve from 0.00000\n",
            "Epoch 270/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.1495e-04 - accuracy: 1.0000 - val_loss: 0.1368 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00270: loss did not improve from 0.00000\n",
            "Epoch 271/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0087 - accuracy: 0.9955 - val_loss: 0.1796 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00271: loss did not improve from 0.00000\n",
            "Epoch 272/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 9.0006e-05 - accuracy: 1.0000 - val_loss: 0.1880 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00272: loss did not improve from 0.00000\n",
            "Epoch 273/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0033 - accuracy: 0.9999 - val_loss: 0.1197 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00273: loss did not improve from 0.00000\n",
            "Epoch 274/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.1908 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00274: loss did not improve from 0.00000\n",
            "Epoch 275/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0156 - accuracy: 0.9985 - val_loss: 0.1729 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00275: loss did not improve from 0.00000\n",
            "Epoch 276/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.7761e-05 - accuracy: 1.0000 - val_loss: 0.1719 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00276: loss did not improve from 0.00000\n",
            "Epoch 277/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0046 - accuracy: 0.9956 - val_loss: 0.1456 - val_accuracy: 0.9715\n",
            "\n",
            "Epoch 00277: loss did not improve from 0.00000\n",
            "Epoch 278/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.5333e-04 - accuracy: 1.0000 - val_loss: 0.0833 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00278: loss did not improve from 0.00000\n",
            "Epoch 279/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.9655e-04 - accuracy: 1.0000 - val_loss: 0.1654 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00279: loss did not improve from 0.00000\n",
            "Epoch 280/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.5683e-06 - accuracy: 1.0000 - val_loss: 0.1646 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00280: loss did not improve from 0.00000\n",
            "Epoch 281/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.5580e-04 - accuracy: 1.0000 - val_loss: 0.1342 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00281: loss did not improve from 0.00000\n",
            "Epoch 282/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.1387e-05 - accuracy: 1.0000 - val_loss: 0.1809 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00282: loss did not improve from 0.00000\n",
            "Epoch 283/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.4840e-06 - accuracy: 1.0000 - val_loss: 0.1532 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00283: loss did not improve from 0.00000\n",
            "Epoch 284/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 4.8209e-07 - accuracy: 1.0000 - val_loss: 0.1472 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00284: loss improved from 0.00000 to 0.00000, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 285/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 8.3943e-08 - accuracy: 1.0000 - val_loss: 0.1543 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00285: loss improved from 0.00000 to 0.00000, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 286/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.2636e-06 - accuracy: 1.0000 - val_loss: 0.2022 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00286: loss did not improve from 0.00000\n",
            "Epoch 287/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.5163e-04 - accuracy: 0.9999 - val_loss: 0.2887 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00287: loss did not improve from 0.00000\n",
            "Epoch 288/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 7.6239e-05 - accuracy: 1.0000 - val_loss: 0.0906 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00288: loss did not improve from 0.00000\n",
            "Epoch 289/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 6.5836e-06 - accuracy: 1.0000 - val_loss: 0.1210 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00289: loss did not improve from 0.00000\n",
            "Epoch 290/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.0146e-06 - accuracy: 1.0000 - val_loss: 0.1897 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00290: loss did not improve from 0.00000\n",
            "Epoch 291/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.3092e-06 - accuracy: 1.0000 - val_loss: 0.4084 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00291: loss did not improve from 0.00000\n",
            "Epoch 292/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 4.3315e-05 - accuracy: 1.0000 - val_loss: 0.1881 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00292: loss did not improve from 0.00000\n",
            "Epoch 293/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0037 - accuracy: 0.9983 - val_loss: 0.3031 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00293: loss did not improve from 0.00000\n",
            "Epoch 294/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 8.7595e-05 - accuracy: 1.0000 - val_loss: 0.2635 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00294: loss did not improve from 0.00000\n",
            "Epoch 295/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0057 - accuracy: 0.9965 - val_loss: 0.2379 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00295: loss did not improve from 0.00000\n",
            "Epoch 296/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.0903e-04 - accuracy: 1.0000 - val_loss: 0.2144 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00296: loss did not improve from 0.00000\n",
            "Epoch 297/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.1271e-06 - accuracy: 1.0000 - val_loss: 0.2170 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00297: loss did not improve from 0.00000\n",
            "Epoch 298/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2079 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00298: loss did not improve from 0.00000\n",
            "Epoch 299/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 6.1212e-07 - accuracy: 1.0000 - val_loss: 0.2034 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00299: loss did not improve from 0.00000\n",
            "Epoch 300/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.8204e-04 - accuracy: 0.9998 - val_loss: 0.1247 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00300: loss did not improve from 0.00000\n",
            "Epoch 301/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.1959 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00301: loss did not improve from 0.00000\n",
            "Epoch 302/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 2.7160e-05 - accuracy: 1.0000 - val_loss: 0.1968 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00302: loss did not improve from 0.00000\n",
            "Epoch 303/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.9444e-05 - accuracy: 1.0000 - val_loss: 0.2385 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00303: loss did not improve from 0.00000\n",
            "Epoch 304/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.6095e-04 - accuracy: 1.0000 - val_loss: 0.2511 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00304: loss did not improve from 0.00000\n",
            "Epoch 305/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.2333 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00305: loss did not improve from 0.00000\n",
            "Epoch 306/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.5088e-05 - accuracy: 1.0000 - val_loss: 0.2411 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00306: loss did not improve from 0.00000\n",
            "Epoch 307/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.7742e-05 - accuracy: 1.0000 - val_loss: 0.2559 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00307: loss did not improve from 0.00000\n",
            "Epoch 308/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 2.0519e-04 - accuracy: 0.9998 - val_loss: 0.4008 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00308: loss did not improve from 0.00000\n",
            "Epoch 309/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.9930e-04 - accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00309: loss did not improve from 0.00000\n",
            "Epoch 310/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0024 - accuracy: 0.9990 - val_loss: 0.1592 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00310: loss did not improve from 0.00000\n",
            "Epoch 311/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0026 - accuracy: 0.9985 - val_loss: 0.1923 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00311: loss did not improve from 0.00000\n",
            "Epoch 312/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.9983 - val_loss: 0.3259 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00312: loss did not improve from 0.00000\n",
            "Epoch 313/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0051 - accuracy: 0.9975 - val_loss: 0.2326 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00313: loss did not improve from 0.00000\n",
            "Epoch 314/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.5571e-04 - accuracy: 1.0000 - val_loss: 0.2409 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00314: loss did not improve from 0.00000\n",
            "Epoch 315/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 6.9986e-05 - accuracy: 1.0000 - val_loss: 0.2573 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00315: loss did not improve from 0.00000\n",
            "Epoch 316/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.9040e-07 - accuracy: 1.0000 - val_loss: 0.2540 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00316: loss did not improve from 0.00000\n",
            "Epoch 317/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.0462e-04 - accuracy: 1.0000 - val_loss: 0.3825 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00317: loss did not improve from 0.00000\n",
            "Epoch 318/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 4.4829e-06 - accuracy: 1.0000 - val_loss: 0.2688 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00318: loss did not improve from 0.00000\n",
            "Epoch 319/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 9.6106e-05 - accuracy: 1.0000 - val_loss: 0.2165 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00319: loss did not improve from 0.00000\n",
            "Epoch 320/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.6267e-04 - accuracy: 1.0000 - val_loss: 0.2312 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00320: loss did not improve from 0.00000\n",
            "Epoch 321/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.8231e-04 - accuracy: 1.0000 - val_loss: 0.2077 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00321: loss did not improve from 0.00000\n",
            "Epoch 322/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.9951e-07 - accuracy: 1.0000 - val_loss: 0.2256 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00322: loss did not improve from 0.00000\n",
            "Epoch 323/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0083 - accuracy: 0.9987 - val_loss: 0.3183 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00323: loss did not improve from 0.00000\n",
            "Epoch 324/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.6555e-04 - accuracy: 1.0000 - val_loss: 0.2921 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00324: loss did not improve from 0.00000\n",
            "Epoch 325/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.1668e-04 - accuracy: 1.0000 - val_loss: 0.3154 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00325: loss did not improve from 0.00000\n",
            "Epoch 326/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0028 - accuracy: 0.9989 - val_loss: 0.1994 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00326: loss did not improve from 0.00000\n",
            "Epoch 327/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 9.7998e-05 - accuracy: 1.0000 - val_loss: 0.2095 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00327: loss did not improve from 0.00000\n",
            "Epoch 328/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0035 - accuracy: 0.9981 - val_loss: 0.2452 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00328: loss did not improve from 0.00000\n",
            "Epoch 329/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.3276e-05 - accuracy: 1.0000 - val_loss: 0.2571 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00329: loss did not improve from 0.00000\n",
            "Epoch 330/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.0004e-05 - accuracy: 1.0000 - val_loss: 0.2401 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00330: loss did not improve from 0.00000\n",
            "Epoch 331/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.1798e-04 - accuracy: 1.0000 - val_loss: 0.2528 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00331: loss did not improve from 0.00000\n",
            "Epoch 332/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.2111 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00332: loss did not improve from 0.00000\n",
            "Epoch 333/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0599 - accuracy: 0.9951 - val_loss: 0.1650 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00333: loss did not improve from 0.00000\n",
            "Epoch 334/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0129 - accuracy: 0.9951 - val_loss: 0.2625 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00334: loss did not improve from 0.00000\n",
            "Epoch 335/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.3543e-04 - accuracy: 1.0000 - val_loss: 0.1891 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00335: loss did not improve from 0.00000\n",
            "Epoch 336/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 4.9631e-04 - accuracy: 0.9996 - val_loss: 0.1471 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00336: loss did not improve from 0.00000\n",
            "Epoch 337/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0200 - accuracy: 0.9946 - val_loss: 0.1669 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00337: loss did not improve from 0.00000\n",
            "Epoch 338/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.0882e-04 - accuracy: 1.0000 - val_loss: 0.1656 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00338: loss did not improve from 0.00000\n",
            "Epoch 339/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 5.3022e-04 - accuracy: 0.9998 - val_loss: 0.3067 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00339: loss did not improve from 0.00000\n",
            "Epoch 340/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.1541e-04 - accuracy: 0.9999 - val_loss: 0.3097 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00340: loss did not improve from 0.00000\n",
            "Epoch 341/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0044 - accuracy: 0.9996 - val_loss: 0.1068 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00341: loss did not improve from 0.00000\n",
            "Epoch 342/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.1815e-04 - accuracy: 1.0000 - val_loss: 0.1071 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00342: loss did not improve from 0.00000\n",
            "Epoch 343/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.7835e-05 - accuracy: 1.0000 - val_loss: 0.1175 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00343: loss did not improve from 0.00000\n",
            "Epoch 344/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.0910e-05 - accuracy: 1.0000 - val_loss: 0.1765 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00344: loss did not improve from 0.00000\n",
            "Epoch 345/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0706 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00345: loss did not improve from 0.00000\n",
            "Epoch 346/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0161 - accuracy: 0.9951 - val_loss: 0.0913 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00346: loss did not improve from 0.00000\n",
            "Epoch 347/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.4634e-05 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00347: loss did not improve from 0.00000\n",
            "Epoch 348/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0054 - accuracy: 0.9994 - val_loss: 0.1707 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00348: loss did not improve from 0.00000\n",
            "Epoch 349/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.6626e-05 - accuracy: 1.0000 - val_loss: 0.1639 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00349: loss did not improve from 0.00000\n",
            "Epoch 350/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0085 - accuracy: 0.9969 - val_loss: 0.1108 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00350: loss did not improve from 0.00000\n",
            "Epoch 351/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0650 - accuracy: 0.9969 - val_loss: 0.2531 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00351: loss did not improve from 0.00000\n",
            "Epoch 352/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 6.2728e-05 - accuracy: 1.0000 - val_loss: 0.2555 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00352: loss did not improve from 0.00000\n",
            "Epoch 353/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.0061e-05 - accuracy: 1.0000 - val_loss: 0.2542 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00353: loss did not improve from 0.00000\n",
            "Epoch 354/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.2148 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00354: loss did not improve from 0.00000\n",
            "Epoch 355/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.3230e-05 - accuracy: 1.0000 - val_loss: 0.2175 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00355: loss did not improve from 0.00000\n",
            "Epoch 356/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0131 - accuracy: 0.9975 - val_loss: 0.2212 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00356: loss did not improve from 0.00000\n",
            "Epoch 357/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.2094e-05 - accuracy: 1.0000 - val_loss: 0.2142 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00357: loss did not improve from 0.00000\n",
            "Epoch 358/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 4.9178e-04 - accuracy: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00358: loss did not improve from 0.00000\n",
            "Epoch 359/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 8.5584e-05 - accuracy: 1.0000 - val_loss: 0.0930 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00359: loss did not improve from 0.00000\n",
            "Epoch 360/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0096 - accuracy: 0.9983 - val_loss: 0.0945 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00360: loss did not improve from 0.00000\n",
            "Epoch 361/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1335 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00361: loss did not improve from 0.00000\n",
            "Epoch 362/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.2460e-05 - accuracy: 1.0000 - val_loss: 0.1452 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00362: loss did not improve from 0.00000\n",
            "Epoch 363/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 5.8668e-04 - accuracy: 1.0000 - val_loss: 0.1457 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00363: loss did not improve from 0.00000\n",
            "Epoch 364/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.1923e-04 - accuracy: 1.0000 - val_loss: 0.1374 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00364: loss did not improve from 0.00000\n",
            "Epoch 365/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.5951e-05 - accuracy: 1.0000 - val_loss: 0.1999 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00365: loss did not improve from 0.00000\n",
            "Epoch 366/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 5.2680e-06 - accuracy: 1.0000 - val_loss: 0.1672 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00366: loss did not improve from 0.00000\n",
            "Epoch 367/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.7598e-04 - accuracy: 1.0000 - val_loss: 0.1047 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00367: loss did not improve from 0.00000\n",
            "Epoch 368/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0039 - accuracy: 0.9951 - val_loss: 0.0911 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00368: loss did not improve from 0.00000\n",
            "Epoch 369/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.6474e-05 - accuracy: 1.0000 - val_loss: 0.1262 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00369: loss did not improve from 0.00000\n",
            "Epoch 370/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0105 - accuracy: 0.9975 - val_loss: 0.1860 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00370: loss did not improve from 0.00000\n",
            "Epoch 371/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.2531e-06 - accuracy: 1.0000 - val_loss: 0.1729 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00371: loss did not improve from 0.00000\n",
            "Epoch 372/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.5168e-04 - accuracy: 0.9998 - val_loss: 0.2899 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00372: loss did not improve from 0.00000\n",
            "Epoch 373/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0029 - accuracy: 0.9987 - val_loss: 0.1794 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00373: loss did not improve from 0.00000\n",
            "Epoch 374/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.1321 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00374: loss did not improve from 0.00000\n",
            "Epoch 375/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0052 - accuracy: 0.9993 - val_loss: 0.2207 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00375: loss did not improve from 0.00000\n",
            "Epoch 376/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0232 - accuracy: 0.9983 - val_loss: 0.0509 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00376: loss did not improve from 0.00000\n",
            "Epoch 377/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 5.8712e-06 - accuracy: 1.0000 - val_loss: 0.0431 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00377: loss did not improve from 0.00000\n",
            "Epoch 378/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.7059e-04 - accuracy: 1.0000 - val_loss: 0.0673 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00378: loss did not improve from 0.00000\n",
            "Epoch 379/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0025 - accuracy: 0.9998 - val_loss: 0.0137 - val_accuracy: 0.9919\n",
            "\n",
            "Epoch 00379: loss did not improve from 0.00000\n",
            "Epoch 380/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0629 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00380: loss did not improve from 0.00000\n",
            "Epoch 381/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0630 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00381: loss did not improve from 0.00000\n",
            "Epoch 382/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0036 - accuracy: 0.9964 - val_loss: 0.0628 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00382: loss did not improve from 0.00000\n",
            "Epoch 383/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.0253e-05 - accuracy: 1.0000 - val_loss: 0.0859 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00383: loss did not improve from 0.00000\n",
            "Epoch 384/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.5011e-05 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00384: loss did not improve from 0.00000\n",
            "Epoch 385/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.2414e-04 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00385: loss did not improve from 0.00000\n",
            "Epoch 386/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.2163e-07 - accuracy: 1.0000 - val_loss: 0.1738 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00386: loss did not improve from 0.00000\n",
            "Epoch 387/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.1677 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00387: loss did not improve from 0.00000\n",
            "Epoch 388/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.1181e-06 - accuracy: 1.0000 - val_loss: 0.1699 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00388: loss did not improve from 0.00000\n",
            "Epoch 389/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.2863e-07 - accuracy: 1.0000 - val_loss: 0.1670 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00389: loss did not improve from 0.00000\n",
            "Epoch 390/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.1281 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00390: loss did not improve from 0.00000\n",
            "Epoch 391/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 6.4354e-06 - accuracy: 1.0000 - val_loss: 0.0983 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00391: loss did not improve from 0.00000\n",
            "Epoch 392/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.1475e-05 - accuracy: 1.0000 - val_loss: 0.1940 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00392: loss did not improve from 0.00000\n",
            "Epoch 393/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 6.8946e-04 - accuracy: 0.9992 - val_loss: 0.2768 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00393: loss did not improve from 0.00000\n",
            "Epoch 394/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.1608 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00394: loss did not improve from 0.00000\n",
            "Epoch 395/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 9.3987e-05 - accuracy: 1.0000 - val_loss: 0.1289 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00395: loss did not improve from 0.00000\n",
            "Epoch 396/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0012 - accuracy: 0.9988 - val_loss: 0.0639 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00396: loss did not improve from 0.00000\n",
            "Epoch 397/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.9389e-04 - accuracy: 1.0000 - val_loss: 0.0998 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00397: loss did not improve from 0.00000\n",
            "Epoch 398/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 9.0775e-04 - accuracy: 1.0000 - val_loss: 0.1002 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00398: loss did not improve from 0.00000\n",
            "Epoch 399/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 8.6909e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00399: loss did not improve from 0.00000\n",
            "Epoch 400/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.8500e-05 - accuracy: 1.0000 - val_loss: 0.0584 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00400: loss did not improve from 0.00000\n",
            "Epoch 401/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0160 - accuracy: 0.9983 - val_loss: 0.0358 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00401: loss did not improve from 0.00000\n",
            "Epoch 402/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 5.9259e-04 - accuracy: 1.0000 - val_loss: 0.1069 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00402: loss did not improve from 0.00000\n",
            "Epoch 403/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.3171e-05 - accuracy: 1.0000 - val_loss: 0.1468 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00403: loss did not improve from 0.00000\n",
            "Epoch 404/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.5020e-07 - accuracy: 1.0000 - val_loss: 0.1382 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00404: loss did not improve from 0.00000\n",
            "Epoch 405/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0023 - accuracy: 0.9988 - val_loss: 0.1675 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00405: loss did not improve from 0.00000\n",
            "Epoch 406/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.3344e-06 - accuracy: 1.0000 - val_loss: 0.1747 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00406: loss did not improve from 0.00000\n",
            "Epoch 407/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.8922e-04 - accuracy: 1.0000 - val_loss: 0.0926 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00407: loss did not improve from 0.00000\n",
            "Epoch 408/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.1664e-06 - accuracy: 1.0000 - val_loss: 0.0946 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00408: loss did not improve from 0.00000\n",
            "Epoch 409/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 9.9545e-07 - accuracy: 1.0000 - val_loss: 0.1334 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00409: loss did not improve from 0.00000\n",
            "Epoch 410/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.6133e-05 - accuracy: 1.0000 - val_loss: 0.0569 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00410: loss did not improve from 0.00000\n",
            "Epoch 411/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.4833e-05 - accuracy: 1.0000 - val_loss: 0.1533 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00411: loss did not improve from 0.00000\n",
            "Epoch 412/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.6843e-05 - accuracy: 1.0000 - val_loss: 0.2309 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00412: loss did not improve from 0.00000\n",
            "Epoch 413/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.6681e-05 - accuracy: 1.0000 - val_loss: 0.2702 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00413: loss did not improve from 0.00000\n",
            "Epoch 414/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0159 - accuracy: 0.9945 - val_loss: 0.1513 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00414: loss did not improve from 0.00000\n",
            "Epoch 415/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.7765e-05 - accuracy: 1.0000 - val_loss: 0.1876 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00415: loss did not improve from 0.00000\n",
            "Epoch 416/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0031 - accuracy: 0.9988 - val_loss: 0.1882 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00416: loss did not improve from 0.00000\n",
            "Epoch 417/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0068 - accuracy: 0.9994 - val_loss: 0.2039 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00417: loss did not improve from 0.00000\n",
            "Epoch 418/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 7.0269e-05 - accuracy: 1.0000 - val_loss: 0.2167 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00418: loss did not improve from 0.00000\n",
            "Epoch 419/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 9.0057e-07 - accuracy: 1.0000 - val_loss: 0.2252 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00419: loss did not improve from 0.00000\n",
            "Epoch 420/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.6966e-04 - accuracy: 1.0000 - val_loss: 0.1737 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00420: loss did not improve from 0.00000\n",
            "Epoch 421/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 5.4144e-05 - accuracy: 1.0000 - val_loss: 0.2635 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00421: loss did not improve from 0.00000\n",
            "Epoch 422/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0152 - accuracy: 0.9976 - val_loss: 0.2394 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00422: loss did not improve from 0.00000\n",
            "Epoch 423/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.4698e-06 - accuracy: 1.0000 - val_loss: 0.2299 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00423: loss did not improve from 0.00000\n",
            "Epoch 424/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 9.9700e-05 - accuracy: 1.0000 - val_loss: 0.2487 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00424: loss did not improve from 0.00000\n",
            "Epoch 425/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 8.9879e-05 - accuracy: 1.0000 - val_loss: 0.2088 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00425: loss did not improve from 0.00000\n",
            "Epoch 426/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0084 - accuracy: 0.9970 - val_loss: 0.1687 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00426: loss did not improve from 0.00000\n",
            "Epoch 427/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.6593e-04 - accuracy: 1.0000 - val_loss: 0.2299 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00427: loss did not improve from 0.00000\n",
            "Epoch 428/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.1872e-04 - accuracy: 1.0000 - val_loss: 0.2048 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00428: loss did not improve from 0.00000\n",
            "Epoch 429/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.7484e-07 - accuracy: 1.0000 - val_loss: 0.1945 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00429: loss did not improve from 0.00000\n",
            "Epoch 430/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 5.3651e-06 - accuracy: 1.0000 - val_loss: 0.1777 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00430: loss did not improve from 0.00000\n",
            "Epoch 431/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.3782e-06 - accuracy: 1.0000 - val_loss: 0.2246 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00431: loss did not improve from 0.00000\n",
            "Epoch 432/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.4339e-06 - accuracy: 1.0000 - val_loss: 0.2683 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00432: loss did not improve from 0.00000\n",
            "Epoch 433/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.7760e-05 - accuracy: 1.0000 - val_loss: 0.6300 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00433: loss did not improve from 0.00000\n",
            "Epoch 434/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0023 - accuracy: 0.9983 - val_loss: 0.1673 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00434: loss did not improve from 0.00000\n",
            "Epoch 435/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 4.0614e-05 - accuracy: 1.0000 - val_loss: 0.1963 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00435: loss did not improve from 0.00000\n",
            "Epoch 436/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.2725e-06 - accuracy: 1.0000 - val_loss: 0.1981 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00436: loss did not improve from 0.00000\n",
            "Epoch 437/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.5048e-04 - accuracy: 0.9999 - val_loss: 0.1416 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00437: loss did not improve from 0.00000\n",
            "Epoch 438/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.0553e-06 - accuracy: 1.0000 - val_loss: 0.1076 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00438: loss did not improve from 0.00000\n",
            "Epoch 439/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.9751e-09 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00439: loss improved from 0.00000 to 0.00000, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 440/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.2329e-05 - accuracy: 1.0000 - val_loss: 0.2862 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00440: loss did not improve from 0.00000\n",
            "Epoch 441/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0299 - accuracy: 0.9951 - val_loss: 0.2206 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00441: loss did not improve from 0.00000\n",
            "Epoch 442/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.3612e-04 - accuracy: 1.0000 - val_loss: 0.1449 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00442: loss did not improve from 0.00000\n",
            "Epoch 443/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0043 - accuracy: 0.9978 - val_loss: 0.0867 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00443: loss did not improve from 0.00000\n",
            "Epoch 444/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.9281e-04 - accuracy: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00444: loss did not improve from 0.00000\n",
            "Epoch 445/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.5982e-04 - accuracy: 0.9999 - val_loss: 0.4805 - val_accuracy: 0.9715\n",
            "\n",
            "Epoch 00445: loss did not improve from 0.00000\n",
            "Epoch 446/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.0569e-05 - accuracy: 1.0000 - val_loss: 0.1902 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00446: loss did not improve from 0.00000\n",
            "Epoch 447/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.2282e-05 - accuracy: 1.0000 - val_loss: 0.1992 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00447: loss did not improve from 0.00000\n",
            "Epoch 448/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.8020e-05 - accuracy: 1.0000 - val_loss: 0.1775 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00448: loss did not improve from 0.00000\n",
            "Epoch 449/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 8.5484e-04 - accuracy: 0.9997 - val_loss: 0.1850 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00449: loss did not improve from 0.00000\n",
            "Epoch 450/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 9.0518e-06 - accuracy: 1.0000 - val_loss: 0.2560 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00450: loss did not improve from 0.00000\n",
            "Epoch 451/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.8472e-06 - accuracy: 1.0000 - val_loss: 0.2354 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00451: loss did not improve from 0.00000\n",
            "Epoch 452/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.8148e-07 - accuracy: 1.0000 - val_loss: 0.2271 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00452: loss did not improve from 0.00000\n",
            "Epoch 453/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0066 - accuracy: 0.9994 - val_loss: 0.1990 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00453: loss did not improve from 0.00000\n",
            "Epoch 454/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 6.1216e-04 - accuracy: 0.9995 - val_loss: 0.2554 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00454: loss did not improve from 0.00000\n",
            "Epoch 455/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.0195e-05 - accuracy: 1.0000 - val_loss: 0.2797 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00455: loss did not improve from 0.00000\n",
            "Epoch 456/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 6.1829e-05 - accuracy: 1.0000 - val_loss: 0.2273 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00456: loss did not improve from 0.00000\n",
            "Epoch 457/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.3396 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00457: loss did not improve from 0.00000\n",
            "Epoch 458/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0047 - accuracy: 0.9975 - val_loss: 0.2288 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00458: loss did not improve from 0.00000\n",
            "Epoch 459/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.7866e-06 - accuracy: 1.0000 - val_loss: 0.2295 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00459: loss did not improve from 0.00000\n",
            "Epoch 460/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.4831e-07 - accuracy: 1.0000 - val_loss: 0.2272 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00460: loss did not improve from 0.00000\n",
            "Epoch 461/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.1445e-08 - accuracy: 1.0000 - val_loss: 0.2168 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00461: loss did not improve from 0.00000\n",
            "Epoch 462/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0053 - accuracy: 0.9961 - val_loss: 0.2229 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00462: loss did not improve from 0.00000\n",
            "Epoch 463/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0144 - accuracy: 0.9973 - val_loss: 0.2137 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00463: loss did not improve from 0.00000\n",
            "Epoch 464/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.7437e-06 - accuracy: 1.0000 - val_loss: 0.2440 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00464: loss did not improve from 0.00000\n",
            "Epoch 465/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.6764e-07 - accuracy: 1.0000 - val_loss: 0.2228 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00465: loss did not improve from 0.00000\n",
            "Epoch 466/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.9875e-07 - accuracy: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00466: loss did not improve from 0.00000\n",
            "Epoch 467/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.9427e-07 - accuracy: 1.0000 - val_loss: 0.1464 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00467: loss did not improve from 0.00000\n",
            "Epoch 468/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.8273e-04 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00468: loss did not improve from 0.00000\n",
            "Epoch 469/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 6.1148e-08 - accuracy: 1.0000 - val_loss: 0.1762 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00469: loss did not improve from 0.00000\n",
            "Epoch 470/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.3429e-05 - accuracy: 1.0000 - val_loss: 0.1907 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00470: loss did not improve from 0.00000\n",
            "Epoch 471/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0190 - accuracy: 0.9964 - val_loss: 0.1980 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00471: loss did not improve from 0.00000\n",
            "Epoch 472/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.1264e-05 - accuracy: 1.0000 - val_loss: 0.1750 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00472: loss did not improve from 0.00000\n",
            "Epoch 473/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 6.1449e-08 - accuracy: 1.0000 - val_loss: 0.1834 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00473: loss did not improve from 0.00000\n",
            "Epoch 474/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 8.3674e-04 - accuracy: 1.0000 - val_loss: 0.2197 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00474: loss did not improve from 0.00000\n",
            "Epoch 475/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.0679e-06 - accuracy: 1.0000 - val_loss: 0.2159 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00475: loss did not improve from 0.00000\n",
            "Epoch 476/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.2942e-04 - accuracy: 1.0000 - val_loss: 0.1627 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00476: loss did not improve from 0.00000\n",
            "Epoch 477/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.3190e-04 - accuracy: 1.0000 - val_loss: 0.0920 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00477: loss did not improve from 0.00000\n",
            "Epoch 478/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.1257e-05 - accuracy: 1.0000 - val_loss: 0.0698 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00478: loss did not improve from 0.00000\n",
            "Epoch 479/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.8062e-05 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00479: loss did not improve from 0.00000\n",
            "Epoch 480/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.9338e-05 - accuracy: 1.0000 - val_loss: 0.1047 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00480: loss did not improve from 0.00000\n",
            "Epoch 481/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.2326e-08 - accuracy: 1.0000 - val_loss: 0.1065 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00481: loss did not improve from 0.00000\n",
            "Epoch 482/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.0081e-07 - accuracy: 1.0000 - val_loss: 0.1027 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00482: loss did not improve from 0.00000\n",
            "Epoch 483/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.2604e-06 - accuracy: 1.0000 - val_loss: 0.1459 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00483: loss did not improve from 0.00000\n",
            "Epoch 484/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.1097e-08 - accuracy: 1.0000 - val_loss: 0.1779 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00484: loss did not improve from 0.00000\n",
            "Epoch 485/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0014 - accuracy: 0.9991 - val_loss: 0.0683 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00485: loss did not improve from 0.00000\n",
            "Epoch 486/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.4730e-06 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00486: loss did not improve from 0.00000\n",
            "Epoch 487/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.6784e-05 - accuracy: 1.0000 - val_loss: 0.0779 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00487: loss did not improve from 0.00000\n",
            "Epoch 488/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.7636e-07 - accuracy: 1.0000 - val_loss: 0.1081 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00488: loss did not improve from 0.00000\n",
            "Epoch 489/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.6556e-07 - accuracy: 1.0000 - val_loss: 0.1150 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00489: loss did not improve from 0.00000\n",
            "Epoch 490/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 8.2484e-06 - accuracy: 1.0000 - val_loss: 0.1670 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00490: loss did not improve from 0.00000\n",
            "Epoch 491/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.9393e-04 - accuracy: 0.9997 - val_loss: 0.2700 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00491: loss did not improve from 0.00000\n",
            "Epoch 492/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.3833e-06 - accuracy: 1.0000 - val_loss: 0.2369 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00492: loss did not improve from 0.00000\n",
            "Epoch 493/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0011 - accuracy: 0.9985 - val_loss: 0.2159 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00493: loss did not improve from 0.00000\n",
            "Epoch 494/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.2120 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00494: loss did not improve from 0.00000\n",
            "Epoch 495/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.8713e-06 - accuracy: 1.0000 - val_loss: 0.1735 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00495: loss did not improve from 0.00000\n",
            "Epoch 496/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.2301e-08 - accuracy: 1.0000 - val_loss: 0.1746 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00496: loss did not improve from 0.00000\n",
            "Epoch 497/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.1766e-04 - accuracy: 1.0000 - val_loss: 0.1986 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00497: loss did not improve from 0.00000\n",
            "Epoch 498/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.8929e-07 - accuracy: 1.0000 - val_loss: 0.1951 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00498: loss did not improve from 0.00000\n",
            "Epoch 499/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.3999e-07 - accuracy: 1.0000 - val_loss: 0.1824 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00499: loss did not improve from 0.00000\n",
            "Epoch 500/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 9.9682e-10 - accuracy: 1.0000 - val_loss: 0.1814 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00500: loss improved from 0.00000 to 0.00000, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 501/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1729 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00501: loss did not improve from 0.00000\n",
            "Epoch 502/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.1270e-04 - accuracy: 1.0000 - val_loss: 0.2145 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00502: loss did not improve from 0.00000\n",
            "Epoch 503/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.2052e-04 - accuracy: 1.0000 - val_loss: 0.0710 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00503: loss did not improve from 0.00000\n",
            "Epoch 504/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 7.9587e-06 - accuracy: 1.0000 - val_loss: 0.1569 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00504: loss did not improve from 0.00000\n",
            "Epoch 505/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.7885e-04 - accuracy: 0.9999 - val_loss: 0.1966 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00505: loss did not improve from 0.00000\n",
            "Epoch 506/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.6013e-06 - accuracy: 1.0000 - val_loss: 0.1848 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00506: loss did not improve from 0.00000\n",
            "Epoch 507/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 8.9403e-06 - accuracy: 1.0000 - val_loss: 0.1930 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00507: loss did not improve from 0.00000\n",
            "Epoch 508/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.6723e-04 - accuracy: 1.0000 - val_loss: 0.2264 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00508: loss did not improve from 0.00000\n",
            "Epoch 509/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 5.7019e-04 - accuracy: 1.0000 - val_loss: 0.2061 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00509: loss did not improve from 0.00000\n",
            "Epoch 510/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 0.1474 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00510: loss did not improve from 0.00000\n",
            "Epoch 511/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 9.1604e-05 - accuracy: 1.0000 - val_loss: 0.1502 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00511: loss did not improve from 0.00000\n",
            "Epoch 512/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.3931e-06 - accuracy: 1.0000 - val_loss: 0.1393 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00512: loss did not improve from 0.00000\n",
            "Epoch 513/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.5979e-06 - accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00513: loss did not improve from 0.00000\n",
            "Epoch 514/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 5.0670e-05 - accuracy: 1.0000 - val_loss: 0.2447 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00514: loss did not improve from 0.00000\n",
            "Epoch 515/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 5.2623e-07 - accuracy: 1.0000 - val_loss: 0.2577 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00515: loss did not improve from 0.00000\n",
            "Epoch 516/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.2217e-08 - accuracy: 1.0000 - val_loss: 0.2326 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00516: loss did not improve from 0.00000\n",
            "Epoch 517/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.6577e-05 - accuracy: 1.0000 - val_loss: 0.2784 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00517: loss did not improve from 0.00000\n",
            "Epoch 518/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.2397 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00518: loss did not improve from 0.00000\n",
            "Epoch 519/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0010 - accuracy: 0.9995 - val_loss: 0.2258 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00519: loss did not improve from 0.00000\n",
            "Epoch 520/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 6.0900e-07 - accuracy: 1.0000 - val_loss: 0.2313 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00520: loss did not improve from 0.00000\n",
            "Epoch 521/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.3518e-05 - accuracy: 1.0000 - val_loss: 0.2907 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00521: loss did not improve from 0.00000\n",
            "Epoch 522/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.4031e-08 - accuracy: 1.0000 - val_loss: 0.2919 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00522: loss did not improve from 0.00000\n",
            "Epoch 523/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 8.1882e-06 - accuracy: 1.0000 - val_loss: 0.2604 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00523: loss did not improve from 0.00000\n",
            "Epoch 524/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 9.7128e-09 - accuracy: 1.0000 - val_loss: 0.2637 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00524: loss did not improve from 0.00000\n",
            "Epoch 525/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0033 - accuracy: 0.9983 - val_loss: 0.2501 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00525: loss did not improve from 0.00000\n",
            "Epoch 526/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.9163e-04 - accuracy: 1.0000 - val_loss: 0.2203 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00526: loss did not improve from 0.00000\n",
            "Epoch 527/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.6893e-04 - accuracy: 1.0000 - val_loss: 0.2104 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00527: loss did not improve from 0.00000\n",
            "Epoch 528/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.0377e-06 - accuracy: 1.0000 - val_loss: 0.2184 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00528: loss did not improve from 0.00000\n",
            "Epoch 529/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.8391e-06 - accuracy: 1.0000 - val_loss: 0.2534 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00529: loss did not improve from 0.00000\n",
            "Epoch 530/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3259 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00530: loss did not improve from 0.00000\n",
            "Epoch 531/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2406 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00531: loss did not improve from 0.00000\n",
            "Epoch 532/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.7469e-09 - accuracy: 1.0000 - val_loss: 0.2418 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00532: loss did not improve from 0.00000\n",
            "Epoch 533/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.9252e-08 - accuracy: 1.0000 - val_loss: 0.2456 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00533: loss did not improve from 0.00000\n",
            "Epoch 534/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.3590e-05 - accuracy: 1.0000 - val_loss: 0.1621 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00534: loss did not improve from 0.00000\n",
            "Epoch 535/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.0701e-04 - accuracy: 0.9996 - val_loss: 0.1563 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00535: loss did not improve from 0.00000\n",
            "Epoch 536/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0096 - accuracy: 0.9988 - val_loss: 0.2543 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00536: loss did not improve from 0.00000\n",
            "Epoch 537/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 7.7008e-07 - accuracy: 1.0000 - val_loss: 0.2441 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00537: loss did not improve from 0.00000\n",
            "Epoch 538/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.3611e-06 - accuracy: 1.0000 - val_loss: 0.2718 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00538: loss did not improve from 0.00000\n",
            "Epoch 539/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.5140e-06 - accuracy: 1.0000 - val_loss: 0.2337 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00539: loss did not improve from 0.00000\n",
            "Epoch 540/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 3.3849e-07 - accuracy: 1.0000 - val_loss: 0.2304 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00540: loss did not improve from 0.00000\n",
            "Epoch 541/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.4166e-09 - accuracy: 1.0000 - val_loss: 0.2290 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00541: loss did not improve from 0.00000\n",
            "Epoch 542/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.0433e-08 - accuracy: 1.0000 - val_loss: 0.2306 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00542: loss did not improve from 0.00000\n",
            "Epoch 543/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0066 - accuracy: 0.9989 - val_loss: 0.1452 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00543: loss did not improve from 0.00000\n",
            "Epoch 544/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.0105e-05 - accuracy: 1.0000 - val_loss: 0.1977 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00544: loss did not improve from 0.00000\n",
            "Epoch 545/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.9775e-07 - accuracy: 1.0000 - val_loss: 0.1926 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00545: loss did not improve from 0.00000\n",
            "Epoch 546/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.4364e-09 - accuracy: 1.0000 - val_loss: 0.1924 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00546: loss improved from 0.00000 to 0.00000, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 547/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.5414e-06 - accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00547: loss did not improve from 0.00000\n",
            "Epoch 548/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2779 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00548: loss did not improve from 0.00000\n",
            "Epoch 549/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.3232e-09 - accuracy: 1.0000 - val_loss: 0.2773 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00549: loss did not improve from 0.00000\n",
            "Epoch 550/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.3704e-07 - accuracy: 1.0000 - val_loss: 0.2674 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00550: loss did not improve from 0.00000\n",
            "Epoch 551/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.1751e-07 - accuracy: 1.0000 - val_loss: 0.3072 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00551: loss did not improve from 0.00000\n",
            "Epoch 552/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.1833e-05 - accuracy: 1.0000 - val_loss: 0.3023 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00552: loss did not improve from 0.00000\n",
            "Epoch 553/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.6846e-07 - accuracy: 1.0000 - val_loss: 0.2639 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00553: loss did not improve from 0.00000\n",
            "Epoch 554/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0210 - accuracy: 0.9969 - val_loss: 0.2205 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00554: loss did not improve from 0.00000\n",
            "Epoch 555/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.5866e-06 - accuracy: 1.0000 - val_loss: 0.2326 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00555: loss did not improve from 0.00000\n",
            "Epoch 556/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0189 - accuracy: 0.9964 - val_loss: 0.2142 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00556: loss did not improve from 0.00000\n",
            "Epoch 557/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.7443e-05 - accuracy: 1.0000 - val_loss: 0.2938 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00557: loss did not improve from 0.00000\n",
            "Epoch 558/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.3473e-06 - accuracy: 1.0000 - val_loss: 0.3057 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00558: loss did not improve from 0.00000\n",
            "Epoch 559/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.7722e-06 - accuracy: 1.0000 - val_loss: 0.3184 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00559: loss did not improve from 0.00000\n",
            "Epoch 560/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.9709e-06 - accuracy: 1.0000 - val_loss: 0.2637 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00560: loss did not improve from 0.00000\n",
            "Epoch 561/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.0253e-04 - accuracy: 1.0000 - val_loss: 0.2211 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00561: loss did not improve from 0.00000\n",
            "Epoch 562/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 4.1816e-04 - accuracy: 0.9995 - val_loss: 0.2292 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00562: loss did not improve from 0.00000\n",
            "Epoch 563/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 4.4811e-05 - accuracy: 1.0000 - val_loss: 0.2949 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00563: loss did not improve from 0.00000\n",
            "Epoch 564/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.7205e-06 - accuracy: 1.0000 - val_loss: 0.2801 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00564: loss did not improve from 0.00000\n",
            "Epoch 565/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 5.9236e-08 - accuracy: 1.0000 - val_loss: 0.2892 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00565: loss did not improve from 0.00000\n",
            "Epoch 566/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.2585e-04 - accuracy: 1.0000 - val_loss: 0.4207 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00566: loss did not improve from 0.00000\n",
            "Epoch 567/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0238 - accuracy: 0.9929 - val_loss: 0.3068 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00567: loss did not improve from 0.00000\n",
            "Epoch 568/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.0633e-08 - accuracy: 1.0000 - val_loss: 0.3028 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00568: loss did not improve from 0.00000\n",
            "Epoch 569/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 2.0881e-06 - accuracy: 1.0000 - val_loss: 0.3296 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00569: loss did not improve from 0.00000\n",
            "Epoch 570/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.2656e-06 - accuracy: 1.0000 - val_loss: 0.3380 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00570: loss did not improve from 0.00000\n",
            "Epoch 571/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.4810e-06 - accuracy: 1.0000 - val_loss: 0.3231 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00571: loss did not improve from 0.00000\n",
            "Epoch 572/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.1951e-05 - accuracy: 1.0000 - val_loss: 0.3383 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00572: loss did not improve from 0.00000\n",
            "Epoch 573/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.1685e-06 - accuracy: 1.0000 - val_loss: 0.3222 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00573: loss did not improve from 0.00000\n",
            "Epoch 574/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.1893e-06 - accuracy: 1.0000 - val_loss: 0.4205 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00574: loss did not improve from 0.00000\n",
            "Epoch 575/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 2.1048e-07 - accuracy: 1.0000 - val_loss: 0.3591 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00575: loss did not improve from 0.00000\n",
            "Epoch 576/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.6854e-06 - accuracy: 1.0000 - val_loss: 0.1970 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00576: loss did not improve from 0.00000\n",
            "Epoch 577/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.8491e-07 - accuracy: 1.0000 - val_loss: 0.4159 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00577: loss did not improve from 0.00000\n",
            "Epoch 578/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.2368e-07 - accuracy: 1.0000 - val_loss: 0.3980 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00578: loss did not improve from 0.00000\n",
            "Epoch 579/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.0907e-06 - accuracy: 1.0000 - val_loss: 0.7673 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00579: loss did not improve from 0.00000\n",
            "Epoch 580/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 0.3104 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00580: loss did not improve from 0.00000\n",
            "Epoch 581/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.3199e-09 - accuracy: 1.0000 - val_loss: 0.3104 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00581: loss did not improve from 0.00000\n",
            "Epoch 582/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.2887e-06 - accuracy: 1.0000 - val_loss: 0.3153 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00582: loss did not improve from 0.00000\n",
            "Epoch 583/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.2706 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00583: loss did not improve from 0.00000\n",
            "Epoch 584/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.2900e-05 - accuracy: 1.0000 - val_loss: 0.1880 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00584: loss did not improve from 0.00000\n",
            "Epoch 585/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.6063e-05 - accuracy: 1.0000 - val_loss: 0.2912 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00585: loss did not improve from 0.00000\n",
            "Epoch 586/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.1938e-07 - accuracy: 1.0000 - val_loss: 0.2729 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00586: loss did not improve from 0.00000\n",
            "Epoch 587/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.0927e-05 - accuracy: 1.0000 - val_loss: 0.2790 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00587: loss did not improve from 0.00000\n",
            "Epoch 588/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.8138e-09 - accuracy: 1.0000 - val_loss: 0.2787 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00588: loss did not improve from 0.00000\n",
            "Epoch 589/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0045 - accuracy: 0.9968 - val_loss: 0.3583 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00589: loss did not improve from 0.00000\n",
            "Epoch 590/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.9200e-06 - accuracy: 1.0000 - val_loss: 0.3609 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00590: loss did not improve from 0.00000\n",
            "Epoch 591/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.4873e-08 - accuracy: 1.0000 - val_loss: 0.3728 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00591: loss did not improve from 0.00000\n",
            "Epoch 592/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.4885e-08 - accuracy: 1.0000 - val_loss: 0.3751 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00592: loss did not improve from 0.00000\n",
            "Epoch 593/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 8.4433e-07 - accuracy: 1.0000 - val_loss: 0.3212 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00593: loss did not improve from 0.00000\n",
            "Epoch 594/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 4.2318e-06 - accuracy: 1.0000 - val_loss: 0.3763 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00594: loss did not improve from 0.00000\n",
            "Epoch 595/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.0033e-09 - accuracy: 1.0000 - val_loss: 0.3725 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00595: loss did not improve from 0.00000\n",
            "Epoch 596/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.3725 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00596: loss improved from 0.00000 to 0.00000, saving model to /content/drive/MyDrive/Colab/DL/CKPLUS/Emotion_detection.h5\n",
            "Epoch 597/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 5.6477e-07 - accuracy: 1.0000 - val_loss: 0.2920 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00597: loss did not improve from 0.00000\n",
            "Epoch 598/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0126 - accuracy: 0.9991 - val_loss: 0.4707 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00598: loss did not improve from 0.00000\n",
            "Epoch 599/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.9951e-04 - accuracy: 1.0000 - val_loss: 0.2462 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00599: loss did not improve from 0.00000\n",
            "Epoch 600/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0130 - accuracy: 0.9989 - val_loss: 0.2698 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00600: loss did not improve from 0.00000\n",
            "Epoch 601/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 4.5946e-07 - accuracy: 1.0000 - val_loss: 0.2443 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00601: loss did not improve from 0.00000\n",
            "Epoch 602/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.9458e-05 - accuracy: 1.0000 - val_loss: 0.2271 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00602: loss did not improve from 0.00000\n",
            "Epoch 603/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.3399e-06 - accuracy: 1.0000 - val_loss: 0.2115 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00603: loss did not improve from 0.00000\n",
            "Epoch 604/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 4.4753e-06 - accuracy: 1.0000 - val_loss: 0.1699 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00604: loss did not improve from 0.00000\n",
            "Epoch 605/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 6.7865e-05 - accuracy: 1.0000 - val_loss: 0.1143 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00605: loss did not improve from 0.00000\n",
            "Epoch 606/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.8371e-04 - accuracy: 0.9999 - val_loss: 0.0877 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00606: loss did not improve from 0.00000\n",
            "Epoch 607/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 5.1032e-04 - accuracy: 1.0000 - val_loss: 0.1372 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00607: loss did not improve from 0.00000\n",
            "Epoch 608/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.2934e-07 - accuracy: 1.0000 - val_loss: 0.1071 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00608: loss did not improve from 0.00000\n",
            "Epoch 609/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.3494e-06 - accuracy: 1.0000 - val_loss: 0.0994 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00609: loss did not improve from 0.00000\n",
            "Epoch 610/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0994 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00610: loss did not improve from 0.00000\n",
            "Epoch 611/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0770 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00611: loss did not improve from 0.00000\n",
            "Epoch 612/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0062 - accuracy: 0.9994 - val_loss: 0.3228 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00612: loss did not improve from 0.00000\n",
            "Epoch 613/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 6.1721e-04 - accuracy: 1.0000 - val_loss: 0.3179 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00613: loss did not improve from 0.00000\n",
            "Epoch 614/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.4615e-06 - accuracy: 1.0000 - val_loss: 0.3171 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00614: loss did not improve from 0.00000\n",
            "Epoch 615/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0023 - accuracy: 0.9990 - val_loss: 0.1938 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00615: loss did not improve from 0.00000\n",
            "Epoch 616/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.7038e-07 - accuracy: 1.0000 - val_loss: 0.1902 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00616: loss did not improve from 0.00000\n",
            "Epoch 617/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 5.3961e-06 - accuracy: 1.0000 - val_loss: 0.1640 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00617: loss did not improve from 0.00000\n",
            "Epoch 618/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0011 - accuracy: 0.9990 - val_loss: 0.2191 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00618: loss did not improve from 0.00000\n",
            "Epoch 619/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.2010 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00619: loss did not improve from 0.00000\n",
            "Epoch 620/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0015 - accuracy: 0.9990 - val_loss: 0.2975 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00620: loss did not improve from 0.00000\n",
            "Epoch 621/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 2.6629e-05 - accuracy: 1.0000 - val_loss: 0.2983 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00621: loss did not improve from 0.00000\n",
            "Epoch 622/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0093 - accuracy: 0.9991 - val_loss: 0.2898 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00622: loss did not improve from 0.00000\n",
            "Epoch 623/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0054 - accuracy: 0.9989 - val_loss: 0.2308 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00623: loss did not improve from 0.00000\n",
            "Epoch 624/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 8.8163e-06 - accuracy: 1.0000 - val_loss: 0.2444 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00624: loss did not improve from 0.00000\n",
            "Epoch 625/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.2694 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00625: loss did not improve from 0.00000\n",
            "Epoch 626/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 9.9524e-07 - accuracy: 1.0000 - val_loss: 0.2604 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00626: loss did not improve from 0.00000\n",
            "Epoch 627/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.1884e-05 - accuracy: 1.0000 - val_loss: 0.2850 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00627: loss did not improve from 0.00000\n",
            "Epoch 628/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.1780e-06 - accuracy: 1.0000 - val_loss: 0.2839 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00628: loss did not improve from 0.00000\n",
            "Epoch 629/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 3.3044e-06 - accuracy: 1.0000 - val_loss: 0.2848 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00629: loss did not improve from 0.00000\n",
            "Epoch 630/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 7.2544e-08 - accuracy: 1.0000 - val_loss: 0.2475 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00630: loss did not improve from 0.00000\n",
            "Epoch 631/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.6195e-07 - accuracy: 1.0000 - val_loss: 0.2326 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00631: loss did not improve from 0.00000\n",
            "Epoch 632/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.3431 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00632: loss did not improve from 0.00000\n",
            "Epoch 633/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.1374e-07 - accuracy: 1.0000 - val_loss: 0.3539 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00633: loss did not improve from 0.00000\n",
            "Epoch 634/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 8.8564e-05 - accuracy: 1.0000 - val_loss: 0.3336 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00634: loss did not improve from 0.00000\n",
            "Epoch 635/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 9.8974e-04 - accuracy: 0.9996 - val_loss: 0.2946 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00635: loss did not improve from 0.00000\n",
            "Epoch 636/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.1848e-05 - accuracy: 1.0000 - val_loss: 0.3091 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00636: loss did not improve from 0.00000\n",
            "Epoch 637/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 7.7904e-05 - accuracy: 1.0000 - val_loss: 0.3064 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00637: loss did not improve from 0.00000\n",
            "Epoch 638/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.7229e-08 - accuracy: 1.0000 - val_loss: 0.3072 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00638: loss did not improve from 0.00000\n",
            "Epoch 639/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.0940e-04 - accuracy: 1.0000 - val_loss: 0.3817 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00639: loss did not improve from 0.00000\n",
            "Epoch 640/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 6.5446e-06 - accuracy: 1.0000 - val_loss: 0.2578 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00640: loss did not improve from 0.00000\n",
            "Epoch 641/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.8795e-06 - accuracy: 1.0000 - val_loss: 0.2273 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00641: loss did not improve from 0.00000\n",
            "Epoch 642/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.3586e-06 - accuracy: 1.0000 - val_loss: 0.2721 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00642: loss did not improve from 0.00000\n",
            "Epoch 643/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 8.1887e-04 - accuracy: 0.9997 - val_loss: 0.2674 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00643: loss did not improve from 0.00000\n",
            "Epoch 644/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.7188e-04 - accuracy: 1.0000 - val_loss: 0.2511 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00644: loss did not improve from 0.00000\n",
            "Epoch 645/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 4.3473e-06 - accuracy: 1.0000 - val_loss: 0.2597 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00645: loss did not improve from 0.00000\n",
            "Epoch 646/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.5850e-05 - accuracy: 1.0000 - val_loss: 0.2843 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00646: loss did not improve from 0.00000\n",
            "Epoch 647/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.3974e-05 - accuracy: 1.0000 - val_loss: 0.2284 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00647: loss did not improve from 0.00000\n",
            "Epoch 648/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.2362 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00648: loss did not improve from 0.00000\n",
            "Epoch 649/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 6.3104e-07 - accuracy: 1.0000 - val_loss: 0.1984 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00649: loss did not improve from 0.00000\n",
            "Epoch 650/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.4199e-07 - accuracy: 1.0000 - val_loss: 0.2048 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00650: loss did not improve from 0.00000\n",
            "Epoch 651/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0125 - accuracy: 0.9985 - val_loss: 0.2521 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00651: loss did not improve from 0.00000\n",
            "Epoch 652/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 6.8442e-06 - accuracy: 1.0000 - val_loss: 0.2397 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00652: loss did not improve from 0.00000\n",
            "Epoch 653/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.6803e-06 - accuracy: 1.0000 - val_loss: 0.2378 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00653: loss did not improve from 0.00000\n",
            "Epoch 654/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.7697e-05 - accuracy: 1.0000 - val_loss: 0.2502 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00654: loss did not improve from 0.00000\n",
            "Epoch 655/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.0497e-07 - accuracy: 1.0000 - val_loss: 0.2606 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00655: loss did not improve from 0.00000\n",
            "Epoch 656/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.4690e-07 - accuracy: 1.0000 - val_loss: 0.2677 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00656: loss did not improve from 0.00000\n",
            "Epoch 657/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.5856e-06 - accuracy: 1.0000 - val_loss: 0.2215 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00657: loss did not improve from 0.00000\n",
            "Epoch 658/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0103 - accuracy: 0.9964 - val_loss: 0.3403 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00658: loss did not improve from 0.00000\n",
            "Epoch 659/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.2306e-08 - accuracy: 1.0000 - val_loss: 0.3388 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00659: loss did not improve from 0.00000\n",
            "Epoch 660/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.0006e-07 - accuracy: 1.0000 - val_loss: 0.3388 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00660: loss did not improve from 0.00000\n",
            "Epoch 661/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.0195e-05 - accuracy: 1.0000 - val_loss: 0.2240 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00661: loss did not improve from 0.00000\n",
            "Epoch 662/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 8.0750e-06 - accuracy: 1.0000 - val_loss: 0.1603 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00662: loss did not improve from 0.00000\n",
            "Epoch 663/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.3939e-04 - accuracy: 1.0000 - val_loss: 0.2692 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00663: loss did not improve from 0.00000\n",
            "Epoch 664/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.8463e-06 - accuracy: 1.0000 - val_loss: 0.2953 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00664: loss did not improve from 0.00000\n",
            "Epoch 665/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 6.1501e-07 - accuracy: 1.0000 - val_loss: 0.4332 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00665: loss did not improve from 0.00000\n",
            "Epoch 666/1000\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.3445 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00666: loss did not improve from 0.00000\n",
            "Epoch 667/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.5965e-04 - accuracy: 1.0000 - val_loss: 0.2034 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00667: loss did not improve from 0.00000\n",
            "Epoch 668/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.8724e-06 - accuracy: 1.0000 - val_loss: 0.2176 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00668: loss did not improve from 0.00000\n",
            "Epoch 669/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.6813e-06 - accuracy: 1.0000 - val_loss: 0.2040 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00669: loss did not improve from 0.00000\n",
            "Epoch 670/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.2055e-05 - accuracy: 1.0000 - val_loss: 0.2829 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00670: loss did not improve from 0.00000\n",
            "Epoch 671/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0052 - accuracy: 0.9991 - val_loss: 0.2452 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00671: loss did not improve from 0.00000\n",
            "Epoch 672/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.1299e-06 - accuracy: 1.0000 - val_loss: 0.2700 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00672: loss did not improve from 0.00000\n",
            "Epoch 673/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.0146e-08 - accuracy: 1.0000 - val_loss: 0.2682 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00673: loss did not improve from 0.00000\n",
            "Epoch 674/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.3050 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00674: loss did not improve from 0.00000\n",
            "Epoch 675/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 8.8616e-07 - accuracy: 1.0000 - val_loss: 0.2864 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00675: loss did not improve from 0.00000\n",
            "Epoch 676/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0076 - accuracy: 0.9989 - val_loss: 0.2729 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00676: loss did not improve from 0.00000\n",
            "Epoch 677/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 9.3802e-07 - accuracy: 1.0000 - val_loss: 0.2658 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00677: loss did not improve from 0.00000\n",
            "Epoch 678/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.2414e-04 - accuracy: 1.0000 - val_loss: 0.2057 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00678: loss did not improve from 0.00000\n",
            "Epoch 679/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 5.3771e-09 - accuracy: 1.0000 - val_loss: 0.2063 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00679: loss did not improve from 0.00000\n",
            "Epoch 680/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 8.5733e-07 - accuracy: 1.0000 - val_loss: 0.1637 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00680: loss did not improve from 0.00000\n",
            "Epoch 681/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.9615e-07 - accuracy: 1.0000 - val_loss: 0.1994 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00681: loss did not improve from 0.00000\n",
            "Epoch 682/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.8955e-09 - accuracy: 1.0000 - val_loss: 0.2136 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00682: loss did not improve from 0.00000\n",
            "Epoch 683/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 8.1891e-07 - accuracy: 1.0000 - val_loss: 0.4600 - val_accuracy: 0.9715\n",
            "\n",
            "Epoch 00683: loss did not improve from 0.00000\n",
            "Epoch 684/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.1775e-04 - accuracy: 1.0000 - val_loss: 0.3162 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00684: loss did not improve from 0.00000\n",
            "Epoch 685/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.1164e-04 - accuracy: 1.0000 - val_loss: 0.2919 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00685: loss did not improve from 0.00000\n",
            "Epoch 686/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.2919 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00686: loss did not improve from 0.00000\n",
            "Epoch 687/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.2919 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00687: loss did not improve from 0.00000\n",
            "Epoch 688/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.3390 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00688: loss did not improve from 0.00000\n",
            "Epoch 689/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 8.0273e-08 - accuracy: 1.0000 - val_loss: 0.3617 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00689: loss did not improve from 0.00000\n",
            "Epoch 690/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.3353 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00690: loss did not improve from 0.00000\n",
            "Epoch 691/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.7410e-07 - accuracy: 1.0000 - val_loss: 0.3397 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00691: loss did not improve from 0.00000\n",
            "Epoch 692/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.0922e-04 - accuracy: 1.0000 - val_loss: 0.3972 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00692: loss did not improve from 0.00000\n",
            "Epoch 693/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.2813e-07 - accuracy: 1.0000 - val_loss: 0.3952 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00693: loss did not improve from 0.00000\n",
            "Epoch 694/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.3710e-08 - accuracy: 1.0000 - val_loss: 0.3942 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00694: loss did not improve from 0.00000\n",
            "Epoch 695/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 9.1341e-09 - accuracy: 1.0000 - val_loss: 0.3949 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00695: loss did not improve from 0.00000\n",
            "Epoch 696/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.6352e-08 - accuracy: 1.0000 - val_loss: 0.3944 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00696: loss did not improve from 0.00000\n",
            "Epoch 697/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.5749e-05 - accuracy: 1.0000 - val_loss: 0.3977 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00697: loss did not improve from 0.00000\n",
            "Epoch 698/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.9395e-08 - accuracy: 1.0000 - val_loss: 0.3901 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00698: loss did not improve from 0.00000\n",
            "Epoch 699/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 7.2106e-04 - accuracy: 0.9996 - val_loss: 0.4335 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00699: loss did not improve from 0.00000\n",
            "Epoch 700/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.4927e-06 - accuracy: 1.0000 - val_loss: 0.4021 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00700: loss did not improve from 0.00000\n",
            "Epoch 701/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.4279e-04 - accuracy: 0.9998 - val_loss: 0.1415 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00701: loss did not improve from 0.00000\n",
            "Epoch 702/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 6.3837e-05 - accuracy: 1.0000 - val_loss: 0.1832 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00702: loss did not improve from 0.00000\n",
            "Epoch 703/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.0665e-07 - accuracy: 1.0000 - val_loss: 0.1735 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00703: loss did not improve from 0.00000\n",
            "Epoch 704/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0011 - accuracy: 0.9986 - val_loss: 0.2475 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00704: loss did not improve from 0.00000\n",
            "Epoch 705/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.6911e-07 - accuracy: 1.0000 - val_loss: 0.2442 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00705: loss did not improve from 0.00000\n",
            "Epoch 706/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 9.3172e-07 - accuracy: 1.0000 - val_loss: 0.2342 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00706: loss did not improve from 0.00000\n",
            "Epoch 707/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 6.0032e-08 - accuracy: 1.0000 - val_loss: 0.2330 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00707: loss did not improve from 0.00000\n",
            "Epoch 708/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.3322e-07 - accuracy: 1.0000 - val_loss: 0.2489 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00708: loss did not improve from 0.00000\n",
            "Epoch 709/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.8279e-04 - accuracy: 1.0000 - val_loss: 0.2460 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00709: loss did not improve from 0.00000\n",
            "Epoch 710/1000\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 8.3098e-08 - accuracy: 1.0000 - val_loss: 0.2439 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00710: loss did not improve from 0.00000\n",
            "Epoch 711/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.9894e-08 - accuracy: 1.0000 - val_loss: 0.2381 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00711: loss did not improve from 0.00000\n",
            "Epoch 712/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0166 - accuracy: 0.9985 - val_loss: 0.2989 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00712: loss did not improve from 0.00000\n",
            "Epoch 713/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.7012e-04 - accuracy: 1.0000 - val_loss: 0.2974 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00713: loss did not improve from 0.00000\n",
            "Epoch 714/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.0898e-05 - accuracy: 1.0000 - val_loss: 0.2605 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00714: loss did not improve from 0.00000\n",
            "Epoch 715/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.7979e-08 - accuracy: 1.0000 - val_loss: 0.2661 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00715: loss did not improve from 0.00000\n",
            "Epoch 716/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.8362e-07 - accuracy: 1.0000 - val_loss: 0.2440 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00716: loss did not improve from 0.00000\n",
            "Epoch 717/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.8595e-07 - accuracy: 1.0000 - val_loss: 0.2417 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00717: loss did not improve from 0.00000\n",
            "Epoch 718/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 9.2006e-07 - accuracy: 1.0000 - val_loss: 0.1950 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00718: loss did not improve from 0.00000\n",
            "Epoch 719/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.1728e-09 - accuracy: 1.0000 - val_loss: 0.1948 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00719: loss did not improve from 0.00000\n",
            "Epoch 720/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.2111e-06 - accuracy: 1.0000 - val_loss: 0.3065 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00720: loss did not improve from 0.00000\n",
            "Epoch 721/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.8728e-05 - accuracy: 1.0000 - val_loss: 0.3279 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00721: loss did not improve from 0.00000\n",
            "Epoch 722/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 8.1772e-08 - accuracy: 1.0000 - val_loss: 0.3375 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00722: loss did not improve from 0.00000\n",
            "Epoch 723/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.3375 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00723: loss did not improve from 0.00000\n",
            "Epoch 724/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 7.1447e-11 - accuracy: 1.0000 - val_loss: 0.3386 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00724: loss did not improve from 0.00000\n",
            "Epoch 725/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.6542e-09 - accuracy: 1.0000 - val_loss: 0.3332 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00725: loss did not improve from 0.00000\n",
            "Epoch 726/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.9601e-08 - accuracy: 1.0000 - val_loss: 0.2955 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00726: loss did not improve from 0.00000\n",
            "Epoch 727/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 6.8120e-09 - accuracy: 1.0000 - val_loss: 0.3878 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00727: loss did not improve from 0.00000\n",
            "Epoch 728/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0011 - accuracy: 0.9986 - val_loss: 0.3206 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00728: loss did not improve from 0.00000\n",
            "Epoch 729/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.8191e-08 - accuracy: 1.0000 - val_loss: 0.3281 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00729: loss did not improve from 0.00000\n",
            "Epoch 730/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 5.8067e-09 - accuracy: 1.0000 - val_loss: 0.3295 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00730: loss did not improve from 0.00000\n",
            "Epoch 731/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.4400e-04 - accuracy: 1.0000 - val_loss: 0.3338 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00731: loss did not improve from 0.00000\n",
            "Epoch 732/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.1558e-04 - accuracy: 1.0000 - val_loss: 0.3709 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00732: loss did not improve from 0.00000\n",
            "Epoch 733/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.4192e-07 - accuracy: 1.0000 - val_loss: 0.3787 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00733: loss did not improve from 0.00000\n",
            "Epoch 734/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.7889e-08 - accuracy: 1.0000 - val_loss: 0.3701 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00734: loss did not improve from 0.00000\n",
            "Epoch 735/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.7536e-07 - accuracy: 1.0000 - val_loss: 0.3629 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00735: loss did not improve from 0.00000\n",
            "Epoch 736/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 6.9709e-05 - accuracy: 1.0000 - val_loss: 0.4934 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00736: loss did not improve from 0.00000\n",
            "Epoch 737/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 9.7441e-05 - accuracy: 1.0000 - val_loss: 0.4298 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00737: loss did not improve from 0.00000\n",
            "Epoch 738/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.4049e-06 - accuracy: 1.0000 - val_loss: 0.4280 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00738: loss did not improve from 0.00000\n",
            "Epoch 739/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.2770e-06 - accuracy: 1.0000 - val_loss: 0.3450 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00739: loss did not improve from 0.00000\n",
            "Epoch 740/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.3602 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00740: loss did not improve from 0.00000\n",
            "Epoch 741/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.8575e-07 - accuracy: 1.0000 - val_loss: 0.3617 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00741: loss did not improve from 0.00000\n",
            "Epoch 742/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.2389e-06 - accuracy: 1.0000 - val_loss: 0.3396 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00742: loss did not improve from 0.00000\n",
            "Epoch 743/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.8794e-05 - accuracy: 1.0000 - val_loss: 0.3728 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00743: loss did not improve from 0.00000\n",
            "Epoch 744/1000\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 6.0916e-07 - accuracy: 1.0000 - val_loss: 0.3909 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00744: loss did not improve from 0.00000\n",
            "Epoch 745/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.1351e-08 - accuracy: 1.0000 - val_loss: 0.3934 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00745: loss did not improve from 0.00000\n",
            "Epoch 746/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.8724e-08 - accuracy: 1.0000 - val_loss: 0.3714 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00746: loss did not improve from 0.00000\n",
            "Epoch 747/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.2437e-09 - accuracy: 1.0000 - val_loss: 0.3728 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00747: loss did not improve from 0.00000\n",
            "Epoch 748/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.6269e-06 - accuracy: 1.0000 - val_loss: 0.4318 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00748: loss did not improve from 0.00000\n",
            "Epoch 749/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0249 - accuracy: 0.9975 - val_loss: 0.3384 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00749: loss did not improve from 0.00000\n",
            "Epoch 750/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 7.5698e-08 - accuracy: 1.0000 - val_loss: 0.3084 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00750: loss did not improve from 0.00000\n",
            "Epoch 751/1000\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 6.7400e-07 - accuracy: 1.0000 - val_loss: 0.3283 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00751: loss did not improve from 0.00000\n",
            "Epoch 752/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.8690e-05 - accuracy: 1.0000 - val_loss: 0.3651 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00752: loss did not improve from 0.00000\n",
            "Epoch 753/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0011 - accuracy: 0.9993 - val_loss: 0.3750 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00753: loss did not improve from 0.00000\n",
            "Epoch 754/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.1740e-06 - accuracy: 1.0000 - val_loss: 0.3287 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00754: loss did not improve from 0.00000\n",
            "Epoch 755/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.2847e-05 - accuracy: 1.0000 - val_loss: 0.2384 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00755: loss did not improve from 0.00000\n",
            "Epoch 756/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.5594e-07 - accuracy: 1.0000 - val_loss: 0.2321 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00756: loss did not improve from 0.00000\n",
            "Epoch 757/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.0790e-08 - accuracy: 1.0000 - val_loss: 0.2312 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00757: loss did not improve from 0.00000\n",
            "Epoch 758/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.2313 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00758: loss did not improve from 0.00000\n",
            "Epoch 759/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.7137e-05 - accuracy: 1.0000 - val_loss: 0.2660 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00759: loss did not improve from 0.00000\n",
            "Epoch 760/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.4050e-07 - accuracy: 1.0000 - val_loss: 0.2696 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00760: loss did not improve from 0.00000\n",
            "Epoch 761/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.4053e-08 - accuracy: 1.0000 - val_loss: 0.2589 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00761: loss did not improve from 0.00000\n",
            "Epoch 762/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.6496e-08 - accuracy: 1.0000 - val_loss: 0.2745 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00762: loss did not improve from 0.00000\n",
            "Epoch 763/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 7.5736e-09 - accuracy: 1.0000 - val_loss: 0.2632 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00763: loss did not improve from 0.00000\n",
            "Epoch 764/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.2633 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00764: loss did not improve from 0.00000\n",
            "Epoch 765/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0011 - accuracy: 0.9993 - val_loss: 0.2942 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00765: loss did not improve from 0.00000\n",
            "Epoch 766/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 6.5174e-05 - accuracy: 1.0000 - val_loss: 0.3456 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00766: loss did not improve from 0.00000\n",
            "Epoch 767/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 7.6826e-09 - accuracy: 1.0000 - val_loss: 0.3357 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00767: loss did not improve from 0.00000\n",
            "Epoch 768/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 8.4579e-07 - accuracy: 1.0000 - val_loss: 0.3169 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00768: loss did not improve from 0.00000\n",
            "Epoch 769/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.2697e-05 - accuracy: 1.0000 - val_loss: 0.3104 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00769: loss did not improve from 0.00000\n",
            "Epoch 770/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.3104 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00770: loss did not improve from 0.00000\n",
            "Epoch 771/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.4495e-05 - accuracy: 1.0000 - val_loss: 0.3113 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00771: loss did not improve from 0.00000\n",
            "Epoch 772/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.6638e-08 - accuracy: 1.0000 - val_loss: 0.3249 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00772: loss did not improve from 0.00000\n",
            "Epoch 773/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 6.8623e-06 - accuracy: 1.0000 - val_loss: 0.3891 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00773: loss did not improve from 0.00000\n",
            "Epoch 774/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0145 - accuracy: 0.9975 - val_loss: 0.3260 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00774: loss did not improve from 0.00000\n",
            "Epoch 775/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.3284e-08 - accuracy: 1.0000 - val_loss: 0.3514 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00775: loss did not improve from 0.00000\n",
            "Epoch 776/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.0288e-04 - accuracy: 1.0000 - val_loss: 0.2042 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00776: loss did not improve from 0.00000\n",
            "Epoch 777/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 7.3996e-07 - accuracy: 1.0000 - val_loss: 0.2521 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00777: loss did not improve from 0.00000\n",
            "Epoch 778/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.5974e-09 - accuracy: 1.0000 - val_loss: 0.2516 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00778: loss did not improve from 0.00000\n",
            "Epoch 779/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.0605e-07 - accuracy: 1.0000 - val_loss: 0.2358 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00779: loss did not improve from 0.00000\n",
            "Epoch 780/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0126 - accuracy: 0.9974 - val_loss: 0.4281 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00780: loss did not improve from 0.00000\n",
            "Epoch 781/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.0674e-07 - accuracy: 1.0000 - val_loss: 0.4129 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00781: loss did not improve from 0.00000\n",
            "Epoch 782/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.4607e-05 - accuracy: 1.0000 - val_loss: 0.3997 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00782: loss did not improve from 0.00000\n",
            "Epoch 783/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.8515e-05 - accuracy: 1.0000 - val_loss: 0.4510 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00783: loss did not improve from 0.00000\n",
            "Epoch 784/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.8691e-07 - accuracy: 1.0000 - val_loss: 0.4362 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00784: loss did not improve from 0.00000\n",
            "Epoch 785/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 9.7590e-09 - accuracy: 1.0000 - val_loss: 0.4405 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00785: loss did not improve from 0.00000\n",
            "Epoch 786/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.0405e-06 - accuracy: 1.0000 - val_loss: 0.4702 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00786: loss did not improve from 0.00000\n",
            "Epoch 787/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0135 - accuracy: 0.9994 - val_loss: 0.4259 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00787: loss did not improve from 0.00000\n",
            "Epoch 788/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 6.5618e-05 - accuracy: 1.0000 - val_loss: 0.4066 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00788: loss did not improve from 0.00000\n",
            "Epoch 789/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.8691e-08 - accuracy: 1.0000 - val_loss: 0.3931 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00789: loss did not improve from 0.00000\n",
            "Epoch 790/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.0392e-06 - accuracy: 1.0000 - val_loss: 0.3983 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00790: loss did not improve from 0.00000\n",
            "Epoch 791/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.5166e-10 - accuracy: 1.0000 - val_loss: 0.3984 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00791: loss did not improve from 0.00000\n",
            "Epoch 792/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.8959e-05 - accuracy: 1.0000 - val_loss: 0.4203 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00792: loss did not improve from 0.00000\n",
            "Epoch 793/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.8994e-05 - accuracy: 1.0000 - val_loss: 0.4050 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00793: loss did not improve from 0.00000\n",
            "Epoch 794/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.8506e-04 - accuracy: 1.0000 - val_loss: 0.5812 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00794: loss did not improve from 0.00000\n",
            "Epoch 795/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.5411 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00795: loss did not improve from 0.00000\n",
            "Epoch 796/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0445 - accuracy: 0.9975 - val_loss: 0.4949 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00796: loss did not improve from 0.00000\n",
            "Epoch 797/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0128 - accuracy: 0.9986 - val_loss: 0.3956 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00797: loss did not improve from 0.00000\n",
            "Epoch 798/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.3532 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00798: loss did not improve from 0.00000\n",
            "Epoch 799/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.0763e-08 - accuracy: 1.0000 - val_loss: 0.3522 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00799: loss did not improve from 0.00000\n",
            "Epoch 800/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 6.9627e-05 - accuracy: 1.0000 - val_loss: 0.3718 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00800: loss did not improve from 0.00000\n",
            "Epoch 801/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.8369e-04 - accuracy: 1.0000 - val_loss: 0.3706 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00801: loss did not improve from 0.00000\n",
            "Epoch 802/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.3681e-08 - accuracy: 1.0000 - val_loss: 0.3730 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00802: loss did not improve from 0.00000\n",
            "Epoch 803/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0254 - accuracy: 0.9934 - val_loss: 0.2693 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00803: loss did not improve from 0.00000\n",
            "Epoch 804/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 6.9576e-05 - accuracy: 1.0000 - val_loss: 0.2886 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00804: loss did not improve from 0.00000\n",
            "Epoch 805/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.8394e-10 - accuracy: 1.0000 - val_loss: 0.2886 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00805: loss did not improve from 0.00000\n",
            "Epoch 806/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.8168e-04 - accuracy: 1.0000 - val_loss: 0.3655 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00806: loss did not improve from 0.00000\n",
            "Epoch 807/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.0477e-05 - accuracy: 1.0000 - val_loss: 0.3966 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00807: loss did not improve from 0.00000\n",
            "Epoch 808/1000\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 3.1200e-06 - accuracy: 1.0000 - val_loss: 0.3935 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00808: loss did not improve from 0.00000\n",
            "Epoch 809/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.1348e-11 - accuracy: 1.0000 - val_loss: 0.3933 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00809: loss did not improve from 0.00000\n",
            "Epoch 810/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 8.1575e-07 - accuracy: 1.0000 - val_loss: 0.3412 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00810: loss did not improve from 0.00000\n",
            "Epoch 811/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 5.4212e-05 - accuracy: 1.0000 - val_loss: 0.2377 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00811: loss did not improve from 0.00000\n",
            "Epoch 812/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.8010e-05 - accuracy: 1.0000 - val_loss: 0.3606 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00812: loss did not improve from 0.00000\n",
            "Epoch 813/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.3727e-08 - accuracy: 1.0000 - val_loss: 0.3622 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00813: loss did not improve from 0.00000\n",
            "Epoch 814/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.6746e-09 - accuracy: 1.0000 - val_loss: 0.3612 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00814: loss did not improve from 0.00000\n",
            "Epoch 815/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 8.0228e-10 - accuracy: 1.0000 - val_loss: 0.3566 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00815: loss did not improve from 0.00000\n",
            "Epoch 816/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.7549e-08 - accuracy: 1.0000 - val_loss: 0.3300 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00816: loss did not improve from 0.00000\n",
            "Epoch 817/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 6.1647e-11 - accuracy: 1.0000 - val_loss: 0.3304 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00817: loss did not improve from 0.00000\n",
            "Epoch 818/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.8231e-08 - accuracy: 1.0000 - val_loss: 0.3493 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00818: loss did not improve from 0.00000\n",
            "Epoch 819/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.0233e-09 - accuracy: 1.0000 - val_loss: 0.3577 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00819: loss did not improve from 0.00000\n",
            "Epoch 820/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 9.3997e-09 - accuracy: 1.0000 - val_loss: 0.4128 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00820: loss did not improve from 0.00000\n",
            "Epoch 821/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.0087e-05 - accuracy: 1.0000 - val_loss: 0.4804 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00821: loss did not improve from 0.00000\n",
            "Epoch 822/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.0460e-04 - accuracy: 1.0000 - val_loss: 0.4394 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00822: loss did not improve from 0.00000\n",
            "Epoch 823/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.0199e-05 - accuracy: 1.0000 - val_loss: 0.3693 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00823: loss did not improve from 0.00000\n",
            "Epoch 824/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 6.0314e-04 - accuracy: 0.9997 - val_loss: 0.5289 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00824: loss did not improve from 0.00000\n",
            "Epoch 825/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.2246e-04 - accuracy: 1.0000 - val_loss: 0.4578 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00825: loss did not improve from 0.00000\n",
            "Epoch 826/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 6.7045e-07 - accuracy: 1.0000 - val_loss: 0.4767 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00826: loss did not improve from 0.00000\n",
            "Epoch 827/1000\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 2.1213e-04 - accuracy: 1.0000 - val_loss: 0.4309 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00827: loss did not improve from 0.00000\n",
            "Epoch 828/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.6139e-08 - accuracy: 1.0000 - val_loss: 0.4297 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00828: loss did not improve from 0.00000\n",
            "Epoch 829/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 9.2243e-07 - accuracy: 1.0000 - val_loss: 0.3643 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00829: loss did not improve from 0.00000\n",
            "Epoch 830/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.1792e-05 - accuracy: 1.0000 - val_loss: 0.4456 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00830: loss did not improve from 0.00000\n",
            "Epoch 831/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 6.9355e-07 - accuracy: 1.0000 - val_loss: 0.4837 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00831: loss did not improve from 0.00000\n",
            "Epoch 832/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.9300e-08 - accuracy: 1.0000 - val_loss: 0.4911 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00832: loss did not improve from 0.00000\n",
            "Epoch 833/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.3795e-08 - accuracy: 1.0000 - val_loss: 0.4923 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00833: loss did not improve from 0.00000\n",
            "Epoch 834/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 5.7032e-04 - accuracy: 0.9998 - val_loss: 0.3370 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00834: loss did not improve from 0.00000\n",
            "Epoch 835/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.3518e-05 - accuracy: 1.0000 - val_loss: 0.3386 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00835: loss did not improve from 0.00000\n",
            "Epoch 836/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.0225e-06 - accuracy: 1.0000 - val_loss: 0.3109 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00836: loss did not improve from 0.00000\n",
            "Epoch 837/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.9024e-09 - accuracy: 1.0000 - val_loss: 0.3152 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00837: loss did not improve from 0.00000\n",
            "Epoch 838/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.8994e-04 - accuracy: 1.0000 - val_loss: 0.2397 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00838: loss did not improve from 0.00000\n",
            "Epoch 839/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.5724e-11 - accuracy: 1.0000 - val_loss: 0.2395 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00839: loss did not improve from 0.00000\n",
            "Epoch 840/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.0950e-05 - accuracy: 1.0000 - val_loss: 0.2190 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00840: loss did not improve from 0.00000\n",
            "Epoch 841/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.8291e-08 - accuracy: 1.0000 - val_loss: 0.2376 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00841: loss did not improve from 0.00000\n",
            "Epoch 842/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.4674e-10 - accuracy: 1.0000 - val_loss: 0.2381 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00842: loss did not improve from 0.00000\n",
            "Epoch 843/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.8494e-06 - accuracy: 1.0000 - val_loss: 0.2065 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00843: loss did not improve from 0.00000\n",
            "Epoch 844/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0033 - accuracy: 0.9999 - val_loss: 0.2564 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00844: loss did not improve from 0.00000\n",
            "Epoch 845/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.5187e-05 - accuracy: 1.0000 - val_loss: 0.2542 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00845: loss did not improve from 0.00000\n",
            "Epoch 846/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.0263e-10 - accuracy: 1.0000 - val_loss: 0.2556 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00846: loss did not improve from 0.00000\n",
            "Epoch 847/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.3590e-05 - accuracy: 1.0000 - val_loss: 0.2439 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00847: loss did not improve from 0.00000\n",
            "Epoch 848/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 8.9725e-04 - accuracy: 0.9992 - val_loss: 0.1728 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00848: loss did not improve from 0.00000\n",
            "Epoch 849/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.0943e-07 - accuracy: 1.0000 - val_loss: 0.1915 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00849: loss did not improve from 0.00000\n",
            "Epoch 850/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.4346 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00850: loss did not improve from 0.00000\n",
            "Epoch 851/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 6.2681e-05 - accuracy: 1.0000 - val_loss: 0.4712 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00851: loss did not improve from 0.00000\n",
            "Epoch 852/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 7.7195e-06 - accuracy: 1.0000 - val_loss: 0.4624 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00852: loss did not improve from 0.00000\n",
            "Epoch 853/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.7529e-08 - accuracy: 1.0000 - val_loss: 0.4478 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00853: loss did not improve from 0.00000\n",
            "Epoch 854/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.9874e-07 - accuracy: 1.0000 - val_loss: 0.4477 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00854: loss did not improve from 0.00000\n",
            "Epoch 855/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0155 - accuracy: 0.9975 - val_loss: 0.5308 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00855: loss did not improve from 0.00000\n",
            "Epoch 856/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.3672e-09 - accuracy: 1.0000 - val_loss: 0.5326 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00856: loss did not improve from 0.00000\n",
            "Epoch 857/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 7.5956e-04 - accuracy: 1.0000 - val_loss: 0.4559 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00857: loss did not improve from 0.00000\n",
            "Epoch 858/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.3419e-05 - accuracy: 1.0000 - val_loss: 0.4759 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00858: loss did not improve from 0.00000\n",
            "Epoch 859/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.5653e-09 - accuracy: 1.0000 - val_loss: 0.4761 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00859: loss did not improve from 0.00000\n",
            "Epoch 860/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 6.6720e-07 - accuracy: 1.0000 - val_loss: 0.4323 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00860: loss did not improve from 0.00000\n",
            "Epoch 861/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.6548e-06 - accuracy: 1.0000 - val_loss: 0.4697 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00861: loss did not improve from 0.00000\n",
            "Epoch 862/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.4694 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00862: loss did not improve from 0.00000\n",
            "Epoch 863/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.3794e-08 - accuracy: 1.0000 - val_loss: 0.4399 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00863: loss did not improve from 0.00000\n",
            "Epoch 864/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.6220e-09 - accuracy: 1.0000 - val_loss: 0.4415 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00864: loss did not improve from 0.00000\n",
            "Epoch 865/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.1012e-05 - accuracy: 1.0000 - val_loss: 0.2981 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00865: loss did not improve from 0.00000\n",
            "Epoch 866/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.2205e-05 - accuracy: 1.0000 - val_loss: 0.3541 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00866: loss did not improve from 0.00000\n",
            "Epoch 867/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 9.6781e-09 - accuracy: 1.0000 - val_loss: 0.3539 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00867: loss did not improve from 0.00000\n",
            "Epoch 868/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.8476e-10 - accuracy: 1.0000 - val_loss: 0.3540 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00868: loss did not improve from 0.00000\n",
            "Epoch 869/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.0825e-06 - accuracy: 1.0000 - val_loss: 0.3678 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00869: loss did not improve from 0.00000\n",
            "Epoch 870/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.6229e-07 - accuracy: 1.0000 - val_loss: 0.3473 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00870: loss did not improve from 0.00000\n",
            "Epoch 871/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.3472 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00871: loss did not improve from 0.00000\n",
            "Epoch 872/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.5923e-06 - accuracy: 1.0000 - val_loss: 0.5047 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00872: loss did not improve from 0.00000\n",
            "Epoch 873/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.0084e-07 - accuracy: 1.0000 - val_loss: 0.5755 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00873: loss did not improve from 0.00000\n",
            "Epoch 874/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 1.2075e-10 - accuracy: 1.0000 - val_loss: 0.5743 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00874: loss did not improve from 0.00000\n",
            "Epoch 875/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.0377e-04 - accuracy: 0.9999 - val_loss: 0.3372 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00875: loss did not improve from 0.00000\n",
            "Epoch 876/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.3372 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00876: loss did not improve from 0.00000\n",
            "Epoch 877/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0052 - accuracy: 0.9992 - val_loss: 0.2671 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00877: loss did not improve from 0.00000\n",
            "Epoch 878/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.1992e-08 - accuracy: 1.0000 - val_loss: 0.2882 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00878: loss did not improve from 0.00000\n",
            "Epoch 879/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.5626 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00879: loss did not improve from 0.00000\n",
            "Epoch 880/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.4119e-08 - accuracy: 1.0000 - val_loss: 0.5546 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00880: loss did not improve from 0.00000\n",
            "Epoch 881/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 4.6943e-06 - accuracy: 1.0000 - val_loss: 0.5949 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00881: loss did not improve from 0.00000\n",
            "Epoch 882/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.1928e-08 - accuracy: 1.0000 - val_loss: 0.5906 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00882: loss did not improve from 0.00000\n",
            "Epoch 883/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.7121e-07 - accuracy: 1.0000 - val_loss: 0.5896 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00883: loss did not improve from 0.00000\n",
            "Epoch 884/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.3364e-06 - accuracy: 1.0000 - val_loss: 0.4666 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00884: loss did not improve from 0.00000\n",
            "Epoch 885/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 7.7990e-09 - accuracy: 1.0000 - val_loss: 0.4642 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00885: loss did not improve from 0.00000\n",
            "Epoch 886/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.1554e-09 - accuracy: 1.0000 - val_loss: 0.4634 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00886: loss did not improve from 0.00000\n",
            "Epoch 887/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.0844e-05 - accuracy: 1.0000 - val_loss: 0.4960 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00887: loss did not improve from 0.00000\n",
            "Epoch 888/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.2913e-05 - accuracy: 1.0000 - val_loss: 0.4358 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00888: loss did not improve from 0.00000\n",
            "Epoch 889/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.6721e-08 - accuracy: 1.0000 - val_loss: 0.4282 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00889: loss did not improve from 0.00000\n",
            "Epoch 890/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.1143e-11 - accuracy: 1.0000 - val_loss: 0.4292 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00890: loss did not improve from 0.00000\n",
            "Epoch 891/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.7824e-09 - accuracy: 1.0000 - val_loss: 0.4291 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00891: loss did not improve from 0.00000\n",
            "Epoch 892/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0075 - accuracy: 0.9975 - val_loss: 0.4150 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00892: loss did not improve from 0.00000\n",
            "Epoch 893/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 5.2648e-09 - accuracy: 1.0000 - val_loss: 0.4080 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00893: loss did not improve from 0.00000\n",
            "Epoch 894/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.4080 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00894: loss did not improve from 0.00000\n",
            "Epoch 895/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 9.9235e-08 - accuracy: 1.0000 - val_loss: 0.3896 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00895: loss did not improve from 0.00000\n",
            "Epoch 896/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.2602e-07 - accuracy: 1.0000 - val_loss: 0.3968 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00896: loss did not improve from 0.00000\n",
            "Epoch 897/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.2510e-04 - accuracy: 1.0000 - val_loss: 0.3522 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00897: loss did not improve from 0.00000\n",
            "Epoch 898/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 5.2898e-06 - accuracy: 1.0000 - val_loss: 0.3800 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00898: loss did not improve from 0.00000\n",
            "Epoch 899/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 5.7343e-07 - accuracy: 1.0000 - val_loss: 0.3983 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00899: loss did not improve from 0.00000\n",
            "Epoch 900/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.3163e-09 - accuracy: 1.0000 - val_loss: 0.3982 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00900: loss did not improve from 0.00000\n",
            "Epoch 901/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.1090e-08 - accuracy: 1.0000 - val_loss: 0.4032 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00901: loss did not improve from 0.00000\n",
            "Epoch 902/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.0308e-06 - accuracy: 1.0000 - val_loss: 0.3933 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00902: loss did not improve from 0.00000\n",
            "Epoch 903/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 8.6237e-10 - accuracy: 1.0000 - val_loss: 0.3933 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00903: loss did not improve from 0.00000\n",
            "Epoch 904/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 8.7885e-06 - accuracy: 1.0000 - val_loss: 0.2714 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00904: loss did not improve from 0.00000\n",
            "Epoch 905/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.3674 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00905: loss did not improve from 0.00000\n",
            "Epoch 906/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.3753e-10 - accuracy: 1.0000 - val_loss: 0.3680 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00906: loss did not improve from 0.00000\n",
            "Epoch 907/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0012 - accuracy: 0.9992 - val_loss: 0.4380 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00907: loss did not improve from 0.00000\n",
            "Epoch 908/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.3679 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00908: loss did not improve from 0.00000\n",
            "Epoch 909/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 5.1138e-05 - accuracy: 1.0000 - val_loss: 0.3617 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00909: loss did not improve from 0.00000\n",
            "Epoch 910/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.9809e-07 - accuracy: 1.0000 - val_loss: 0.3588 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00910: loss did not improve from 0.00000\n",
            "Epoch 911/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 5.2634e-09 - accuracy: 1.0000 - val_loss: 0.3590 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00911: loss did not improve from 0.00000\n",
            "Epoch 912/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0089 - accuracy: 0.9970 - val_loss: 0.2910 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00912: loss did not improve from 0.00000\n",
            "Epoch 913/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 5.1691e-07 - accuracy: 1.0000 - val_loss: 0.2753 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00913: loss did not improve from 0.00000\n",
            "Epoch 914/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 9.4814e-08 - accuracy: 1.0000 - val_loss: 0.2822 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00914: loss did not improve from 0.00000\n",
            "Epoch 915/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.0973e-08 - accuracy: 1.0000 - val_loss: 0.2917 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00915: loss did not improve from 0.00000\n",
            "Epoch 916/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.0001e-08 - accuracy: 1.0000 - val_loss: 0.2946 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00916: loss did not improve from 0.00000\n",
            "Epoch 917/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 8.3165e-05 - accuracy: 1.0000 - val_loss: 0.3721 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00917: loss did not improve from 0.00000\n",
            "Epoch 918/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.0401e-09 - accuracy: 1.0000 - val_loss: 0.3753 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00918: loss did not improve from 0.00000\n",
            "Epoch 919/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 7.7158e-06 - accuracy: 1.0000 - val_loss: 0.2594 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00919: loss did not improve from 0.00000\n",
            "Epoch 920/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 8.1202e-06 - accuracy: 1.0000 - val_loss: 0.2208 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00920: loss did not improve from 0.00000\n",
            "Epoch 921/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.4498e-07 - accuracy: 1.0000 - val_loss: 0.1586 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00921: loss did not improve from 0.00000\n",
            "Epoch 922/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.3906e-06 - accuracy: 1.0000 - val_loss: 0.1991 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00922: loss did not improve from 0.00000\n",
            "Epoch 923/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 6.2814e-08 - accuracy: 1.0000 - val_loss: 0.2287 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00923: loss did not improve from 0.00000\n",
            "Epoch 924/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.0571e-11 - accuracy: 1.0000 - val_loss: 0.2282 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00924: loss did not improve from 0.00000\n",
            "Epoch 925/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.2282 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00925: loss did not improve from 0.00000\n",
            "Epoch 926/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.2373e-07 - accuracy: 1.0000 - val_loss: 0.2236 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00926: loss did not improve from 0.00000\n",
            "Epoch 927/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.2237 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00927: loss did not improve from 0.00000\n",
            "Epoch 928/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 4.3893e-10 - accuracy: 1.0000 - val_loss: 0.2121 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00928: loss did not improve from 0.00000\n",
            "Epoch 929/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.8999e-07 - accuracy: 1.0000 - val_loss: 0.2373 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00929: loss did not improve from 0.00000\n",
            "Epoch 930/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.6464e-09 - accuracy: 1.0000 - val_loss: 0.2268 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00930: loss did not improve from 0.00000\n",
            "Epoch 931/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.2268 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00931: loss did not improve from 0.00000\n",
            "Epoch 932/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.1438e-09 - accuracy: 1.0000 - val_loss: 0.2488 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00932: loss did not improve from 0.00000\n",
            "Epoch 933/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.2487 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00933: loss did not improve from 0.00000\n",
            "Epoch 934/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 5.8214e-09 - accuracy: 1.0000 - val_loss: 0.3081 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00934: loss did not improve from 0.00000\n",
            "Epoch 935/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.3081 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00935: loss did not improve from 0.00000\n",
            "Epoch 936/1000\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.3093 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00936: loss did not improve from 0.00000\n",
            "Epoch 937/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.8394e-10 - accuracy: 1.0000 - val_loss: 0.3115 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00937: loss did not improve from 0.00000\n",
            "Epoch 938/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.3115 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00938: loss did not improve from 0.00000\n",
            "Epoch 939/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.3422e-09 - accuracy: 1.0000 - val_loss: 0.1995 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00939: loss did not improve from 0.00000\n",
            "Epoch 940/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.1996 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00940: loss did not improve from 0.00000\n",
            "Epoch 941/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.6454e-09 - accuracy: 1.0000 - val_loss: 0.2236 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00941: loss did not improve from 0.00000\n",
            "Epoch 942/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.2236 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00942: loss did not improve from 0.00000\n",
            "Epoch 943/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.4867e-10 - accuracy: 1.0000 - val_loss: 0.2688 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00943: loss did not improve from 0.00000\n",
            "Epoch 944/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.7823e-09 - accuracy: 1.0000 - val_loss: 0.3688 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00944: loss did not improve from 0.00000\n",
            "Epoch 945/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 4.0576e-05 - accuracy: 1.0000 - val_loss: 0.2510 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00945: loss did not improve from 0.00000\n",
            "Epoch 946/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 2.4920e-04 - accuracy: 0.9998 - val_loss: 0.4729 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00946: loss did not improve from 0.00000\n",
            "Epoch 947/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0061 - accuracy: 0.9998 - val_loss: 0.2184 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00947: loss did not improve from 0.00000\n",
            "Epoch 948/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.3177e-08 - accuracy: 1.0000 - val_loss: 0.2004 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00948: loss did not improve from 0.00000\n",
            "Epoch 949/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.2004 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00949: loss did not improve from 0.00000\n",
            "Epoch 950/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.1339e-07 - accuracy: 1.0000 - val_loss: 0.2191 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00950: loss did not improve from 0.00000\n",
            "Epoch 951/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.2515e-05 - accuracy: 1.0000 - val_loss: 0.1611 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00951: loss did not improve from 0.00000\n",
            "Epoch 952/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.1809e-05 - accuracy: 1.0000 - val_loss: 0.1929 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00952: loss did not improve from 0.00000\n",
            "Epoch 953/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.1929 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00953: loss did not improve from 0.00000\n",
            "Epoch 954/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.1929 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00954: loss did not improve from 0.00000\n",
            "Epoch 955/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 5.5507e-08 - accuracy: 1.0000 - val_loss: 0.1731 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00955: loss did not improve from 0.00000\n",
            "Epoch 956/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 6.5790e-05 - accuracy: 1.0000 - val_loss: 0.3427 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00956: loss did not improve from 0.00000\n",
            "Epoch 957/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 9.2784e-11 - accuracy: 1.0000 - val_loss: 0.3436 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00957: loss did not improve from 0.00000\n",
            "Epoch 958/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.3436 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00958: loss did not improve from 0.00000\n",
            "Epoch 959/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.7943e-08 - accuracy: 1.0000 - val_loss: 0.3339 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00959: loss did not improve from 0.00000\n",
            "Epoch 960/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.6925e-08 - accuracy: 1.0000 - val_loss: 0.3051 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00960: loss did not improve from 0.00000\n",
            "Epoch 961/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.0224e-06 - accuracy: 1.0000 - val_loss: 0.4005 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00961: loss did not improve from 0.00000\n",
            "Epoch 962/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.3547e-07 - accuracy: 1.0000 - val_loss: 0.2776 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00962: loss did not improve from 0.00000\n",
            "Epoch 963/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.4466e-04 - accuracy: 1.0000 - val_loss: 0.2476 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00963: loss did not improve from 0.00000\n",
            "Epoch 964/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.2476 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00964: loss did not improve from 0.00000\n",
            "Epoch 965/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 1.2763e-04 - accuracy: 1.0000 - val_loss: 0.1550 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00965: loss did not improve from 0.00000\n",
            "Epoch 966/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.9553e-08 - accuracy: 1.0000 - val_loss: 0.1604 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00966: loss did not improve from 0.00000\n",
            "Epoch 967/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 4.0357e-08 - accuracy: 1.0000 - val_loss: 0.2166 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00967: loss did not improve from 0.00000\n",
            "Epoch 968/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.2068e-08 - accuracy: 1.0000 - val_loss: 0.2269 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00968: loss did not improve from 0.00000\n",
            "Epoch 969/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 5.0120e-04 - accuracy: 0.9997 - val_loss: 0.3189 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00969: loss did not improve from 0.00000\n",
            "Epoch 970/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 9.7695e-10 - accuracy: 1.0000 - val_loss: 0.3173 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00970: loss did not improve from 0.00000\n",
            "Epoch 971/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 3.0071e-05 - accuracy: 1.0000 - val_loss: 0.3327 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00971: loss did not improve from 0.00000\n",
            "Epoch 972/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.8077e-07 - accuracy: 1.0000 - val_loss: 0.3035 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00972: loss did not improve from 0.00000\n",
            "Epoch 973/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 5.2296e-08 - accuracy: 1.0000 - val_loss: 0.2786 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00973: loss did not improve from 0.00000\n",
            "Epoch 974/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.8753e-08 - accuracy: 1.0000 - val_loss: 0.2824 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00974: loss did not improve from 0.00000\n",
            "Epoch 975/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 5.8917e-10 - accuracy: 1.0000 - val_loss: 0.2833 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00975: loss did not improve from 0.00000\n",
            "Epoch 976/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.2831 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00976: loss did not improve from 0.00000\n",
            "Epoch 977/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 4.3587e-05 - accuracy: 1.0000 - val_loss: 0.1364 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00977: loss did not improve from 0.00000\n",
            "Epoch 978/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.8108e-05 - accuracy: 1.0000 - val_loss: 0.1409 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00978: loss did not improve from 0.00000\n",
            "Epoch 979/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.8196e-05 - accuracy: 1.0000 - val_loss: 0.1719 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00979: loss did not improve from 0.00000\n",
            "Epoch 980/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.6672e-07 - accuracy: 1.0000 - val_loss: 0.1890 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00980: loss did not improve from 0.00000\n",
            "Epoch 981/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.1890 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00981: loss did not improve from 0.00000\n",
            "Epoch 982/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.2370e-10 - accuracy: 1.0000 - val_loss: 0.1895 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00982: loss did not improve from 0.00000\n",
            "Epoch 983/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 4.3133e-06 - accuracy: 1.0000 - val_loss: 0.2557 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00983: loss did not improve from 0.00000\n",
            "Epoch 984/1000\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0251 - accuracy: 0.9986 - val_loss: 0.2524 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00984: loss did not improve from 0.00000\n",
            "Epoch 985/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 3.8631e-05 - accuracy: 1.0000 - val_loss: 0.2714 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00985: loss did not improve from 0.00000\n",
            "Epoch 986/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.4463e-07 - accuracy: 1.0000 - val_loss: 0.3012 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00986: loss did not improve from 0.00000\n",
            "Epoch 987/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.3013 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00987: loss did not improve from 0.00000\n",
            "Epoch 988/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.9809e-07 - accuracy: 1.0000 - val_loss: 0.3080 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00988: loss did not improve from 0.00000\n",
            "Epoch 989/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.1345e-05 - accuracy: 1.0000 - val_loss: 0.3711 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00989: loss did not improve from 0.00000\n",
            "Epoch 990/1000\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 2.3236e-06 - accuracy: 1.0000 - val_loss: 0.2774 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00990: loss did not improve from 0.00000\n",
            "Epoch 991/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.2774 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00991: loss did not improve from 0.00000\n",
            "Epoch 992/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.2775 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00992: loss did not improve from 0.00000\n",
            "Epoch 993/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 1.6632e-08 - accuracy: 1.0000 - val_loss: 0.2925 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00993: loss did not improve from 0.00000\n",
            "Epoch 994/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 2.0960e-09 - accuracy: 1.0000 - val_loss: 0.2835 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00994: loss did not improve from 0.00000\n",
            "Epoch 995/1000\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 3.5540e-08 - accuracy: 1.0000 - val_loss: 0.3031 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00995: loss did not improve from 0.00000\n",
            "Epoch 996/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 4.9362e-10 - accuracy: 1.0000 - val_loss: 0.3045 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00996: loss did not improve from 0.00000\n",
            "Epoch 997/1000\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 3.5261e-08 - accuracy: 1.0000 - val_loss: 0.3260 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00997: loss did not improve from 0.00000\n",
            "Epoch 998/1000\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 2.5351e-07 - accuracy: 1.0000 - val_loss: 0.3967 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00998: loss did not improve from 0.00000\n",
            "Epoch 999/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 9.8578e-07 - accuracy: 1.0000 - val_loss: 0.3104 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 00999: loss did not improve from 0.00000\n",
            "Epoch 1000/1000\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.2626 - val_accuracy: 0.9878\n",
            "\n",
            "Epoch 01000: loss did not improve from 0.00000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbALY2K1o8mM",
        "outputId": "cdfd1e2a-381f-4952-8643-52c8bcb3db6c"
      },
      "source": [
        "# Evaluation\n",
        "score = Model.evaluate(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwIH31lCo8jo"
      },
      "source": [
        "Pred=Model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SemNX_Pso8gb",
        "outputId": "1319dcf6-3d26-4e0c-e751-41a42eff65b2"
      },
      "source": [
        "Pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 1.0000000e+00],\n",
              "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 0.0000000e+00],\n",
              "       [0.0000000e+00, 0.0000000e+00, 1.0000000e+00, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 0.0000000e+00],\n",
              "       ...,\n",
              "       [0.0000000e+00, 0.0000000e+00, 1.0000000e+00, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 0.0000000e+00],\n",
              "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "        2.6351725e-37, 1.0000000e+00],\n",
              "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "        1.0000000e+00, 0.0000000e+00]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60qIt-1D40uq",
        "outputId": "1984aa56-e346-4b96-ba00-f7d055686a9a"
      },
      "source": [
        "Y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EKIPlFU40qS",
        "outputId": "3b181d9d-1d8e-42c0-da68-d6e55701c8f6"
      },
      "source": [
        "len(Pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9msFn3sS5Sw0",
        "outputId": "60261942-53b9-4cc5-e146-968d0a9695b9"
      },
      "source": [
        "images_f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[ 85,  85,  85],\n",
              "         [ 81,  81,  81],\n",
              "         [ 62,  62,  62],\n",
              "         ...,\n",
              "         [ 64,  64,  64],\n",
              "         [ 65,  65,  65],\n",
              "         [ 81,  81,  81]],\n",
              "\n",
              "        [[ 84,  84,  84],\n",
              "         [ 71,  71,  71],\n",
              "         [ 36,  36,  36],\n",
              "         ...,\n",
              "         [ 62,  62,  62],\n",
              "         [ 66,  66,  66],\n",
              "         [ 72,  72,  72]],\n",
              "\n",
              "        [[ 79,  79,  79],\n",
              "         [ 47,  47,  47],\n",
              "         [ 14,  14,  14],\n",
              "         ...,\n",
              "         [ 54,  54,  54],\n",
              "         [ 64,  64,  64],\n",
              "         [ 62,  62,  62]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 66,  66,  66],\n",
              "         [ 67,  67,  67],\n",
              "         [ 68,  68,  68],\n",
              "         ...,\n",
              "         [  2,   2,   2],\n",
              "         [  1,   1,   1],\n",
              "         [  5,   5,   5]],\n",
              "\n",
              "        [[ 65,  65,  65],\n",
              "         [ 66,  66,  66],\n",
              "         [ 67,  67,  67],\n",
              "         ...,\n",
              "         [ 33,  33,  33],\n",
              "         [ 36,  36,  36],\n",
              "         [ 54,  54,  54]],\n",
              "\n",
              "        [[ 66,  66,  66],\n",
              "         [ 67,  67,  67],\n",
              "         [ 67,  67,  67],\n",
              "         ...,\n",
              "         [ 78,  78,  78],\n",
              "         [ 78,  78,  78],\n",
              "         [ 76,  76,  76]]],\n",
              "\n",
              "\n",
              "       [[[101, 101, 101],\n",
              "         [ 65,  65,  65],\n",
              "         [  7,   7,   7],\n",
              "         ...,\n",
              "         [ 19,  19,  19],\n",
              "         [ 65,  65,  65],\n",
              "         [127, 127, 127]],\n",
              "\n",
              "        [[ 95,  95,  95],\n",
              "         [ 34,  34,  34],\n",
              "         [  4,   4,   4],\n",
              "         ...,\n",
              "         [ 16,  16,  16],\n",
              "         [ 42,  42,  42],\n",
              "         [122, 122, 122]],\n",
              "\n",
              "        [[ 79,  79,  79],\n",
              "         [ 11,  11,  11],\n",
              "         [  3,   3,   3],\n",
              "         ...,\n",
              "         [  9,   9,   9],\n",
              "         [ 31,  31,  31],\n",
              "         [116, 116, 116]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 76,  76,  76],\n",
              "         [ 77,  77,  77],\n",
              "         [ 79,  79,  79],\n",
              "         ...,\n",
              "         [101, 101, 101],\n",
              "         [ 98,  98,  98],\n",
              "         [ 97,  97,  97]],\n",
              "\n",
              "        [[ 74,  74,  74],\n",
              "         [ 75,  75,  75],\n",
              "         [ 76,  76,  76],\n",
              "         ...,\n",
              "         [100, 100, 100],\n",
              "         [ 97,  97,  97],\n",
              "         [ 97,  97,  97]],\n",
              "\n",
              "        [[ 72,  72,  72],\n",
              "         [ 73,  73,  73],\n",
              "         [ 74,  74,  74],\n",
              "         ...,\n",
              "         [ 97,  97,  97],\n",
              "         [ 97,  97,  97],\n",
              "         [ 96,  96,  96]]],\n",
              "\n",
              "\n",
              "       [[[134, 134, 134],\n",
              "         [125, 125, 125],\n",
              "         [ 85,  85,  85],\n",
              "         ...,\n",
              "         [ 70,  70,  70],\n",
              "         [113, 113, 113],\n",
              "         [112, 112, 112]],\n",
              "\n",
              "        [[133, 133, 133],\n",
              "         [117, 117, 117],\n",
              "         [ 65,  65,  65],\n",
              "         ...,\n",
              "         [ 53,  53,  53],\n",
              "         [ 84,  84,  84],\n",
              "         [112, 112, 112]],\n",
              "\n",
              "        [[132, 132, 132],\n",
              "         [ 83,  83,  83],\n",
              "         [ 46,  46,  46],\n",
              "         ...,\n",
              "         [ 54,  54,  54],\n",
              "         [ 59,  59,  59],\n",
              "         [ 91,  91,  91]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 65,  65,  65],\n",
              "         [ 70,  70,  70],\n",
              "         [ 84,  84,  84],\n",
              "         ...,\n",
              "         [193, 193, 193],\n",
              "         [ 91,  91,  91],\n",
              "         [ 67,  67,  67]],\n",
              "\n",
              "        [[ 67,  67,  67],\n",
              "         [ 78,  78,  78],\n",
              "         [ 85,  85,  85],\n",
              "         ...,\n",
              "         [173, 173, 173],\n",
              "         [ 79,  79,  79],\n",
              "         [ 73,  73,  73]],\n",
              "\n",
              "        [[ 65,  65,  65],\n",
              "         [ 79,  79,  79],\n",
              "         [ 82,  82,  82],\n",
              "         ...,\n",
              "         [156, 156, 156],\n",
              "         [ 74,  74,  74],\n",
              "         [ 68,  68,  68]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[ 87,  87,  87],\n",
              "         [ 87,  87,  87],\n",
              "         [ 88,  88,  88],\n",
              "         ...,\n",
              "         [ 94,  94,  94],\n",
              "         [ 94,  94,  94],\n",
              "         [ 95,  95,  95]],\n",
              "\n",
              "        [[ 83,  83,  83],\n",
              "         [ 85,  85,  85],\n",
              "         [ 85,  85,  85],\n",
              "         ...,\n",
              "         [ 92,  92,  92],\n",
              "         [ 94,  94,  94],\n",
              "         [ 94,  94,  94]],\n",
              "\n",
              "        [[ 83,  83,  83],\n",
              "         [ 84,  84,  84],\n",
              "         [ 84,  84,  84],\n",
              "         ...,\n",
              "         [ 90,  90,  90],\n",
              "         [ 93,  93,  93],\n",
              "         [ 93,  93,  93]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 59,  59,  59],\n",
              "         [ 60,  60,  60],\n",
              "         [ 60,  60,  60],\n",
              "         ...,\n",
              "         [ 67,  67,  67],\n",
              "         [ 69,  69,  69],\n",
              "         [ 68,  68,  68]],\n",
              "\n",
              "        [[ 63,  63,  63],\n",
              "         [ 62,  62,  62],\n",
              "         [ 62,  62,  62],\n",
              "         ...,\n",
              "         [ 66,  66,  66],\n",
              "         [ 69,  69,  69],\n",
              "         [ 67,  67,  67]],\n",
              "\n",
              "        [[ 60,  60,  60],\n",
              "         [ 61,  61,  61],\n",
              "         [ 63,  63,  63],\n",
              "         ...,\n",
              "         [ 65,  65,  65],\n",
              "         [ 66,  66,  66],\n",
              "         [ 65,  65,  65]]],\n",
              "\n",
              "\n",
              "       [[[ 76,  76,  76],\n",
              "         [ 35,  35,  35],\n",
              "         [ 16,  16,  16],\n",
              "         ...,\n",
              "         [ 56,  56,  56],\n",
              "         [ 95,  95,  95],\n",
              "         [110, 110, 110]],\n",
              "\n",
              "        [[ 52,  52,  52],\n",
              "         [ 19,  19,  19],\n",
              "         [ 16,  16,  16],\n",
              "         ...,\n",
              "         [ 48,  48,  48],\n",
              "         [ 77,  77,  77],\n",
              "         [108, 108, 108]],\n",
              "\n",
              "        [[ 27,  27,  27],\n",
              "         [ 13,  13,  13],\n",
              "         [ 15,  15,  15],\n",
              "         ...,\n",
              "         [ 41,  41,  41],\n",
              "         [ 58,  58,  58],\n",
              "         [ 96,  96,  96]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 10,  10,  10],\n",
              "         [  9,   9,   9],\n",
              "         [  5,   5,   5],\n",
              "         ...,\n",
              "         [ 46,  46,  46],\n",
              "         [ 25,  25,  25],\n",
              "         [ 18,  18,  18]],\n",
              "\n",
              "        [[ 15,  15,  15],\n",
              "         [ 11,  11,  11],\n",
              "         [  7,   7,   7],\n",
              "         ...,\n",
              "         [ 16,  16,  16],\n",
              "         [ 14,  14,  14],\n",
              "         [ 12,  12,  12]],\n",
              "\n",
              "        [[ 16,  16,  16],\n",
              "         [ 11,  11,  11],\n",
              "         [ 10,  10,  10],\n",
              "         ...,\n",
              "         [  3,   3,   3],\n",
              "         [  3,   3,   3],\n",
              "         [ 10,  10,  10]]],\n",
              "\n",
              "\n",
              "       [[[ 19,  19,  19],\n",
              "         [ 18,  18,  18],\n",
              "         [ 24,  24,  24],\n",
              "         ...,\n",
              "         [126, 126, 126],\n",
              "         [127, 127, 127],\n",
              "         [125, 125, 125]],\n",
              "\n",
              "        [[ 13,  13,  13],\n",
              "         [ 14,  14,  14],\n",
              "         [ 22,  22,  22],\n",
              "         ...,\n",
              "         [127, 127, 127],\n",
              "         [128, 128, 128],\n",
              "         [126, 126, 126]],\n",
              "\n",
              "        [[ 12,  12,  12],\n",
              "         [ 14,  14,  14],\n",
              "         [ 21,  21,  21],\n",
              "         ...,\n",
              "         [126, 126, 126],\n",
              "         [127, 127, 127],\n",
              "         [128, 128, 128]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 74,  74,  74],\n",
              "         [ 76,  76,  76],\n",
              "         [ 77,  77,  77],\n",
              "         ...,\n",
              "         [100, 100, 100],\n",
              "         [ 98,  98,  98],\n",
              "         [ 97,  97,  97]],\n",
              "\n",
              "        [[ 73,  73,  73],\n",
              "         [ 74,  74,  74],\n",
              "         [ 75,  75,  75],\n",
              "         ...,\n",
              "         [ 95,  95,  95],\n",
              "         [ 95,  95,  95],\n",
              "         [ 95,  95,  95]],\n",
              "\n",
              "        [[ 71,  71,  71],\n",
              "         [ 72,  72,  72],\n",
              "         [ 74,  74,  74],\n",
              "         ...,\n",
              "         [ 91,  91,  91],\n",
              "         [ 91,  91,  91],\n",
              "         [ 93,  93,  93]]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoN52SQk5otz",
        "outputId": "2cd59dda-f41e-44dc-f724-a9f4b611a080"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz9FNR8140mm"
      },
      "source": [
        "def test_image(ind,images_f,images_f_2,Model):\n",
        "  cv2_imshow(images_f[ind])\n",
        "  image_test=images_f_2[ind]\n",
        "  print(\"Label actual:  \" + Exp[labels[ind]]  )\n",
        "  pred_1=Model.predict(np.array([image_test]))\n",
        "  #print(pred_1)\n",
        "  pred_class=Exp[int(np.argmax(pred_1))]\n",
        "  print(\"Predicted Label: \"+ pred_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "cZQir9fc5P50",
        "outputId": "9e01813b-1794-48a6-9115-8a3a813326f6"
      },
      "source": [
        "test_image(72,images_f, images_f_2, Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7E92CF8410>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  happy\n",
            "Predicted Label: happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "_SCce9iO59eU",
        "outputId": "a355f4c1-01d9-498c-b6e8-651c91a24e3e"
      },
      "source": [
        "test_image(36,images_f,images_f_2,Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7E9342ED10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  happy\n",
            "Predicted Label: happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "jIN5_ryg6Bl2",
        "outputId": "48a81f36-3703-436d-80c0-96effc76016e"
      },
      "source": [
        "test_image(122,images_f,images_f_2,Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7ED0327590>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  happy\n",
            "Predicted Label: happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "SlVUHYZr6Jym",
        "outputId": "c95f03be-e2aa-48f4-ae4a-afe9d3897adc"
      },
      "source": [
        "test_image(612,images_f,images_f_2,Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7E9344FDD0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  contempt\n",
            "Predicted Label: contempt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "hH8b7gZp6yGJ",
        "outputId": "92a6d447-8afb-445c-ad13-05f9711572e2"
      },
      "source": [
        "test_image(827,images_f,images_f_2,Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7E92C78F90>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  surprise\n",
            "Predicted Label: surprise\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "LRpg37mF66O0",
        "outputId": "a51c8cdf-8308-4fb9-b77c-0c024b8b6089"
      },
      "source": [
        "test_image(400,images_f,images_f_2,Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F7E92C411D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  sadness\n",
            "Predicted Label: sadness\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "hsSdFdp77CJK",
        "outputId": "780596cc-aeba-4fbb-ad79-659d06d9e834"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(History.history['loss'])\n",
        "plt.plot(History.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25, wspace=0.35)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "Qxg-dWZz7CGI",
        "outputId": "1a948a25-80de-4904-f71e-ebfccf9a4b28"
      },
      "source": [
        "plt.plot(History.history['accuracy'])\n",
        "plt.plot(History.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n",
        "                        wspace=0.35)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94vSmibo7CCv"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix \n",
        "\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6PChlNe8UQJ"
      },
      "source": [
        "i=0\n",
        "Y_test_l=[]\n",
        "Pred_l=[]\n",
        "while(i<len(Pred)):\n",
        "  Y_test_l.append(int(np.argmax(Y_test[i])))\n",
        "  Pred_l.append(int(np.argmax(Pred[i])))\n",
        "  i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDCtpacw8ZIH"
      },
      "source": [
        "report=classification_report(Y_test_l, Pred_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMdkkEMs8dBI",
        "outputId": "0d1f9521-9dc2-4f52-ed4b-aeacd6d8ddd4"
      },
      "source": [
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        16\n",
            "           1       1.00      1.00      1.00        11\n",
            "           2       1.00      1.00      1.00        59\n",
            "           3       0.90      1.00      0.95        28\n",
            "           4       1.00      1.00      1.00        63\n",
            "           5       1.00      0.93      0.96        43\n",
            "           6       1.00      1.00      1.00        26\n",
            "\n",
            "    accuracy                           0.99       246\n",
            "   macro avg       0.99      0.99      0.99       246\n",
            "weighted avg       0.99      0.99      0.99       246\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q4aWQp48677"
      },
      "source": [
        "results = confusion_matrix(Y_test_l, Pred_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "bBpWiKm29L06",
        "outputId": "3dc0282b-31a6-458d-bc87-b15e6016cfb3"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.heatmap(results, annot=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7e929fa250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spHZu_8E99Y4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uw97-VU-G15"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}